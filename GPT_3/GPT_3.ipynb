{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875074c0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# GPT-3\n",
    "\n",
    "## Building Innovative NLP Products using Large Language Models\n",
    "\n",
    "Note to follow along with this really cool book by Sandra Kublic & Shubham Saboo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b801ee",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1 The Era of Large Language Models\n",
    "\n",
    "OpenAI's paper: [Language Models are Few Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)\n",
    "\n",
    "Some Definitions\n",
    "* NLP - combines computational lingustics and machine learning to create intelligent machines capable of identifying the context of understanding the intent of natural language\n",
    "* Language Modeling - the task of assigning a probability to a sequence of words in a text in a specific language\n",
    "\n",
    "Generative Pre-Trained Transformers\n",
    "* Generative Models - generate text\n",
    "* Pre-trained Models - trained for a more general task and is then available to be fine-tuned for different tasks.\n",
    "* Transformer Models - a machine learning model that processes a sequence of text al at once (instead of a word at a time), and that has a powerful 'attention' mechanism to understand the connection between words.\n",
    "* Sequence-to-Sequence (Seq2Seq) - transformes a given sequence of elements (ex a sentence) into another sequence (ex a translation to another language). consist of two parts: an encoder and a decoder\n",
    "\n",
    "Transformer Attention Mechanisms\n",
    "* attention mechanism - a technique that mimics cognitive attention: it looks at an input sequence, piece by piece and, on the basis of probabilities, decides at each step which other parts of the sequence are important\n",
    "* self-attention - connections of words within a sentence\n",
    "* encoder-decoder attention - connection between words from the source sentence to words form the target sentence\n",
    "* GPT is just the decoder part of the transformer\n",
    "\n",
    "A brief history of GPT-3\n",
    "* GPT-1 - Trained with the 'Book Corpus' dataset and uses the decoder component of the original transformer. was able to perform decent **zero-shot learning** (perform a task without having seen an example of that kind in th epast). **zero-shot task transfer** - the model is presented with few to no examples and asked to understand the task based on the examples and the instruction.\n",
    "* GPT-2 - bigger! and with multitasking capabilities. trained on a larger collection of data and with more parameters (10x GPT-1). trained with WedText (Reddit data). Reading comp. summarization etc\n",
    "* GPT-3 - in num parameters and size of training data are 2 orders of magnitude larger than GPT-2! accessible to the public via an API\n",
    "* API - application programming interface - a software intermediary that sends information back and forth between a website or app\n",
    "* Model-as-a-Service - MaaS, developers can pay per API call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29020dae",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2 Using the Open AI API\n",
    "\n",
    "**Playground** a \"text in, text out\" interface  \n",
    "generate robust prompts that generate favorable responses for applications  \n",
    "\n",
    "### Navigating the OpenAI Playground\n",
    "\n",
    "#### Prompt Engineering and Design\n",
    "* There is a direct relation between the training prompt you provide and the quality of the completion you get  \n",
    "* The user's job is to get the model to use the information it already has to generate useful results: give GPT-3 just enough context (in the form of a training prompt) to figure out patterns and perform a given task\n",
    "* The standard flow for designing a training prompt is to try for zer-shot first, then few-shot, then go for corpus-based fine-tuning\n",
    "* Steps for designing a training prompt:\n",
    "    1. Define the task (e.g. classficiation, text generation etc.)\n",
    "    2. Is there a way to get a zero-shot solution?\n",
    "    3. Formulate the problem in a textual fashion: text-in, text-out\n",
    "    4. If you do end up using existing examples, use as few as possible and try to incorporate diversity, capturing all the representations to avoid overfitting the model or skewing the predictions\n",
    "    \n",
    "### How the OpenAI API Works\n",
    "\n",
    "#### API Components\n",
    "* Model - chose and execution engine (e.g. da vinci, babbage, ada, curie)\n",
    "* Response length - how much text the engine should return\n",
    "* Temperature - scope of randomness\n",
    "    - low temp: most 'correct' but perhaps most boring with little variation. \n",
    "    - high temp: more diverse text, but higher prob of grammer mistakes and nonsense\n",
    "* Top-P How many random results should the model consider\n",
    "    - low Top-P: a deterministic response with limited creativity\n",
    "    - high Top-P: \n",
    "    - **Tip**: change either Top-P or Temperature while keeping the dial for the other set at 1\n",
    "* Frequency penalty - reduce likelihood that model will repeat lines\n",
    "* Presence penalty - increase the likelihood that the model will incorporate new topics/sources\n",
    "* Best of - specify the number of completions/results exemplars to return\n",
    "* Stop Sequence - a set of characters that signal the API to stop generating completions\n",
    "* Inject start/restart text - allows you to insert text at the beginning/end of the completion\n",
    "    - inject start example: Once upon a time...\n",
    "    - restart text example: ...and they lived happily ever after\n",
    "* Show probabilities - show probability of tokens\n",
    "    - helpful for 'debugging' the text prompt\n",
    "    \n",
    "#### Execution Engines\n",
    "* Davinci \n",
    "    - Pros: Largest, most performant, most generalizable, better at complex tasks\n",
    "    - Cons: Most expensive and slowest\n",
    "* Curie\n",
    "    - Tries to optimize between power and speed\n",
    "    - performant and fast for classifications of chat-bot style responses\n",
    "* Babbage \n",
    "    - Faster than Curie, but best for relatively simple tasks\n",
    "    - less expensive than Davinci and Curie\n",
    "* Ada\n",
    "    - Fastest and cheapest of the GPT-3 architectures\n",
    "    - Best for use with simple tasks, but given the right context, can do more complicated jobs\n",
    "* InstructGPT Models - produce better results than their base counterparts and are now the default models of the API (`text-davinci-002` vs `davinci`)\n",
    "\n",
    "\n",
    "[OpenAI's Comparison Tool](https://gpttools.com/comparisontool)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
