{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875074c0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# GPT-3\n",
    "\n",
    "## Building Innovative NLP Products using Large Language Models\n",
    "\n",
    "Note to follow along with this really cool book by Sandra Kublic & Shubham Saboo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b801ee",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 1 The Era of Large Language Models\n",
    "\n",
    "OpenAI's paper: [Language Models are Few Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)\n",
    "\n",
    "Some Definitions\n",
    "* NLP - combines computational lingustics and machine learning to create intelligent machines capable of identifying the context of understanding the intent of natural language\n",
    "* Language Modeling - the task of assigning a probability to a sequence of words in a text in a specific language\n",
    "\n",
    "Generative Pre-Trained Transformers\n",
    "* Generative Models - generate text\n",
    "* Pre-trained Models - trained for a more general task and is then available to be fine-tuned for different tasks.\n",
    "* Transformer Models - a machine learning model that processes a sequence of text al at once (instead of a word at a time), and that has a powerful 'attention' mechanism to understand the connection between words.\n",
    "* Sequence-to-Sequence (Seq2Seq) - transformes a given sequence of elements (ex a sentence) into another sequence (ex a translation to another language). consist of two parts: an encoder and a decoder\n",
    "\n",
    "Transformer Attention Mechanisms\n",
    "* attention mechanism - a technique that mimics cognitive attention: it looks at an input sequence, piece by piece and, on the basis of probabilities, decides at each step which other parts of the sequence are important\n",
    "* self-attention - connections of words within a sentence\n",
    "* encoder-decoder attention - connection between words from the source sentence to words form the target sentence\n",
    "* GPT is just the decoder part of the transformer\n",
    "\n",
    "A brief history of GPT-3\n",
    "* GPT-1 - Trained with the 'Book Corpus' dataset and uses the decoder component of the original transformer. was able to perform decent **zero-shot learning** (perform a task without having seen an example of that kind in th epast). **zero-shot task transfer** - the model is presented with few to no examples and asked to understand the task based on the examples and the instruction.\n",
    "* GPT-2 - bigger! and with multitasking capabilities. trained on a larger collection of data and with more parameters (10x GPT-1). trained with WedText (Reddit data). Reading comp. summarization etc\n",
    "* GPT-3 - in num parameters and size of training data are 2 orders of magnitude larger than GPT-2! accessible to the public via an API\n",
    "* API - application programming interface - a software intermediary that sends information back and forth between a website or app\n",
    "* Model-as-a-Service - MaaS, developers can pay per API call."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a8204",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 2 Using the Open AI API\n",
    "\n",
    "**Playground** a \"text in, text out\" interface  \n",
    "generate robust prompts that generate favorable responses for applications  \n",
    "\n",
    "### Navigating the OpenAI Playground\n",
    "\n",
    "#### Prompt Engineering and Design\n",
    "* There is a direct relation between the training prompt you provide and the quality of the completion you get  \n",
    "* The user's job is to get the model to use the information it already has to generate useful results: give GPT-3 just enough context (in the form of a training prompt) to figure out patterns and perform a given task\n",
    "* The standard flow for designing a training prompt is to try for zer-shot first, then few-shot, then go for corpus-based fine-tuning\n",
    "* Steps for designing a training prompt:\n",
    "    1. Define the task (e.g. classficiation, text generation etc.)\n",
    "    2. Is there a way to get a zero-shot solution?\n",
    "    3. Formulate the problem in a textual fashion: text-in, text-out\n",
    "    4. If you do end up using existing examples, use as few as possible and try to incorporate diversity, capturing all the representations to avoid overfitting the model or skewing the predictions\n",
    "    \n",
    "### How the OpenAI API Works\n",
    "\n",
    "#### API Components\n",
    "* Model - chose and execution engine (e.g. da vinci, babbage, ada, curie)\n",
    "* Response length - how much text the engine should return\n",
    "* Temperature - scope of randomness\n",
    "    - low temp: most 'correct' but perhaps most boring with little variation. \n",
    "    - high temp: more diverse text, but higher prob of grammer mistakes and nonsense\n",
    "* Top-P How many random results should the model consider\n",
    "    - low Top-P: a deterministic response with limited creativity\n",
    "    - high Top-P: \n",
    "    - **Tip**: change either Top-P or Temperature while keeping the dial for the other set at 1\n",
    "* Frequency penalty - reduce likelihood that model will repeat lines\n",
    "* Presence penalty - increase the likelihood that the model will incorporate new topics/sources\n",
    "* Best of - specify the number of completions/results exemplars to return\n",
    "* Stop Sequence - a set of characters that signal the API to stop generating completions\n",
    "* Inject start/restart text - allows you to insert text at the beginning/end of the completion\n",
    "    - inject start example: Once upon a time...\n",
    "    - restart text example: ...and they lived happily ever after\n",
    "* Show probabilities - show probability of tokens\n",
    "    - helpful for 'debugging' the text prompt\n",
    "    \n",
    "#### Execution Engines\n",
    "* Davinci \n",
    "    - Pros: Largest, most performant, most generalizable, better at complex tasks\n",
    "    - Cons: Most expensive and slowest\n",
    "* Curie\n",
    "    - Tries to optimize between power and speed\n",
    "    - performant and fast for classifications of chat-bot style responses\n",
    "* Babbage \n",
    "    - Faster than Curie, but best for relatively simple tasks\n",
    "    - less expensive than Davinci and Curie\n",
    "* Ada\n",
    "    - Fastest and cheapest of the GPT-3 architectures\n",
    "    - Best for use with simple tasks, but given the right context, can do more complicated jobs\n",
    "* InstructGPT Models - produce better results than their base counterparts and are now the default models of the API (`text-davinci-002` vs `davinci`)\n",
    "\n",
    "\n",
    "[OpenAI's Comparison Tool](https://gpttools.com/comparisontool)\n",
    "\n",
    "#### Endpoints\n",
    "\n",
    "**List Engines** - a metadata endpoint that provides a list of engines  \n",
    "\n",
    "    GET https://api.openai.com/v1/engines\n",
    "    \n",
    "**Retreive Engine** - returns metadata about an engine  \n",
    "\n",
    "    GET https://api.openai.com/v1/engines/{engine_id}\n",
    "    \n",
    "**Completions** - takes a text prompt as input and returns the completed response as output  \n",
    "\n",
    "    POST https://api.openai.com/v1/engines/{engine_id}/completions\n",
    "    \n",
    "**Sematic Search** - provide a query in natural language to search a set of documents. returns a similarity score of the query to a corpus  \n",
    "\n",
    "    POST https://api.openai.com/v1/engines/{engine_id}/search  \n",
    "    \n",
    "**Files**  \n",
    "* List files `GET https://api.openai.com/v1/files`\n",
    "* Upload files `POST https://api.openai.com/v1/files`\n",
    "* Retreive file `GET https://api.openai.com/v1/files/{file_id}\n",
    "* Delete file `DELETE https://api.openai.com/v1/files/{file_id}  \n",
    "\n",
    "**Classification** - leverage a labeled set of exampes for machine learning classification tasks. an 'autoML solution  \n",
    "\n",
    "    POST https://api.openai.com/v1/classifications  \n",
    "    \n",
    "**Answers** (Beta) - GPT-3's question-answering endpoint. When given a question, the QA endpoint generates answers based on the information provided in a set of documents or training examples  \n",
    "\n",
    "    POST https://api.openai.com/v1/answers  \n",
    "    \n",
    "**Embeddings** - generate embeddings for input data from a hosted model of choice \n",
    "\n",
    "    POST https://api.openai.com/v1/engines/{engine_id}/embeddings\n",
    "    \n",
    "    \n",
    "### Customizing GPT-3\n",
    "\n",
    "[Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets](https://cdn.openai.com/palms.pdf)  \n",
    "\n",
    "Customizing ('fine-tuning') GPT-3 improves performance of any natural language task GPT-3 is capable of performing for your specific use case. **fine-tuning** is about tweaking the whole model so that it performs every time in the way you wish it to perform. the capability and knowlwedge of the model will be narrowed and focused on the contents and sematics of the dataset used for fine-tuning. Customizing GPT-3 seems to yield better results than what can be achieved with prompt design, because during this process you can provide more examples.  \n",
    "\n",
    "#### How to Customize GPT-3 for Your Application\n",
    "1. Prepare and upload training data\n",
    "    * must be JSONL where each line is a prompt-completion pair corresponding to a training example\n",
    "        * {\"prompt\": \"prompt text\", \"completion\": \"ideal generated text\"}\n",
    "    * OpenAI's CLI data preparation tool accepts files of other formats as long as there is a 'prompt'/'completion' column or key\n",
    "        * `openai tools fine_tunes.prepare_data -f <LOCAL_FILE>`\n",
    "            * LOCAL_FILE is the file you would like to have converted\n",
    "2. Training the fine-tune model\n",
    "    * use the OpenAI CLI\n",
    "        * `openai api fine_tunes.create -t <TRAIN_FILE_ID_OR_PATH> -m <BASE_MODEL>\n",
    "            * TRAIN_FILE_ID_OR_PATH - file used for training\n",
    "            * BASE_MODEL - base model you're starting training from (e.g. Ada or Davinci)\n",
    "    * does the following:\n",
    "        1. uploads the training file\n",
    "        2. fine-tunes the model\n",
    "        3. streams event logs until job is complete\n",
    "3. Use the fine-tuned model\n",
    "    * make requests by passing your model by name using the OpenAI CLI\n",
    "        * `openai api compltetions.create -m <FINE_TUNED_MODEL> -p <YOUR_PROMPT>`\n",
    "            * FINE_TUNED_MODEL is the customized GPT-3 model\n",
    "            * YOUR_PROMPT is a new prompt you would like evaluated by your fine-tuned model\n",
    "            \n",
    "            \n",
    "### Tokens\n",
    "\n",
    "* **Tokens** are numerical representations of words or characters\n",
    "    * the smallest fungible unit used by OpenAI to determine pricing for API calls\n",
    "    * one token ~ 4 characters\n",
    "    * OpenAI imposes a limit of 2,048 tokens for prompts and completions\n",
    "    * [Tokenizer Tool](https://beta.openai.com/tokenizer)  \n",
    "    \n",
    "    \n",
    "### Pricing\n",
    "\n",
    "* Cloud pricing model (\"pay as you go\")\n",
    "* Monitor token usage and costs on account 'Usage' dashboard  \n",
    "\n",
    "\n",
    "|            Model           |  Price |\n",
    "|:--------------------------:|:------:|\n",
    "| Davinci<br>(most powerful) | 0.0600 |\n",
    "|            Curie           | 0.0060 |\n",
    "|           Babbage          | 0.0012 |\n",
    "|      Ada<br>(fastest)      | 0.0008 |\n",
    "\n",
    "\n",
    "### GPT-3's Performance on Conventional NLP Tasks\n",
    "* Text Classification - categorizing text into organized groups (ex: semantic analysis)\n",
    "* Zero-shot classification - classification task where no prior training or fine-tuning on labeled data is required for the model to classify a piece of text.\n",
    "* Single-shot ad few-shot classification - fine-tuning an AI-model on a single or a few training examples\n",
    "* Batch Classification - classify multiple/numerous input samples in batches in a single API call \n",
    "* Named Entity Recognition (NER) - an information extraction task that seeks to locate and classify named entities mentioned in unstructured text into predefined categories such as person names, organizations, locations etc.\n",
    "* Text Summarization - technique for generating a concise and exact summary of lengthy texts while focusing on the sections that convey useful information, without losing the overall meaning\n",
    "* Text Generation - generate textual content that is almost indistinguishable from human-written text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4781b8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 3 Programming with GPT-3\n",
    "\n",
    "### Using the OpenAI API with Python\n",
    "\n",
    "You can pair GPT-3 with Python using the [Chronology library](https://github.com/OthersideAI/chronology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede967f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dependencies\n",
    "from chronological import read_prompt, cleaned_completion, main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649b436c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### GPT-3 Sandbox Powered by Streamlit\n",
    "\n",
    "Step-by-step to build a smol GPT-3 miniapp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b8291a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4 GPT-3 as a Launchpad for Next-Generation Start-ups\n",
    "\n",
    "### Model-as-a-Service\n",
    "* Provide GPT-3 as an API and let any developers build thier own business on top of it\n",
    "* Using the model via a API instead of needing your own GPU makes it cost-effective and accessible for ordinary developers to play with use cases and try new things\n",
    "* [Codex](https://arxiv.org/pdf/2107.03374.pdf)\n",
    "\n",
    "### New Start-up Ecosystem: Case Studies\n",
    "1. **Fable Studio** - AI story teller. GPT-3 authoring works best as a collaborative tool in the hands of a person who knows the art of storytelling and would like to get better results, rather than expecting GPT-3 to do all the work\n",
    "2. **Viable** - GPT-3 data analysis applications. summarizing customer feedback. helping early stage start-up companies find product-market fit using surveys and product roadmaps\n",
    "3. **Chatbot Applications** - Quickchat: a general purpose conversational AI that can talk about any subject.\n",
    "4. **Copysmith** - marketing applications\n",
    "5. **Steography** - automated writing of code documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1775114f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 5 GPT-3 for Corporations\n",
    "\n",
    "* **GitHub Copilot** - automating mundane tasks like writing redundant code and writing unit test cases. Use the source code to generate comments or use the comments to generate source code. in development: Copilot code reviews\n",
    "* **Algolia** - an intelligent, sematics-driven, single-search endpoint for search queries\n",
    "* **Microsoft Azure OpenAI Service** - is the model actually producing things that are relevant when you have automation around? the use of the capability, when it's actually built into an application --is it doing what it's supposed to be doing? the OpenAI API version is more suitable for companies that are exploring their options but don't have any specific project implementation in mind, (...) but when their production needs become more mature and they start to need more compliance, they should consider transitioning to Azure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ab2d56",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 6 Challenges, Controversies and Shortcoming\n",
    "\n",
    "* AI bias being encoded into the model\n",
    "    - the training data: english speaking internet\n",
    "    - society's biases and dominant world views are already encoded inn the training data\n",
    "    - interacting with GPT-3 is like interacting with a skewed subsample of humanity - rife with toxic biases\n",
    "    - [Persistent Anti-Muslim bias in LLMs](https://arxiv.org/pdf/2101.05783.pdf)\n",
    "* low quality content and the spread of misinformation\n",
    "    - GPT-3 often performs like a clever student who hasn't done thier reading trying to bull shit thier way through an exam.\n",
    "    - By posting GPT-3 generated text we're polluting the data for it's future versions\n",
    "* GPR-3's environmental footprint\n",
    "    - Trainig a deep learning model produces 626,000 pounds of CO2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97f65c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 7 Demacratizing Access to AI\n",
    "\n",
    "### No Code? No Problem!\n",
    "* The no-code movement rests upon the fundamental belief that technology should enable an facilitate creation, not act as a barrier to entry for those who want to develop software.\n",
    "### Access to Model-as-a-Service\n",
    "* many people struggle with writing whether it's because of focus or attention span or something else. But they're brilliant thinkers and will benefit from the support in communicating thier thoughts with the help of an AI tool that can help you put words on the page."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chronology",
   "language": "python",
   "name": "chronology"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
