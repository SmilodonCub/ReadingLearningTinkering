{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7731110e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# 🤗 HuggingFace Course\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb2134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72911f0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 🤗 Transformer Models\n",
    "\n",
    "Transformers, why are they so damn cool?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7104cef6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Introduction\n",
    "\n",
    "* Chapters 1-4: introduction of the main concepts of 🤗 Transformers library\n",
    "* Chapters 5-8: teach the basics of 🤗 Datasets and 🤗 Tokenizers + some intro to NLP'\n",
    "* Chapters 9-12: deeper dive to showcase specialized architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cb3da2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Natural Language Processing\n",
    "\n",
    "**NLP**  \n",
    "a list of common tasks:  \n",
    "* classifying whole sentences - sentiment, classification (spam), grammer, etc.\n",
    "* classifying words in a sentence - grammer, POS, entities\n",
    "* generating text content - filling in blacks, sentence completion\n",
    "* extracting an answer form text - given a question + content, find answer\n",
    "* generating a new sentence from text input - summarizing text, translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af24daa6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Tranformers, what can they do?\n",
    "\n",
    "The 🤗 Transformers library provides the functionality to create and use hosted models. The Model Hub contains many thousands of pretrained models that anyone can download and use.  \n",
    "\n",
    "#### Working with `pipelines`\n",
    "\n",
    "`pipelines` connect a model with it's necessary preprocessing and postprocessing steps\n",
    "1. the text is preprocessed into a format the model can understand\n",
    "2. the preprocessed inputs are passed to the model\n",
    "3. the predictions of the model are post-processed, so you can make sense of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8617ce71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0b7bf53fc7446f9d64ab9874ee193d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ddc5cd41a84d5eae7064b06e45d3b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669cd712c71441e2be8ad98560a3cae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ecb35d763049468afc9a56d64c184b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.8835864663124084}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline( \"sentiment-analysis\" )\n",
    "# also available: feature-extraction, fill-mask, named entity recognition, questions-answering,\n",
    "#                 summarization, text-generation, translation, zero-shot classification\n",
    "classifier( \"I've been waiting for a HuggingFace course since last Spring\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265d86d7",
   "metadata": {},
   "source": [
    "**Note** - the model is cached, $\\therefore$ it does not need to be loaded when calling the pipeline object again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37da28ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9996492862701416}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier( \"I'm really excited to take this HuggingFace course\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ee3e9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Taking a look at a few other pipelines:  \n",
    "\n",
    "#### Zero-Shot Classfication\n",
    "\n",
    "classifying texts that have not been labeled. allows you to specify which labels to use for the classification, so that you don't need to rely on the labels of the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0aa6c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli (https://huggingface.co/facebook/bart-large-mnli)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebe0a4229b64989a20c0c59b511e00c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f938cbbd3d1b47c0885f31093a06dff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357e2839b4fc49a9884e31b2e1a4c124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9e18fc9c64248e5a3efe8e9af07ae56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a7c3a2762241d0ab72f69986d58e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83284da09c724c168ff2844a41b3e9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'These are my notes from the HuggingFace online course',\n",
       " 'labels': ['education', 'entertainment', 'science', 'politics'],\n",
       " 'scores': [0.6758780479431152,\n",
       "  0.21396446228027344,\n",
       "  0.08056329190731049,\n",
       "  0.029594212770462036]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline( \"zero-shot-classification\" )\n",
    "classifier( \n",
    "    \"These are my notes from the HuggingFace online course\",\n",
    "    candidate_labels = ['education', 'science', 'politics', 'entertainment']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51a73438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'NASA’s James Webb Space Telescope team fully deployed its 21-foot, gold-coated primary mirror, successfully completing the final stage of all major spacecraft deployments to prepare for science operations.',\n",
       "  'labels': ['science', 'education', 'entertainment', 'politics'],\n",
       "  'scores': [0.972022294998169,\n",
       "   0.011234794743359089,\n",
       "   0.010500448755919933,\n",
       "   0.006242458242923021]},\n",
       " {'sequence': 'The world’s largest and most complex space science telescope will now begin moving its 18 primary mirror segments to align the telescope optics.',\n",
       "  'labels': ['science', 'entertainment', 'education', 'politics'],\n",
       "  'scores': [0.9860273599624634,\n",
       "   0.005517215467989445,\n",
       "   0.005045033525675535,\n",
       "   0.0034104110673069954]}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier( [ 'NASA’s James Webb Space Telescope team fully deployed its 21-foot, gold-coated primary mirror, successfully completing the final stage of all major spacecraft deployments to prepare for science operations.',\n",
    "             'The world’s largest and most complex space science telescope will now begin moving its 18 primary mirror segments to align the telescope optics.'],\n",
    "          candidate_labels = ['education', 'science', 'politics', 'entertainment']\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe861fa",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Text Generation\n",
    "\n",
    "provide a prompt and the model will auto-complete to generate the remaining text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8df9b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515c5b34190e4986b38ca5a42b66167a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256650a3baca483cb627a6b4dc39272b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/523M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81256e1da66442a787cbd52c0b52bf6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a231054450834a85be9a377babad0dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506ea0cd458e417ca7c3ce17f3277d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to understand the principles behind the two-sided nature of human being.'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline( 'text-generation' )\n",
    "generator( 'In this course, we will teach you how to understand' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffbdc586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to understand and avoid some common'},\n",
       " {'generated_text': 'In this course, we will teach you how to understand the fundamentals of data'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator( 'In this course, we will teach you how to understand', num_return_sequences = 2, max_length = 15 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07d4a173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Seeing Slayer live was the greatest battle of the year (and probably of all time). It was so fierce that it was the'},\n",
       " {'generated_text': 'Seeing Slayer live was the greatest experience of all.\" After my introduction, Liu Yao glanced at me. \"We will be running'},\n",
       " {'generated_text': 'Seeing Slayer live was the greatest moment of their lives, and how they felt at that moment — their life that never ends —'},\n",
       " {'generated_text': 'Seeing Slayer live was the greatest fantasy that was left out in the world.\"\\n\\n\"And by the same token, if'},\n",
       " {'generated_text': 'Seeing Slayer live was the greatest accomplishment of her lives.\\n\\nShe had never used any supernatural powers before and could hardly feel'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator( 'Seeing Slayer live was the greatest', num_return_sequences = 5, max_length = 25 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42ba401d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb70915f4c1b4fe782fb731d102ddf4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c9f437ec0b84742b8a41d1731756a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/336M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff4f485b17ac47598f7557d82407d73c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9bfb9e90b5a4a8a892e3d4d75a818bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d96a9a076e459cb275df1072020a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Seeing Slayer live was the greatest show of all time - in 1995, the television giant put back its plans, but that was'},\n",
       " {'generated_text': 'Seeing Slayer live was the greatest day in professional soccer history, with many leagues reaching huge financial heights. To qualify for the World'},\n",
       " {'generated_text': 'Seeing Slayer live was the greatest show ever released in Japan.\\n\\n\\n\\nThe Japanese version of Slayer premiered at the U'},\n",
       " {'generated_text': 'Seeing Slayer live was the greatest success in the world,” said the CEO of the agency.\\n\\n\\n\\n\\n'},\n",
       " {'generated_text': 'Seeing Slayer live was the greatest act of the show!\\nThe cast also talked about their experiences in New York, Los Angeles'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specifying a model from the Hub\n",
    "generator = pipeline('text-generation', model='distilgpt2')\n",
    "generator( 'Seeing Slayer live was the greatest', num_return_sequences = 5, max_length = 25 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3298e9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Mask Filling\n",
    "\n",
    "fill in blanks in a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ab29d21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilroberta-base (https://huggingface.co/distilroberta-base)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897f7b935f7e4a41985608c23ed76139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf3e5fc92b840f1a3a83f44aa047426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/316M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9f713839054b57a18354120da851ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7303243e40394f3db1282e633ac51299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c14b04100845f580f058623b7b1bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'This course will teach you all about mathematical models',\n",
       "  'score': 0.19631721079349518,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical'},\n",
       " {'sequence': 'This course will teach you all about building models',\n",
       "  'score': 0.04449249804019928,\n",
       "  'token': 745,\n",
       "  'token_str': ' building'},\n",
       " {'sequence': 'This course will teach you all about predictive models',\n",
       "  'score': 0.039371054619550705,\n",
       "  'token': 27930,\n",
       "  'token_str': ' predictive'},\n",
       " {'sequence': 'This course will teach you all about role models',\n",
       "  'score': 0.03575519099831581,\n",
       "  'token': 774,\n",
       "  'token_str': ' role'},\n",
       " {'sequence': 'This course will teach you all about business models',\n",
       "  'score': 0.027736075222492218,\n",
       "  'token': 265,\n",
       "  'token_str': ' business'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline( 'fill-mask' )\n",
    "unmasker( 'This course will teach you all about <mask> models', top_k=5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5e6b4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5bd18dcc8774066b20d12512c9d2f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ee0c1512d24da5aa70abdea262f4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/416M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a673ee18bf43efa676fceb8d7e6e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee65613a515472f85bc7e7f9948e1bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f59cfb4fb542b3aba3fbefdb027399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'This course will teach you all about role models',\n",
       "  'score': 0.07605545967817307,\n",
       "  'token': 1648,\n",
       "  'token_str': 'role'},\n",
       " {'sequence': 'This course will teach you all about the models',\n",
       "  'score': 0.05633450299501419,\n",
       "  'token': 1103,\n",
       "  'token_str': 'the'},\n",
       " {'sequence': 'This course will teach you all about fashion models',\n",
       "  'score': 0.04619521275162697,\n",
       "  'token': 4633,\n",
       "  'token_str': 'fashion'},\n",
       " {'sequence': 'This course will teach you all about computer models',\n",
       "  'score': 0.030348550528287888,\n",
       "  'token': 2775,\n",
       "  'token_str': 'computer'},\n",
       " {'sequence': 'This course will teach you all about life models',\n",
       "  'score': 0.019965192303061485,\n",
       "  'token': 1297,\n",
       "  'token_str': 'life'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline( 'fill-mask', model = 'bert-base-cased' )\n",
    "unmasker( 'This course will teach you all about [MASK] models', top_k=5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dc5e3a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Named Entity Recognition\n",
    "\n",
    "**NER** - find which parts of an input text correspond to entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89bc38e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74cae1d6ba2045dbad0ddd1866b66d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af1af967d1514f55847eb8288ba20d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8084779330b242c7a2b7fb67c16a69e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3566ac2a3384257a61c4b1e67c28518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bonzilla/anaconda3/lib/python3.7/site-packages/transformers/pipelines/token_classification.py:129: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  f'`grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"{aggregation_strategy}\"` instead.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99946195,\n",
       "  'word': 'Adams',\n",
       "  'start': 67,\n",
       "  'end': 72},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.99552476,\n",
       "  'word': 'CNN',\n",
       "  'start': 78,\n",
       "  'end': 81},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.8375926,\n",
       "  'word': 'State of the Union',\n",
       "  'start': 85,\n",
       "  'end': 103},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.99927413,\n",
       "  'word': 'Bernard Williams',\n",
       "  'start': 141,\n",
       "  'end': 157},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9963703,\n",
       "  'word': 'NYPD',\n",
       "  'start': 167,\n",
       "  'end': 171}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline( \"ner\", grouped_entities=True )\n",
    "ner( \"Let me be clear on this: My brother is qualified for the position, Adams told CNN’s 'State of the Union' when asked about making his sibling Bernard Williams a deputy NYPD commissioner.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b844ae27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1be77ced98247588ee6c9b2a5ba77b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/326 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ca1e0e8aa5491bbcc2668d223dc1bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/780k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4b665bc8a3b41529fb9b9cf839c4bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f993cdcacd68488f86663ab8937d908e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2fc77cb91d48dd8d4650f0a73252b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218d035e76e541f2949b8579a29d2192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30cd778e719645e9807bd4d44a5a06eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'VERB',\n",
       "  'score': 0.9986479,\n",
       "  'word': ' Let',\n",
       "  'start': 1,\n",
       "  'end': 3},\n",
       " {'entity_group': 'PRON',\n",
       "  'score': 0.99980384,\n",
       "  'word': ' me',\n",
       "  'start': 4,\n",
       "  'end': 6},\n",
       " {'entity_group': 'AUX',\n",
       "  'score': 0.98620975,\n",
       "  'word': ' be',\n",
       "  'start': 7,\n",
       "  'end': 9},\n",
       " {'entity_group': 'ADJ',\n",
       "  'score': 0.9991356,\n",
       "  'word': ' clear',\n",
       "  'start': 10,\n",
       "  'end': 15},\n",
       " {'entity_group': 'ADP',\n",
       "  'score': 0.99943495,\n",
       "  'word': ' on',\n",
       "  'start': 16,\n",
       "  'end': 18},\n",
       " {'entity_group': 'PRON',\n",
       "  'score': 0.99961954,\n",
       "  'word': ' this',\n",
       "  'start': 19,\n",
       "  'end': 23},\n",
       " {'entity_group': 'PUNCT',\n",
       "  'score': 0.93715745,\n",
       "  'word': ':',\n",
       "  'start': 23,\n",
       "  'end': 24},\n",
       " {'entity_group': 'PRON',\n",
       "  'score': 0.99976885,\n",
       "  'word': ' My',\n",
       "  'start': 25,\n",
       "  'end': 27},\n",
       " {'entity_group': 'NOUN',\n",
       "  'score': 0.9995938,\n",
       "  'word': ' brother',\n",
       "  'start': 28,\n",
       "  'end': 35},\n",
       " {'entity_group': 'AUX',\n",
       "  'score': 0.99777746,\n",
       "  'word': ' is',\n",
       "  'start': 36,\n",
       "  'end': 38},\n",
       " {'entity_group': 'ADJ',\n",
       "  'score': 0.9990982,\n",
       "  'word': ' qualified',\n",
       "  'start': 39,\n",
       "  'end': 48},\n",
       " {'entity_group': 'ADP',\n",
       "  'score': 0.9997824,\n",
       "  'word': ' for',\n",
       "  'start': 49,\n",
       "  'end': 52},\n",
       " {'entity_group': 'DET',\n",
       "  'score': 0.9999352,\n",
       "  'word': ' the',\n",
       "  'start': 53,\n",
       "  'end': 56},\n",
       " {'entity_group': 'NOUN',\n",
       "  'score': 0.9997966,\n",
       "  'word': ' position',\n",
       "  'start': 57,\n",
       "  'end': 65},\n",
       " {'entity_group': 'PUNCT',\n",
       "  'score': 0.7442607,\n",
       "  'word': ',',\n",
       "  'start': 65,\n",
       "  'end': 66},\n",
       " {'entity_group': 'PROPN',\n",
       "  'score': 0.99950045,\n",
       "  'word': ' Adams',\n",
       "  'start': 67,\n",
       "  'end': 72},\n",
       " {'entity_group': 'VERB',\n",
       "  'score': 0.999692,\n",
       "  'word': ' told',\n",
       "  'start': 73,\n",
       "  'end': 77},\n",
       " {'entity_group': 'PROPN',\n",
       "  'score': 0.99951416,\n",
       "  'word': ' CNN',\n",
       "  'start': 78,\n",
       "  'end': 81},\n",
       " {'entity_group': 'DET',\n",
       "  'score': 0.82623136,\n",
       "  'word': '�',\n",
       "  'start': 81,\n",
       "  'end': 82},\n",
       " {'entity_group': 'PART',\n",
       "  'score': 0.9806938,\n",
       "  'word': '�',\n",
       "  'start': 81,\n",
       "  'end': 82},\n",
       " {'entity_group': 'ADP',\n",
       "  'score': 0.5717784,\n",
       "  'word': 's',\n",
       "  'start': 82,\n",
       "  'end': 83},\n",
       " {'entity_group': 'PUNCT',\n",
       "  'score': 0.9272147,\n",
       "  'word': \" '\",\n",
       "  'start': 84,\n",
       "  'end': 85},\n",
       " {'entity_group': 'NOUN',\n",
       "  'score': 0.71977973,\n",
       "  'word': 'State',\n",
       "  'start': 85,\n",
       "  'end': 90},\n",
       " {'entity_group': 'ADP',\n",
       "  'score': 0.9996913,\n",
       "  'word': ' of',\n",
       "  'start': 91,\n",
       "  'end': 93},\n",
       " {'entity_group': 'DET',\n",
       "  'score': 0.9996956,\n",
       "  'word': ' the',\n",
       "  'start': 94,\n",
       "  'end': 97},\n",
       " {'entity_group': 'PROPN',\n",
       "  'score': 0.6245964,\n",
       "  'word': ' Union',\n",
       "  'start': 98,\n",
       "  'end': 103},\n",
       " {'entity_group': 'PUNCT',\n",
       "  'score': 0.99572575,\n",
       "  'word': \"'\",\n",
       "  'start': 103,\n",
       "  'end': 104},\n",
       " {'entity_group': 'SCONJ',\n",
       "  'score': 0.9991322,\n",
       "  'word': ' when',\n",
       "  'start': 105,\n",
       "  'end': 109},\n",
       " {'entity_group': 'VERB',\n",
       "  'score': 0.9997819,\n",
       "  'word': ' asked',\n",
       "  'start': 110,\n",
       "  'end': 115},\n",
       " {'entity_group': 'SCONJ',\n",
       "  'score': 0.9990184,\n",
       "  'word': ' about',\n",
       "  'start': 116,\n",
       "  'end': 121},\n",
       " {'entity_group': 'VERB',\n",
       "  'score': 0.9998317,\n",
       "  'word': ' making',\n",
       "  'start': 122,\n",
       "  'end': 128},\n",
       " {'entity_group': 'PRON',\n",
       "  'score': 0.9997869,\n",
       "  'word': ' his',\n",
       "  'start': 129,\n",
       "  'end': 132},\n",
       " {'entity_group': 'NOUN',\n",
       "  'score': 0.99957514,\n",
       "  'word': ' sibling',\n",
       "  'start': 133,\n",
       "  'end': 140},\n",
       " {'entity_group': 'PROPN',\n",
       "  'score': 0.9996966,\n",
       "  'word': ' Bernard Williams',\n",
       "  'start': 141,\n",
       "  'end': 157},\n",
       " {'entity_group': 'DET',\n",
       "  'score': 0.99989897,\n",
       "  'word': ' a',\n",
       "  'start': 158,\n",
       "  'end': 159},\n",
       " {'entity_group': 'NOUN',\n",
       "  'score': 0.7097997,\n",
       "  'word': ' deputy',\n",
       "  'start': 160,\n",
       "  'end': 166},\n",
       " {'entity_group': 'PROPN',\n",
       "  'score': 0.99958587,\n",
       "  'word': ' NYPD',\n",
       "  'start': 167,\n",
       "  'end': 171},\n",
       " {'entity_group': 'NOUN',\n",
       "  'score': 0.5886441,\n",
       "  'word': ' commissioner.',\n",
       "  'start': 172,\n",
       "  'end': 185}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS tagging\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vblagoje/roberta-base-english-upos\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"vblagoje/roberta-base-english-upos\")\n",
    "ner = pipeline( \"ner\", model = model, tokenizer=tokenizer, grouped_entities=True )\n",
    "ner( \"Let me be clear on this: My brother is qualified for the position, Adams told CNN’s 'State of the Union' when asked about making his sibling Bernard Williams a deputy NYPD commissioner.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcdab81",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c709f9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad (https://huggingface.co/distilbert-base-cased-distilled-squad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.38330498337745667, 'start': 45, 'end': 53, 'answer': 'Brooklyn'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_answering = pipeline( 'question-answering' )\n",
    "q_answering( \n",
    "    question = 'Where does Zach work?',\n",
    "    context = 'Zach is a bike messenger in NYC and lives in Brooklyn'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662204ac",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Summarization\n",
    "\n",
    "distilling text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "88f0dfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 (https://huggingface.co/sshleifer/distilbart-cnn-12-6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f447513a784c538621113b85851fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6039721128b742a79a35265efe4deeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.14G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5edf7e3624c041208f899b9491fc7bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a760f97d6e284de683f35b41dfcfa528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb672ba76d846049ed84bfde36bcfb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' The Witcher is a fantasy drama based on the book series of the same name by Polish writer Andrzej Sapkowski . Set on a fictional, medieval-inspired landmass known as \"the Continent\", The Witcher explores the legend of Geralt of Rivia and Princess Ciri, who are linked to each other by destiny . The first season consisted of eight episodes and was released on Netflix in its entirety on December 20, 2019 .'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline( 'summarization' )\n",
    "summarizer( \"\"\"The Witcher is a fantasy drama streaming television series created by Lauren Schmidt Hissrich, fan fiction based on the book series of the same name by Polish writer Andrzej Sapkowski. Set on a fictional, medieval-inspired landmass known as \"the Continent\", The Witcher explores the legend of Geralt of Rivia and Princess Ciri, who are linked to each other by destiny. It stars Henry Cavill, Freya Allan and Anya Chalotra.\n",
    "\n",
    "The first season consisted of eight episodes and was released on Netflix in its entirety on December 20, 2019. It was based on The Last Wish and Sword of Destiny, which are collections of short stories that precede the main Witcher saga. The second season, consisting of eight episodes, was released on December 17, 2021. In September 2021, Netflix renewed the series for a third season. An animated origin story film, The Witcher: Nightmare of the Wolf, was released on August 23, 2021, while a prequel miniseries, The Witcher: Blood Origin, will be released in 2022.\n",
    "\n",
    "The story begins by following three main characters: the witcher Geralt of Rivia; crown princess Cirilla of Cintra; and the sorceress Yennefer of Vengerberg, meeting them at different points in time. A witcher is a human trained and magically mutated since childhood to be strong and resilient enough to fight the many monsters that plague The Continent. The first season bounces the viewer back and forth in time, exploring formative events that shape the main characters before eventually merging into a single timeline.\n",
    "\n",
    "Yennefer and Geralt encounter each other several times across many decades, as both are magic-users with unnaturally long lives. They clash more than once and are on-and-off lovers, both of them longing for more but afraid to admit it.\n",
    "\n",
    "Meanwhile, Geralt and princess Cirilla are linked by destiny before she is born: for saving her father's life, Geralt elected to be paid by invoking \"the Law of Surprise\", a custom in which the one who was saved gives his savior the next thing he learns that he possesses, but did NOT know about at the time his life was saved. After promising Geralt the surprise, Cirilla's father Duny learns that his bride is pregnant. Thus, the unborn princess Ciri was bound to the witcher in an unbreakable pact of destiny. Geralt wants nothing to do with a child and goes on his solitary way. After the two finally meet, when the 12-year-old princess has been orphaned by war, the witcher becomes her protector; he must keep her safe and fight against many pursuers to prevent Ciri's powerful magic from being used to destroy the world.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d73eca",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "41dafa0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'This course is produced by HuggingFace'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline( 'translation', model = 'Helsinki-NLP/opus-mt-fr-en' )\n",
    "translator('Ce cours est produit par HuggingFace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04660306",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### How do Transformers work?\n",
    "\n",
    "Transformers were introduced in June 2017  \n",
    "They can be broadly grouped into 3 categories: \n",
    "* GPT-like - autoregressive\n",
    "* BERT-like - autoencoding\n",
    "* BART/T5-like - seq2seq \n",
    "\n",
    "These models learned language by **self-supervised learning** - automatically compute from inputs to the model to develop a statistical understanding of the language.  \n",
    "Transformers models are very broad and not very useful for specific tasks. However, they can be used as a base case and be fine-tuned using supervised learning for a specific task == **transfer learning**  \n",
    "\n",
    "\n",
    "Large Models have a big carbon footprint  \n",
    "Several things to consider:\n",
    "* use pretrained models when they are available\n",
    "* use fine-tuning vs. from scratch\n",
    "* starting with smaller experiments and then debugging\n",
    "* doing literature review to choose hyperparameter ranges\n",
    "* random seach vs grid search\n",
    "\n",
    "[**Machine Learning Emissions Calculator**](https://mlco2.github.io/impact/)  \n",
    "also codecarbon  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5dd5450d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class EmissionsTracker in module codecarbon.emissions_tracker:\n",
      "\n",
      "class EmissionsTracker(BaseEmissionsTracker)\n",
      " |  EmissionsTracker(project_name: str = 'codecarbon', measure_power_secs: int = 15, output_dir: str = '.', save_to_file: bool = True, gpu_ids: Union[List, NoneType] = None, emissions_endpoint: Union[str, NoneType] = None, co2_signal_api_token: Union[str, NoneType] = None)\n",
      " |  \n",
      " |  An online emissions tracker that auto infers geographical location,\n",
      " |  using `geojs` API\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      EmissionsTracker\n",
      " |      BaseEmissionsTracker\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseEmissionsTracker:\n",
      " |  \n",
      " |  __init__(self, project_name: str = 'codecarbon', measure_power_secs: int = 15, output_dir: str = '.', save_to_file: bool = True, gpu_ids: Union[List, NoneType] = None, emissions_endpoint: Union[str, NoneType] = None, co2_signal_api_token: Union[str, NoneType] = None)\n",
      " |      :param project_name: Project name for current experiment run, default name\n",
      " |                           as \"codecarbon\"\n",
      " |      :param measure_power_secs: Interval (in seconds) to measure hardware power\n",
      " |                                 usage, defaults to 15\n",
      " |      :param output_dir: Directory path to which the experiment details are logged\n",
      " |                         in a CSV file called `emissions.csv`, defaults to current\n",
      " |                         directory\n",
      " |      :param save_to_file: Indicates if the emission artifacts should be logged to a\n",
      " |                           file, defaults to True\n",
      " |      :param gpu_ids: User-specified known gpu ids to track, defaults to None\n",
      " |      :param emissions_endpoint: Optional URL of http endpoint for sending emissions data\n",
      " |      :param co2_signal_api_token: API token for co2signal.com (requires sign-up for free beta)\n",
      " |  \n",
      " |  start(self) -> None\n",
      " |      Starts tracking the experiment.\n",
      " |      Currently, Nvidia GPUs are supported.\n",
      " |      :return: None\n",
      " |  \n",
      " |  stop(self) -> Union[float, NoneType]\n",
      " |      Stops tracking the experiment\n",
      " |      :return: CO2 emissions in kgs\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseEmissionsTracker:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#pip install codecarbon\n",
    "from codecarbon import EmissionsTracker\n",
    "help( EmissionsTracker )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55be4a74",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Sharing language models is paramount: sharing the trained weights and building on top of already trained weights reduces the overall compute cost and carbon footprint of the community.  \n",
    "\n",
    "#### Transfer Learning\n",
    "\n",
    "**Transfer Learning** - Transferring the knowledge of a pretrained model to a new model by initializing the second model with the first model's weights.  \n",
    "Leverage the knowledge gained from training a model on a very large dataset on another task.  \n",
    "Initialize a model with the same weights as a pretrained model, thus transfering the knowledge.  \n",
    "**pre-training** - the act of trainign a model from scratch: the weights are started at random and the training starts without any prior knowledge. pre-training si usually done on very large training data sets.  \n",
    "**fine-tuning** - training done after a model has been pretrained typically with a comparatively smaller dataset  \n",
    "\n",
    "fine-tuning has lower time, data, financial and environmental costs. However, the resulting model inherits any bias present from the pretrained model.  \n",
    "\n",
    "#### General Archtecture\n",
    "\n",
    "* **Encoder** - acquires and interprets input: receives input and builds representation of its features\n",
    "* **Decoder** - generates output: ises the encoder's representation (features) along with other inputs to generate a target sequence\n",
    "* Types of Models:\n",
    "    * **Encoder-only models** - tasks for understanding the input (classification, NER)\n",
    "    * **Decoder-only models** - generative tasks (text generation)\n",
    "    * **Encoder-decoder models** - Sequence-to-sequence models; good for generative tasks that require an input (translation, summarization)\n",
    "* **Attention Layers** - tells the model to pay specific attention to certain words in the sentence it was passed; a word by itself has meaning, but that meaning is deeply affected by the context, which can be any other word (or words) before or after the wordbeing studied.  \n",
    "* **Architecture** - the skeleton of the model. An architecture is a succession of mathematical functions to build a model and its weights are those functions parameters.\n",
    "* **Checkpoints** - the weights that will be lloaded by a given architecture\n",
    "* **Model** - umbrella term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6dca6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Encoder Models\n",
    "\n",
    "* Encoder models use only the encoder of a Transformer model\n",
    "* **feature vector/tensor** - holds the meaning of the word within the text. retrieves a numerical vector representation of each word with dimensions defined by the architecture of the model. each numerical representation is contexualized: each word in the initial sequence affects every word's representation\n",
    "* **\"Bi-directional\" attention** - the vectorization takes into account tokens both befor and after\n",
    "* **auto-encoding models**\n",
    "* Examples: \n",
    "    - ALBERT\n",
    "    - BERT\n",
    "    - DistilBERT\n",
    "    - ELECTRA\n",
    "    - RoBERTa\n",
    "* When to use an Encoder Model:\n",
    "    - sequence classification (sentiment analysis), question answering, masked language modeling\n",
    "    - NLU: Natural Language Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212cc44c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Decoder Models\n",
    "\n",
    "* Decoder Models use only the Decoder of a Transformer Model\n",
    "* **Auto-regressive** - attention layers can only access the words positioned before it in a sentence\n",
    "* **feature vector/tensor** - one vector per word with length determined by the model architecture differs from Encoder models by its self attention mechnanism\n",
    "* **masked self-attention** - the tokens to the right or left of the word are masked; feature vector is decided by one-directional context\n",
    "* best suited for text generation tasks\n",
    "* Examples:\n",
    "    - CTRL\n",
    "    - GPT\n",
    "    - GPT-2\n",
    "    - Transformer XL\n",
    "* when to use Decoder Model:\n",
    "    - Causal Language Modeling: great at generational tests: generating sequences of text\n",
    "    - NLG: Natural Language Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd793328",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Sequence-to-sequence Models\n",
    "\n",
    "* Encoder-Decoder Models use both parts of the Transformers architecture.\n",
    "* Works by:\n",
    "    - the encoder takes care of understanding the sequence\n",
    "    - takes the numerical representation from the Encoder (meaning of the sequence)\n",
    "    - pass to the Decoder along with a token to start a sequence\n",
    "    - the Decoder takes care of generating a sequence according to the understanding of the Encoder\n",
    "    - the Decoder will try to sequencially decode the Encoder's output as a 'word'\n",
    "    - Decoder works in a auto-regressive way to add tokens until a stop is encountered\n",
    "* Are best for tasks revolving around generation of new text which depends on context: \n",
    "    - summarization\n",
    "    - traslation\n",
    "    - generative question answering\n",
    "* Examples: \n",
    "    - BART\n",
    "    - Marian\n",
    "    - T5\n",
    "* When to use an Encoder-Decoder Model?\n",
    "    - for Sequence-to-Sequence tasks: translation, summarization, many-to-many\n",
    "    - the weights between Encoder & Decoder are not necesarily shared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc367cda",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Bias and limitations\n",
    "\n",
    "To enable pretraining on large amounts of data, researchers often scrape all the content they can find, taking the best as well as the worst of what is available on the internet. Therefore, it is important to keep in mind that the original model you are using could very easily generate sexist, racist, or homophobic content. Fine-tuning the model on your data won't make this intrinsic bias disappear.  \n",
    "\n",
    "Possible sources of bias observed in a model:\n",
    "* The model is a fine-tuned  version of a pretrained model and it picked up its bias from it\n",
    "* The data the model was trained on is biased\n",
    "* The metric the model was optimized for is biased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5f53d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Summary\n",
    "\n",
    "|     **Model**    |                **Examples**                |                           **Tasks**                          |\n",
    "|:----------------:|:------------------------------------------:|:------------------------------------------------------------:|\n",
    "|      Encoder     | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | Sentence classification, NER,  Ectractive question answering |\n",
    "|      Decoder     |       CTRL, GPT, GPT-1 Transformer XL      |                        Text generation                       |\n",
    "| Encoder- Decoder |           BART, T5, Marian, mBART          | Summarization, Translation, Generative question answering    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec5d1b4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### End of Chapter Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "105d5d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english)\n",
      "/home/bonzilla/anaconda3/lib/python3.7/site-packages/transformers/pipelines/token_classification.py:129: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  f'`grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"{aggregation_strategy}\"` instead.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99876773,\n",
       "  'word': 'Sylvian',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9672198,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9846444,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline( 'ner', grouped_entities=True )\n",
    "ner( 'My name is Sylvian and I work at Hugging Face in Brooklyn' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7cffdea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'This man has been waiting for you.',\n",
       "  'score': 0.09050368517637253,\n",
       "  'token': 1299,\n",
       "  'token_str': 'man'},\n",
       " {'sequence': 'This place has been waiting for you.',\n",
       "  'score': 0.07195132970809937,\n",
       "  'token': 1282,\n",
       "  'token_str': 'place'},\n",
       " {'sequence': 'This world has been waiting for you.',\n",
       "  'score': 0.05576983094215393,\n",
       "  'token': 1362,\n",
       "  'token_str': 'world'},\n",
       " {'sequence': 'This one has been waiting for you.',\n",
       "  'score': 0.04573335126042366,\n",
       "  'token': 1141,\n",
       "  'token_str': 'one'},\n",
       " {'sequence': 'This woman has been waiting for you.',\n",
       "  'score': 0.03509223833680153,\n",
       "  'token': 1590,\n",
       "  'token_str': 'woman'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filler = pipeline(\"fill-mask\", model=\"bert-base-cased\")\n",
    "result = filler(\"This [MASK] has been waiting for you.\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b36a05ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli (https://huggingface.co/facebook/bart-large-mnli)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['entertainment', 'education', 'science', 'politics'],\n",
       " 'scores': [0.4878741204738617,\n",
       "  0.4179564118385315,\n",
       "  0.07267913222312927,\n",
       "  0.021490389481186867]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "result = classifier(\"This is a course about the Transformers library\",\n",
    "                   candidate_labels = ['education', 'science', 'politics', 'entertainment'])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77dcf1e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Using 🤗 Transformers\n",
    "\n",
    "Transformers are hard. How can their API be so simple?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d9d2db",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The 🤗 Transformers library: a single API through which any Transformer model can be loaded, trained, and saved.\n",
    "* **Easy to use** - download, load, use\n",
    "* **Flexibility** - models are either `PyTorch nn.Module` or `TensorFlow tf.keras.Model` classes\n",
    "* **Simplicity** - \"All in one file\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a95fdcf",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Behind the Pipeline\n",
    "\n",
    "Let's review the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3b3a788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.951606810092926},\n",
       " {'label': 'POSITIVE', 'score': 0.999339759349823}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline( \"sentiment-analysis\" )\n",
    "classifier( [ \"I've been waiting for a HuggingFace course my whole life\",\n",
    "            \"I hat this so much!\" ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa313f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "the `pipeline` groups together three steps:  \n",
    "1. preprocessing (tokenization)\n",
    "2. passing the inputs through the model\n",
    "3. postprocessing\n",
    "\n",
    "<img src=\"https://huggingface.co/course/static/chapter2/full_nlp_pipeline.png\" style=\"margin-left:auto; margin-right:auto\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100a0c82",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Let's review these terms...\n",
    "\n",
    "#### (1) Preprocessing with a tokenizer\n",
    "\n",
    "**tokenization** - convert text inputs into numbers that the model can interpret\n",
    "* split the text into **tokens** - words, subwords or symbols\n",
    "* map each token to an integer\n",
    "* adding additional inputs\n",
    "\n",
    "Preprocessing needs to be done the same way as when the model was pretrained.  \n",
    "This can be implemented with the `from_pretrained()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21256fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased-finetuned-sst-2-english', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "# \"distilbert-base-uncased-finetuned-sst-2-english\" is the default model for 'sentiment-analysis'\n",
    "tokenizer = AutoTokenizer.from_pretrained( checkpoint )\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da313b44",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "pass sentences to the tokenizer and get a dictionary for tensors back. Transformers models expect tensors, but these can be PyTorch, TensorFlow, numpy etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6357d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,   102],\n",
      "        [  101,  1045,  6045,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [ \"I've been waiting for a HuggingFace course my whole life\",\n",
    "            \"I hat this so much!\" ]\n",
    "inputs = tokenizer( raw_inputs, padding = True, truncation = True, return_tensors = 'pt' )\n",
    "print( inputs )\n",
    "print( inputs.keys() )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d0f39",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The output is a dictionary.  \n",
    "* **input_ids** unique integer identifiers of the tokens in each sentence\n",
    "* **attention_mask** tells the model which tokens to 'pay attention' to\n",
    "\n",
    "\n",
    "#### (2) Passing Inputs Through the Model\n",
    "\n",
    "Download a pretrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d1afa242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertModel(\n",
      "  (embeddings): Embeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layer): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "# use the same checkpoint\n",
    "model = AutoModel.from_pretrained( checkpoint )\n",
    "print( model )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70bf899",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "When given some inputs, this models architecture returns as output the **hidden states**, or features: a high-dimensional vector representing the contexual understanding of the input. Hidden states can be used as inputs to a different model layer, the head, to perform specific NLP tasks.  \n",
    "\n",
    "Hidden State Dimensions\n",
    "* **batch Size** number of sequences processed at the same time\n",
    "* **sequence length** length of the sequences being processed\n",
    "* **hidden state** vector dimension of each model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bee7087c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 15, 768])\n"
     ]
    }
   ],
   "source": [
    "outputs = model( **inputs )\n",
    "print( outputs.last_hidden_state.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ad1bb",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The hidden states are passed to the model **Head**  \n",
    "Model Heads - take the high-dimensional hidden states vectors and project onto a different dimension for output  \n",
    "For example, in the following classification example, the output is mapped onto the number of sequences and the number of classification labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "27f2ffce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.4683,  1.5105],\n",
      "        [-3.4842,  3.8381]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)\n",
      "Output shape: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# using the same checkpoint\n",
    "model = AutoModelForSequenceClassification.from_pretrained( checkpoint )\n",
    "outputs = model( **inputs )\n",
    "print( outputs )\n",
    "print( 'Output shape:', outputs.logits.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b42631",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### (3) Postprocessing the Output\n",
    "\n",
    "Values that come out of the model head do not make sense to a hooman:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d39683f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4683,  1.5105],\n",
      "        [-3.4842,  3.8381]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print( outputs.logits )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a04fba",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The outputs are **logits** - the raw unnormalized score from the last model layer, not probabilities.  \n",
    "To return a probability, the logits need to be passed through a **Softmax** layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f6128d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.8393e-02, 9.5161e-01],\n",
      "        [6.6023e-04, 9.9934e-01]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax( outputs.logits, dim=-1 )\n",
    "print( predictions )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98e5626",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now to return the predicted label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "701f5baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0883f382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9936e-01, 6.4109e-04],\n",
      "        [9.9908e-01, 9.2453e-04],\n",
      "        [3.7674e-04, 9.9962e-01],\n",
      "        [1.9647e-04, 9.9980e-01],\n",
      "        [8.8015e-03, 9.9120e-01],\n",
      "        [9.9797e-01, 2.0295e-03]], grad_fn=<SoftmaxBackward>)\n",
      "{0: 'NEGATIVE', 1: 'POSITIVE'}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [ \"Let me be clear, this is an animated series and NOT anime. Disclaimer (I enjoyed R. Armitage's brilliant performance in North and South). As for Castlevania, I've tried to watch it and in my desperation for some distraction may even watch season 3, which I hear may be promising. But each episode in Seasons 1 and 2 have been like a punishment. So incredibly dull! The only interesting character is Alucard, the beautiful dhampir son of Dracula. Why couldn't this series be more interesting? There's so much potential to make it a truly livening experience. The pacing is awful—each scene drags on with the sloppiness of a messy effort at storytelling. It's too dialog heavy, wit sounds like farce, fights look like a Saturday morning special. What happened here is a miserable attempt at animating a video game series to draw in a new audience who may be blown away by the animation style but have yet to experience the true genius of some fantastic Japanese anime gems.\",\n",
    "             \"Season 1 was awesome, season 2 was AWESOME. Then came season 3 and again Netflix ruined what could have been a series for years. Castlevania is a game, one of the most brutal games you could play. It has material to make stories until we die. If I wanted to watch porno there are sites for that. If I wanted to follow an agenda (lgtb whatever), for that we have Disney. Stop damaging and ruined series the way you are doing it Netflix. Hate what you did to one of the main characters of the series.\",\n",
    "             \"Incredible show. As someone who is usually not into Anime or at least incredibly picky about the shows I actually do watch to the very end instead of dropping them after the first few episodes, I have to say that this adaptation is likely going to be the best we're going to see of the castlevania franchise for a long long time.\",\n",
    "             \"This is by far my favorite animated series ever.  I absolutely cannot get enough.  I think I must have watched it 6 times over.  The characters are sensationally well rounded.  Trevor and Sypha*, Alucard(Dracula's half human son), Issac is especially interesting.  The action is simply on a level of its own.  A good example of this would be Season 2 Episode 7.  It is incredible.  The studio that made this also made 'Blood of Zeus' which is on Netflix.  Blood of Z is not quite as good as Castlevania but still worth while.  I cannot wait for season 4 and hopefully many more seasons after that of Castlevania.  Its the best.\",\n",
    "             \"The show's writing and dialogue are intelligent and witty, and the voice cast is great. But, sadly, by the end of season one, you realize the show succumbs to lambasting male characters as too many other Netflix shows have trended (like the horrific new outing of She-Man, uh, He-Man Mistress of the Universe). Why is this wrong? Well, let's substitute any other gender or race into the category of who gets lambasted or belittled, and see how that goes. If you can ignore the discrimination, it's actually a good show with an interesting story and great dialogue.\",\n",
    "             \"Although overall anime is good. It feels like something is missing. I think this series feels more like a animated version of a Hollywood movie (I think it should be called animated series not Anime)  than Japanese style Anime. If you want deep and profound Anime then this is totally not for you.\"]\n",
    "inputs = tokenizer( raw_inputs, padding = True, truncation = True, return_tensors = 'pt' )\n",
    "model = AutoModelForSequenceClassification.from_pretrained( checkpoint )\n",
    "outputs = model( **inputs )\n",
    "predictions = torch.nn.functional.softmax( outputs.logits, dim=-1 )\n",
    "print(predictions)\n",
    "print( model.config.id2label )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0114cff",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Models\n",
    "\n",
    "Creating and using a model  \n",
    "Instantiating an existing model from a checkpoint\n",
    "\n",
    "\n",
    "#### Creating a Transformer\n",
    "\n",
    "Here we will create a BERT model from the default configuration. this will initialize it with random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9dba2633",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize a BERT model\n",
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "config = BertConfig()\n",
    "model = BertModel( config )\n",
    "print( config )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d80207",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The configuration has many different attributes.  \n",
    "Again, the values are all initialized to a random state, $\\therefore$ the model will output gibberish because it is untrained.  \n",
    "\n",
    "Alternatively, we can load pretrained models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02377b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained( 'bert-base-cased' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a028d95e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The model is now initialized with the weights of the pretrained BERT model\n",
    "\n",
    "\n",
    "#### Saving Methods\n",
    "\n",
    "models can be saved to a folder with a .json file of architecture parameters and a .bin file state dictionary of the models weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa22c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving a model\n",
    "model.save_pretrained( \"place_to_call_home\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4466eea",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Using a Transformer Model for Inference\n",
    "\n",
    "Transformer models can only process numbers - numbers that the tokenizer generates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d787f1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Tokenizers\n",
    "\n",
    "**tokenizers** - translate text into numerical data that can be processed by the model. a balancing act: find the representation that conveys the more meaning and the most sense to the model while fitting to the smallest representation possible.  \n",
    "\n",
    "Types of Tokenization\n",
    "* **Word-based** \n",
    "    - split raw text into words and find a numeric representation \n",
    "    - **splitting text** - split on whitespace, special rules for punctuation\n",
    "    - **vocabulary** - total number of independent tokens present in a corpus\n",
    "    - **corpus** - a body of text\n",
    "    - each word gets assigned an ID starting from 0 and going up to the length of the vocabulary. these numeric IDs are used by the model\n",
    "    - if there are words that are not recognized from the vocabulary, they get assigned an 'unknown' token (e.g. [UNK])\n",
    "    - **[UNK]** - out of vocabulary words result in a loss of information\n",
    "* **Character-based**\n",
    "    - split text into characters rather than words\n",
    "    - a much smaller vocabulary\n",
    "    - fewer [UNK]\n",
    "    - results in a comparatively longer tokenization which impacts the context that can be propagated by the model\n",
    "    - however, this can lose to some loss of meaning (depending on the language)\n",
    "* **Subword tokenization**\n",
    "    - frequently used words should not be split up into smaller words, but rare words should be decomposed into meaningful subwords. ex: annoyingly = annoying + ly\n",
    "    - ends up providing a lot of semantic meaning\n",
    "    - allows us relatively good coverage: small vocabularies with fewer unknown tokens\n",
    "    - EX: WordPiece, BPE, Unigram\n",
    "* **Others...**\n",
    "    - Byte-level\n",
    "    - WordPiece\n",
    "    - SentencePiece\n",
    "    \n",
    "#### Getting Started with the API\n",
    "\n",
    "Loading and Saving a tokenizer is the same as loading/saving a model. Using `.from_pretrained('<checkpoint>')` and `.save_pretrained('<checkpoint>')`\n",
    "\n",
    "* **Encoding** - generating input IDs - translating text into tokens\n",
    "    - splitting the text: use the same rules that were used when the model was pretrained\n",
    "    - convert the tokens into numbers using the same vocabulary as for pretraining\n",
    "* **Tokenization** use the `tokenize()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "570d6e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained( 'bert-base-cased' )\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize( sequence )\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab74af6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* **From tokens to input IDs** use the `convert_tokens_to_ids( tokens )` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3b224983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids( tokens )\n",
    "print( ids )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8f4b1a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2421, 1143, 1129, 2330, 117, 1142, 1110, 1126, 6608, 1326, 1105, 24819, 1942, 9499, 119, 14856, 20737, 4027, 113, 146, 4927, 155, 119, 24446, 5168, 2176, 112, 188, 8431, 2099, 1107, 1456, 1105, 1375, 114, 119, 1249, 1111, 3856, 25373, 1161, 117, 146, 112, 1396, 1793, 1106, 2824, 1122, 1105, 1107, 1139, 17398, 1111, 1199, 15879, 1336, 1256, 2824, 1265, 124, 117, 1134, 146, 2100, 1336, 1129, 10480, 119, 1252, 1296, 2004, 1107, 19939, 122, 1105, 123, 1138, 1151, 1176, 170, 7703, 119, 1573, 12170, 10884, 106, 1109, 1178, 5426, 1959, 1110, 2586, 23315, 2956, 117, 1103, 2712, 173, 2522, 8508, 1197, 1488, 1104, 20128, 119, 2009, 1577, 112, 189, 1142, 1326, 1129, 1167, 5426, 136, 1247, 112, 188, 1177, 1277, 3209, 1106, 1294, 1122, 170, 5098, 1686, 3381, 2541, 119, 1109, 17218, 1110, 9684, 783, 1296, 2741, 8194, 1116, 1113, 1114, 1103, 188, 13200, 18351, 3954, 1104, 170, 20549, 3098, 1120, 25514, 119, 1135, 112, 188, 1315, 17693, 8032, 2302, 117, 20787, 3807, 1176, 1677, 2093, 117, 9718, 1440, 1176, 170, 4306, 2106, 1957, 119, 1327, 2171, 1303, 1110, 170, 14531, 2661, 1120, 1126, 27182, 170, 1888, 1342, 1326, 1106, 3282, 1107, 170, 1207, 3703, 1150, 1336, 1129, 10816, 1283, 1118, 1103, 8794, 1947, 1133, 1138, 1870, 1106, 2541, 1103, 2276, 13533, 1104, 1199, 14820, 1983, 9499, 176, 14587, 119]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize( raw_inputs[0] )\n",
    "ids = tokenizer.convert_tokens_to_ids( tokens )\n",
    "print( ids )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d644d8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* **Decoding** from vocabulary indices, we want to get a string. use the `decode()` method  \n",
    "The decode methods not only translates the tokens, but regroups tokens that were split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0a116ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let me be clear, this is an animated series and NOT anime. Disclaimer ( I enjoyed R. Armitage's brilliant performance in North and South ). As for Castlevania, I've tried to watch it and in my desperation for some distraction may even watch season 3, which I hear may be promising. But each episode in Seasons 1 and 2 have been like a punishment. So incredibly dull! The only interesting character is Alucard, the beautiful dhampir son of Dracula. Why couldn't this series be more interesting? There's so much potential to make it a truly livening experience. The pacing is awful — each scene drags on with the sloppiness of a messy effort at storytelling. It's too dialog heavy, wit sounds like farce, fights look like a Saturday morning special. What happened here is a miserable attempt at animating a video game series to draw in a new audience who may be blown away by the animation style but have yet to experience the true genius of some fantastic Japanese anime gems.\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode(ids)\n",
    "decoded_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff67d5",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Handling Multiple Sequences\n",
    "\n",
    "#### Models Expect Batches of Inputs\n",
    "\n",
    "tokenizers translate text into sequences of numbers. Let's cast the numbers as a tensor and pass to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e3d06eb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-6362d492de00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m         )\n\u001b[1;32m    738\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistilbert_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (bs, seq_len, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (bs, seq_length, dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         return self.transformer(\n\u001b[1;32m    552\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    117\u001b[0m         embeddings)\n\u001b[1;32m    118\u001b[0m         \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# Setting the position-ids to the registered buffer in constructor, it helps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained( checkpoint )\n",
    "model = AutoModelForSequenceClassification.from_pretrained( checkpoint )\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life\"\n",
    "\n",
    "tokens = tokenizer.tokenize( sequence )\n",
    "ids = tokenizer.convert_tokens_to_ids( tokens )\n",
    "input_ids = torch.tensor( ids )\n",
    "model( input_ids )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70285e50",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Note**: Transformers expexts multiple sequences by default  \n",
    "add a new dimension to the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7a59d31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166]])\n",
      "Logits: tensor([[-3.1398,  3.3515]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([ids])\n",
    "print( \"Input IDs:\", input_ids )\n",
    "\n",
    "output = model( input_ids )\n",
    "print( \"Logits:\", output.logits )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dec4341",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Batching** - the act of sending multiple sentences through the model all at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "806f7a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166],\n",
      "        [ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166]])\n",
      "Logits: tensor([[-3.1398,  3.3515],\n",
      "        [-3.1398,  3.3515]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [ids, ids]\n",
    "input_ids = torch.tensor(batched_ids)\n",
    "print( \"Input IDs:\", input_ids )\n",
    "\n",
    "output = model( input_ids )\n",
    "print( \"Logits:\", output.logits )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba23efa",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Padding the Inputs\n",
    "\n",
    "**padding token** - a special token used to pad the length of sequences such that they all have the same length. It is important for tensors to have the same length so that they can be batched and processed uniformly by the linear algebra routines that are running under the hood.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "073f26ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print( tokenizer.pad_token_id )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df4c94",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Send two sequences through individually and compare to running them through batched together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1c3c818a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [[200, 200, 200],\n",
    "              [200, 200, tokenizer.pad_token_id]]\n",
    "\n",
    "print( model( torch.tensor( sequence1_ids )).logits )\n",
    "print( model( torch.tensor( sequence2_ids )).logits )\n",
    "print( model( torch.tensor( batched_ids )).logits )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90fd4a6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Observe** - the logits for the seperately run sequences and the batched sequences are different despite being the identical tokens. The difference is due to the padding that was added to make the sequences the same length for the batched run. To take the padding into account, we make use of an attention mask.\n",
    "\n",
    "#### Attention Masks\n",
    "\n",
    "**Attention masks** - tensors with the exact shape as the input, but filled with 1s where tokens are used and 0s where padding tokens are used; this is used to tell the model where in the sequence to pay attention. Use of an attention mask will ensure that the same sequence with different padding lenghts will result in the same logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f8b6bdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "attention_mask = [[1, 1, 1],\n",
    "                 [1, 1, 0]]\n",
    "\n",
    "print( model( torch.tensor( sequence1_ids )).logits )\n",
    "print( model( torch.tensor( sequence2_ids )).logits )\n",
    "print( model( torch.tensor( batched_ids ), \n",
    "             attention_mask = torch.tensor( attention_mask )).logits )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0f23d9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Wonderful! Now we can see that the use of an attention mask yields the same logit values as when the sequences were run through the model independently.  \n",
    "\n",
    "Another example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7ae0837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:\n",
      "I've been waiting for a HuggingFace course my whole life\n",
      "I hate this so much!\n",
      "\n",
      "tokens:\n",
      "['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life']\n",
      "['i', 'hate', 'this', 'so', 'much', '!']\n",
      "\n",
      "inputs:\n",
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166]\n",
      "[1045, 5223, 2023, 2061, 2172, 999]\n",
      "batched inputs:\n",
      "[[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166], [1045, 5223, 2023, 2061, 2172, 999]]\n",
      "[[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166], [1045, 5223, 2023, 2061, 2172, 999, 0, 0, 0, 0, 0, 0, 0]]\n",
      "\n",
      "tensor([[-3.1398,  3.3515]], grad_fn=<AddmmBackward>)\n",
      "tensor([[ 3.1931, -2.6685]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-3.1398,  3.3515],\n",
      "        [ 2.6743, -2.2346]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-3.1398,  3.3515],\n",
      "        [ 3.1931, -2.6685]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "text1 = \"I've been waiting for a HuggingFace course my whole life\"\n",
    "text2 = \"I hate this so much!\"\n",
    "print( 'text:' )\n",
    "print( text1 )\n",
    "print( text2 )\n",
    "print( '' )\n",
    "\n",
    "# tokenize the text\n",
    "tokens1 = tokenizer.tokenize( text1 )\n",
    "tokens2 = tokenizer.tokenize( text2 )\n",
    "print( 'tokens:' )\n",
    "print( tokens1 )\n",
    "print( tokens2 )\n",
    "print( '' )\n",
    "\n",
    "# return the IDs\n",
    "ids1 = tokenizer.convert_tokens_to_ids( tokens1 )\n",
    "ids2 = tokenizer.convert_tokens_to_ids( tokens2 )\n",
    "batched_ids = [ ids1, ids2 ]\n",
    "print( 'inputs:' )\n",
    "print( ids1 )\n",
    "print( ids2 )\n",
    "print( 'batched inputs:' )\n",
    "print( batched_ids )\n",
    "paddin = tokenizer.pad_token_id\n",
    "num_pads = len( ids1 ) - len( ids2 )\n",
    "ids2_pad = ids2.copy()\n",
    "ids2_pad.extend([0] * num_pads)\n",
    "padded_batched_ids = [ ids1, ids2_pad ]\n",
    "print( padded_batched_ids )\n",
    "print( '' )\n",
    "\n",
    "\n",
    "# pass the inputs through the model\n",
    "print( model( torch.tensor( [ids1] )).logits )\n",
    "print( model( torch.tensor( [ids2] )).logits )\n",
    "print( model( torch.tensor( padded_batched_ids )).logits )\n",
    "attention_mask = [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                 [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]\n",
    "print( model( torch.tensor( padded_batched_ids ), \n",
    "             attention_mask = torch.tensor( [attention_mask] )).logits )\n",
    "\n",
    "#  check and compare the logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f68a2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### Longer Sequences\n",
    "\n",
    "Most models handle sequences with length maximums in the 512-1024 range. If working with longer sequences, check out Longformer or LED models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159ab814",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Putting it all Together\n",
    "\n",
    "The API can facilitate the process of preprocessing and passing inputs to a model.  \n",
    "Use the `tokenizer` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "31b84c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I've been waiting for a HuggingFace course my whole life\", 'I hate this so much!']\n",
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 102], [101, 1045, 5223, 2023, 2061, 2172, 999, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 102], [101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 102, 0, 0, 0, 0, 0], [101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n",
      "{'input_ids': [[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 102], [101, 1045, 5223, 2023, 2061, 2172, 999, 102, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]]}\n",
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "{'input_ids': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
      "array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
      "        12172,  2607,  2026,  2878,  2166,   102],\n",
      "       [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 15), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n",
      "{'input_ids': array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
      "        12172,  2607,  2026,  2878,  2166,   102],\n",
      "       [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0]]), 'attention_mask': array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "batched_text = [ text1, text2 ]\n",
    "print( batched_text )\n",
    "\n",
    "model_inputs = tokenizer( batched_text )\n",
    "print( model_inputs )\n",
    "\n",
    "# padding methods\n",
    "# pad to the longest\n",
    "print( tokenizer( batched_text, padding='longest' ) )\n",
    "# pad to the model's max\n",
    "print( tokenizer( batched_text, padding='max_length' ) )\n",
    "# pad to a specified max\n",
    "print( tokenizer( batched_text, padding='max_length', max_length = 20 ) )\n",
    "# truncate if sequences exceed max length\n",
    "print( tokenizer( batched_text, padding='max_length', max_length = 10, truncation = True ) )\n",
    "# return as pytorch tensors\n",
    "print( tokenizer( batched_text, padding='longest', return_tensors = \"pt\" ) ) \n",
    "# return as tensorflow tensors\n",
    "print( tokenizer( batched_text, padding='longest', return_tensors = \"tf\" ) )\n",
    "# return as numpy arrays\n",
    "print( tokenizer( batched_text, padding='longest', return_tensors = \"np\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23f7dc9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### From Tokenizer to Model\n",
    "\n",
    "Once more ...with feeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d2339df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 4.0592, -3.2925],\n",
       "        [ 3.8101, -3.1752],\n",
       "        [-3.8215,  4.0621],\n",
       "        [-4.1085,  4.4263],\n",
       "        [-2.3191,  2.4049],\n",
       "        [ 3.4054, -2.7925]], grad_fn=<AddmmBackward>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "# raw_inputs == the Wtcher reviews\n",
    "tokens = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aa89b9",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Basic Usage Completed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11bed31",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### End-of-Chapter Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "aa1b264f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', '!']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "result = tokenizer.tokenize(\"Hello!\")\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
