{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74bdbeea-b099-4c83-bd5e-706289077f4a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# AWS Cloud Practitioner Certification Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d352613-7b5f-4fda-bf1e-347879e697ba",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## AWS Certified Cloud Practitioner Domains\n",
    "1. Cloud Concepts\n",
    "    * Define the AWS cloud and its value proposition\n",
    "    * Identify aspects of AWS cloud economics\n",
    "    * List the different cloud architecture design principles\n",
    "2. Security and Compliance\n",
    "    * Define the AWS shared responsibility model\n",
    "    * Define AWS cloud security and compliance concepts\n",
    "    * Identify AWS access management capabilities\n",
    "    * Identify resources for security support\n",
    "3. Technology\n",
    "    * Define methods of deploying & operating in the AWS cloud\n",
    "    * Define the AWS global infrastructure\n",
    "    * Identify the core AWS services\n",
    "    * Idenitify resources for technology support\n",
    "4. Billing and Pricing\n",
    "    * Compare and contrast the various pricing models for AWS\n",
    "    * Recognize the various account structures in relation to AWS billing and pricing\n",
    "    * Identify resources available for billing support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d2d4f8-ce5d-4d42-ba4d-58443d625b8f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## What is Cloud Computing?\n",
    "\n",
    "**Goal 1** - comprehensive understanding of Cloud Computing along with key concepts and benefits  \n",
    "**Goal 2** - when Cloud computing should be used and which model and service to deploy  \n",
    "\n",
    "1. What is Cloud Computing?\n",
    "    * **Cloud Computing** is a remote virtual pool of on-demand shared resources offering Compute, Storage, Database and Network services that can be rapidly deployed at scale\n",
    "    * **Virtualization** - allows the possibility of having multiple virtual machines (VMs) each running essentially seperate operating system and applications, installed on 1 physical server\n",
    "    * **Hypervisor** - a piece of software used to create the virtual environment allowing for multiple VMs to be installed on the same host. Shared hardware is achieved through a hypervisor. VMs make requests to the hardware via the hypervisor which ensures that the resources are shared between the VMs as needed and configured.\n",
    "    * Benefits of virtualization:\n",
    "        1. reduced capital expenditure (less hardware required)\n",
    "        2. reduced operating costs (less resources required for hardware)\n",
    "        3. smaller footprint (less physical space necessary)\n",
    "        4. optimization of resources\n",
    "    * Basic Cloud Resources:\n",
    "        1. **Compute** resources: brains to process your workload\n",
    "        2. **Storage** resources: allows data to be saved\n",
    "        3. **Database** resources: store structured sets of data used by applications\n",
    "        4. **Network** resources: provide connectivity that allows resources to communicate\n",
    "2. Cloud Deployment Models\n",
    "    1. **Public** vendor makes use of shared infrastructure that can be provisioned on demand and accessed over the internet for public usage. the consumer is typically unaware of the hardware and exact location of the data while the vendor provides all the backend and physical maintenance\n",
    "    2. **Private** the infrastructure is privately hosted and managed by the individual company using it. hardware typically lives on premesis\n",
    "    3. **Hybrid** makes use of both public and private clouds. established when a network link is configured between the private and public clouds. typically short-term configurations (e.g. for test/dev purposes)\n",
    "3. Key Cloud COncepts\n",
    "    * **On-demand Resourcing** resources are available almost immediately\n",
    "    * **Scalability** rapidly scale an environment both up/down and in/out. **Vertical Scaling** - increasing or decreasing compute resources on demand.\n",
    "    * **Economy of Scale** exceptionally low resource costs copared t traditional hosting\n",
    "    * **Flexibility and Elasticity** can fully customize the amount, quantity, duration and scale of the resources in your environment\n",
    "    * **Growth** your organization can experience growth that paces with need with far fewer constraints than classic on-premisis approaches\n",
    "    * **Utility Based Metering** you only pay for what you use\n",
    "    * **Shared Infrastructure** reduces overhead\n",
    "    * **Highly Available** replicatable & reusable\n",
    "    * **Security** more secure than on-site data centers see: Shared Responsibility Model\n",
    "4. Cloud Service Models\n",
    "    * **IaaS** - Infrastructure as a Service - a service that allows you to architect your own portion of the cloud (ex: AWS)\n",
    "    * **PaaS** - Platform as a Service - access to a framework from the operating system and up (ex: Heroku)\n",
    "    * **SaaS** - Software as a Service - allows for the delivery of an application that can be widely distributed (ex: GMail)\n",
    "    * Other models: Disaster Recovery as a Service, Communications as a Service, Monitoring as a Service\n",
    "5. Common use cases of Cloud COmputing\n",
    "    * Migrating locally existing production services to the Cloud\n",
    "    * Traffic bursting - dealing with fluctuations on data resources\n",
    "    * Backup/ Data redundancy at a lower cost\n",
    "    * Web Hosting takes out a lot of administrative and maintenance demands\n",
    "    * Test/Dev environments can spin up instances as/when needed and then shut down when finished\n",
    "    * Proof of Concept work becommes more feasible\n",
    "    * Big Data & Data manipulation the cloud scales to large data better\n",
    "6. How Data Center architecture is reflected within the cloud\n",
    "    * classic datacenter: location. physical security, mech/electric infrastructure, network infrastructure, servers, storage\n",
    "    * how are these components reflected in the cloud?\n",
    "        * location: cloud providers have different regions with built in redundancy\n",
    "        * physical security: vender secures the data center while end user has no physical access to where data is stored\n",
    "        * mech/electrical infrastructure: maintained by the vender\n",
    "        * network infrastructure: operates at a software level in the Cloud, not physical devices like switches or routers\n",
    "        * servers: instances or virtual machines\n",
    "        * storage: often regarded as unlimited, hugely scalable. AWS has the Elastic Block Store service\n",
    "        \n",
    "7. Summary\n",
    "    * Cloud Computing is a remote virtual pool of on-demand shared resources offering Compute, Storage, Database and Network services that can be rapidly deployed at scale\n",
    "    * Main concepts: on-demand resources, scalability, economy of sclae, felxibility and elasticity, utility based metering, shared infrastructure, highly available, security\n",
    "    * 3 Main cloud service models: IaaS, PaaS, SaaS\n",
    "    * 3 Cloud Deployment Models: public, private, hybrid\n",
    "    * a variety of use cases such as: web hosting, big data manipulation, traffic bursting etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252cd48f-a2c8-4da0-a663-748a48e5029c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Compute Fundamentals for AWS\n",
    "\n",
    "AWS Foundation Services\n",
    "* Compute\n",
    "* Storage\n",
    "* Database\n",
    "* Network\n",
    "\n",
    "This course will focus on the foundational Comute resources. \n",
    "\n",
    "1. What is 'Compute' in AWS?\n",
    "    * **Compute resources** can be considered the brains and processing power required by applications and systems to carry out computational tasks via a series of instructions\n",
    "    * Compute resources are closely related to common server coponents such as CPUs and RAM\n",
    "    * a very wide range of compute resources for different uses in AWS: Compute services can be as large as hundreds of EC2 instances continually processing millions of instructions. On the other hand, compute services can utilize just a few hundred miliseconds of resources for a few lines of code in an AWS Lambda\n",
    "    * Compute resources can be consumed in different quantities, for different lengths of time, across a range of categories, offering a wide scope of performance and benefit options\n",
    "    * [Cloud Compute Index](https://aws.amazon.com/products/compute/)\n",
    "2. Elastic Compute Cloud (EC2)\n",
    "    * **EC2** allows you to deploy virtual servers within your AWs environment. Most people will require an EC2 instance within their environment as a part of at least one of their solutions\n",
    "    * The EC2 service can e broken down into the following components:\n",
    "        * **Amazon Machine Images** (AMIs) \n",
    "            - are essentially templates of preconfigured EC2 instances which allow you to quickly launch a new EC2 instance based on the configuration within the AMI.\n",
    "            - an AMI is an image baseline with an operating system and applications along with any custom configuration\n",
    "            - can create AMIs to help speed deployments: Amazon Linux AMI $\\rightarrow$ EC2 instance $\\rightarrow$ Customized instance $\\rightarrow$ Custom AMI $\\rightarrow$ launch multiple instances of the Customized Instance ....very good for autoscaling\n",
    "            - the AWS Marketplace has many customized AMIs for purchase from trusted vendors\n",
    "            - Community AMIs are a repo of AMIs created by other AWS members for community use\n",
    "        * **Instance Types** \n",
    "            - **Instance types** define the size of the instance based on a number of parameters: ECUs - # EC2 compute units, vCPUs - # virtual CPUs on the instance, Physical Processor - processing speed, Memory - amount of memory available, Instance Storage & Network Performance. etc.\n",
    "            - **Instance Families** - instance types that offer distinct performance benefits\n",
    "                - MicroInstances - low-cost with minimal CPU\n",
    "                - General Purpose - have a balanced mix of CPU, storage and memory\n",
    "                - Compute Optimized - have a greater focus on Compute with a high vCPU:memory ratio\n",
    "                - GPU - optimized for graphic intensive processes\n",
    "                - FPGA - filled programmable gate arrays for massively parrallel processes (think genomics)\n",
    "                - Memory optimized - large scale enterprize in memry applications. real time processing of unstructured data\n",
    "                - Storage Optimized - high IO and storage capacity\n",
    "        * **Instance Purchasing Optioons**: \n",
    "            - On-demand instances: can be launched at any time, can be used as long as needed, have a flat per second rate. Suggested use: short-term processes that are unpredictable and can't be interupted. Best for test/dev\n",
    "            - reseerved instances: Purchases for a set period of time for a reduced cost. can pay all upfront, partial upfront, or no upfront (w/diminishing returns). Best applied for long-term predictable workloads. preferred: scheduled instances that are continuously running\n",
    "            - scheduled instances: pay for reservations on a recurring schedule, either daily, weekly or monthly. If you set up a scheduled instance and don't use it, you will still be charged. preferred use: scheduled instances that are not continually running\n",
    "            - spot instances: can bid for unused EC2 compute resources. However, there are no guarentees for a fixed period of time. the price fluctuates based on supply/demand. However, there's a chance you could get a large EC2 instance at a very low price. This option is useful for processing data that can be suddenly interupted (batch jobs and background data processing)\n",
    "            - on-demand capacity reservations: reserve capacity based on different attributes such as instance type, platform and tenancy, within a particular availability zone for any period of time. Can be used on reserved instances.\n",
    "        * **Tendancy** - the physical server within an AWS data center that is the underlying host of your EC2 instance\n",
    "            * **Shared Tenancy** EC2 instance launched on any available host with the required resources. The same host may be used by multiple customers while AWS security mechanisms prevent one EC2 instance from accessing another on the same host\n",
    "            * **Dedicated Instances** EC2 instance hosted on hardware that no other customer can access. May be required to meet compiance. Dedicated instances cost mmore.\n",
    "            * **Dedicated Hosts** Additional visibility and control on the physical host. Allows to use the same host for a number of instances. May be required to meet compliance\n",
    "        * **User Data** - enter commands that will run during the first boot cycle of that instance. EX: perform functions upon boot to pull down additional software or run OS updates\n",
    "        * **Storage Options** - selecting storage for EC2 instances will depend on the instance selected\n",
    "            * Persistent Storage: available by attaching Elastic Block Storage (EBS) volumes. The EBS volumes are seperate entities from the EC2 instance and are attached via the AWS network. analogous to attaching a harddrive to your PC. The process of attaching EBS the data get replicated across multiple volumes for persistence. EBS volumes can be detached from the EC2 and reattched or used with other resourced. can also implement encryption and take backup snapshots of the data for further security\n",
    "            * Ephemeral Storage: created by EC2 instances using local storage. is physically attached to the underlying host. analogous to the hard drive on a PC with the exception that when an EC2 instance is stopped, all saved data on disk is lost. if you reboot the instance, the data will remain intact (unlike google colab). you cannot detach the data fro the instance\n",
    "        * **Security** - will be asked to select a Security Group for your instance. [For more info](https://cloudacademy.com/blog/aws-security-groups-instance-level-security/). At the end of EC2 instance creation, you will need to associate a key pair to the instance. **Key pair** - a public & private key. The purpose of the key pair is to encrypt the login informationn for Linux and Windows EC2 instances, and then decrypt the same information allowing you to authenticate onto the instance. The public key is held and kept by AWS, the private key is the users responsibility to keep and ensure that it is not lost/compromised. It is possible to use the same key pair for multipple instances. It is the user's responsibility to maintain and install the latest OS and security patches as part of the shared responsibility model.\n",
    "        * DEMO. From Console $\\rightarrow$ EC2 $\\rightarrow$ launch instance $\\rightarrow$ go with an AWS linux $\\rightarrow$ General Purpose t2.micro $\\rightarrow$ configure instnace details      \n",
    "3. Elastic Container Service (ECS) - allows you to run Docker-enabled applications packaged as containers across a cluster of EC2 instances. The user does not need to manage their own cluster management system beccause this is abstracted with the ECS service by passing the job to AWS Fargate\n",
    "    * **Docker Containers** holds everything an application needs to run from within its container package (runtime, packages etc) except the OS; this makes container applications very portable. \n",
    "    * With AWS ECS there is no need to install any management or monitoring software for your cluster\n",
    "    * 2 different deployment modes:\n",
    "        * Fargate Launch: reqired to specify the CPU and memory required, define networking and IAM policies, and to package application into containers\n",
    "        * EC2 Launch: much more configurable/customizable. User is responsible for patching and scaling instances, specifying instance type and assigning containers\n",
    "    * Monitoring is taken care of through AWS CloudWatch: can create alarms based on metrics for specific events (e.g. cluster size scaling up/down)\n",
    "    * an ECS cluster is comprised of a collection of EC2 instances that operate much the same way as a single EC2 instance\n",
    "    * clusters act a a resource pool to aggregate CPU, memory etc\n",
    "    * clusters are dynamically scalable and can have multiple instances\n",
    "    * ECS clusters are region specific\n",
    "4. Elastic Container Registry (ECR) - a secure location to store and manage your docker images. This is a fully managed service, so you don't need to provision any infrastructure to allow you to create this registry of docker images. This service allows devs to push/pull and manage their library of docker images in a central and secure location.\n",
    "    * Registry - allows you to host and store docker images as well as create image repositories. Your account will have both read/write access by default to any images you create within the registry and any repository. Access to registry and images can be controlled via IAM policies in addition to repository policies\n",
    "    * Auth token - before your docker cliet can access your registry, it needs to be authenticated as an AWS user via an Authorization token. beginning authorization for a docker client with the default registry: 1) run the `get-login` command using the AWS CLI 2) the output response will be a docker login command 3) the resulting auth token can be used for the next 12hrs\n",
    "    * Repository - objects in the registry that allow you to group together and secure different docker images. one registry can hold mu;tiple repositories allowing the user to organize/manage docker images. IAM and repository policies can be used to manage permissions to each repository\n",
    "    * Repository policy - resource-based policies.\n",
    "        * IAM managed policies: AmazonEC2ContaierRegistryFullAccess, AmazonEC2ContaierRegistryPowerUser, AmazonEC2ContaierRegistryReadOnly\n",
    "        * will need to add a principal to a repo policy to determine who has access/permission. this will require access to the ecr:GetAuthorizationToken API call. Actions a user has can be controlled by repository policies.\n",
    "    * Image - to push an image into ECR use the docker push command; to retrieve an image you can use the docker pull command\n",
    "5. Elastic Container Seervice for Kubernetes (EKS)\n",
    "    * **Kubernetes** - an open-source container orchestration tool designed to automate, deploy, scale and operate containerized applications. it can scale dramatically and is run-time agnostic\n",
    "    * **EKS** - a managed service allowing you to run kubernetes across your AWS infrastructure without having to take care of provisioning and running the kubernetes management infrastructure in a control plane. the user only needs to provision the worker nodes\n",
    "    * **Control plane** - a number of different APIs, the kublet processes and the Kubernetes Master. the control plane schedules containers onto nodes and continually monitors the objects\n",
    "    * In EKS, AWS is responsible for provisioning, scaling, and managing the control plane, and they do this by utilizing multiple availability zones for additional resilience\n",
    "    * **Worker Nodes** - kubernetes clusters are composed of nodes. an aggregates is the collection of nodes. A node is a kubernetes worker machine that runs an on-demand EC2 instance which includes software to run containers. Each node has an associated AMI to ensure Docker and kuberlet installation for security controls. the user provisions the nodes which can then connect to EKS using an endpoint.\n",
    "    * Working with EKS\n",
    "        1. Create an EKS Service Role - an IAM role that allows EKS to provision and configure specific resources. must have AmazonEKSServicePolicy & AmazonEKSClusterPolicy\n",
    "        2. Create an EKS Cluster VPC - a CloudFormation stack configured to use EKS\n",
    "        3. Install kubectl and AWS-IAM authenticator. Kubectl is a command line utility for Kubernetes\n",
    "        4. Create your EKS Cluster\n",
    "        5. Configure kubectl for EKS using the update-kubeconfig command in AWS CLI\n",
    "        6. Provision and configure Worker Nodes. once EKS cluster shows 'active' status, launch using CloudFormation\n",
    "        7. Configure the worker node to join the EKS cluster\n",
    "6. AWS Elastic Beanstalk - takes uploaded code of a web application and automatically provisions and deploys the required resources within AWS to make the web application operational. These resources include EC2, Auto scaling, application health monitoring and elastic load balancing in addition to capacity provisioning. Elastic beanstalk is an ideal service for engineers who may not have the familiarity or the necessary skills within AWS to deploy, provision, monitor and scale the correct environment to run the developed application. AWS Elastic beanstalk deploys the correct infrastructure to run the uploaded code. The user can then support and maintain the environment.\n",
    "    * operates with many platforms and languages: ruby, python, PHP, Node.js, Go Docker things etc.\n",
    "    * Elastic Beanstalk, as a service, is free to use. However, any resources that are created on yur applications behalf (e.g. EC2 instances) will be charged standard billing at the time of deployment.\n",
    "    * Elastic beanstalk core components:\n",
    "        * application version - a very specific reference to a section of deployable code. the application version will typically point to S3 where the code might reside.\n",
    "        * environment - All the resources created by Elastic beanstalk. an application version deployed by AWS resources configured and provisioned by AWS Elastic beanstalk. the app becomes operational within your environment.\n",
    "        * environment configurations - collection of parameters and settings that dictate how an environment willl have its resources provisioned by Elastic Beanstalk and how these resources will behave\n",
    "        * environment tier\n",
    "            * web server environment - if the app manages and handles http requests. typically used for standard web applications that operate and serve requests over HTTP port 80. Typical resources: Route 53, Elastic Load Balancer, Auto Scaling, EC2 & Security Groups\n",
    "            * worker environment - does not run http requests and instead pulls data from an SQS queue. applications that will have a back-end processing task, that will interact with AWS SQS. Typical resources: SQS Queue,  IAM Service Role, Auto Scaling, EC2\n",
    "        * configuration template - provides the baseline for creating a new, unique, environment configuration.\n",
    "        * platform - the culmination of components from the OS of the instance, the programming language, the server type and components of Elastic Beanstalk\n",
    "        * Applications - collection of environments, environment configurations and application versions.\n",
    "    * Elastic beanstalk Workflow\n",
    "        1. Create an Application\n",
    "        2. Upload the Application Version (creates the environment\n",
    "        3. Launch the Environment\n",
    "        4. Manage Environment (deploy new versions)\n",
    "7. AWS Lambda - is a serverless compute service that allows you to run yur application code without having to manage EC2 instances.\n",
    "    * **Serverless** - means that you do not need to worry about provisioning and managing your own compute resources to run your own code, instead this is managed and provisioned by AWS. the service does require compute power to carry out the code requests, but because AWS user does not need to be concerned with managing the compute power or where it's provisioned from, its considered 'serverless' from this perspective\n",
    "    * You only ever have to pay for compute power when Lambda is in use via Lambda Functions. AWS Lambda charges power per 100ms of use when your code is running, in addition to the number of times your code runs.\n",
    "    * **Working with AWS Lambda** \n",
    "        1. upload code to Lambda. Supported languages: Node.js, Java, C#, Python, Go, PowerShell, Ruby\n",
    "        2. Configure your Lambda to execute upon specific triggers from supported event sources. EX: an object being uploaded to an S3 bucket\n",
    "        3. Once the trigger is initiated, Lambda will run your code (as per the Lambda function) using only the required compute power as defined.\n",
    "        4. AWS records the compute time in ms and the quantity of Lambda functions run to ascertain the cost of the service\n",
    "    * **Components of AWS Lambda**\n",
    "        * the Lambda functions - compiled code that you want Lambda to invoke when triggered\n",
    "        * Event sources - AWS services that can be used to trigger your Lambda fxn\n",
    "        * Trigger - operation from an event source that causes the function to invoke. ex: 'put' request in S3\n",
    "        * Downstream Resources - required for execution of the Lambda. these are the resources to be used upon invokation of the function\n",
    "        * Log Streams - help to identify issues and troubleshoot your Lambda fxn recorded in CloudWatch\n",
    "    * Creating lambda Functions\n",
    "        1. Selecting a Blueprint - templates provided by AWS Lambda (ex: S3-get-object)\n",
    "        2. Configure Triggers - define the trigger for the Lambda\n",
    "        3. Configure the Function - upload the code or edit in-line. Define the required resources, max execution timeout, IAM role and Handler Name.\n",
    "    * Benefits: AWS Lambda is highly scalable serverless service couped with fantastic cost optimization compared to EC2 as you are only charged for Compute power while the code is running and for the number of functions called.\n",
    "8. AWS Batch - used to manage and run batch computing workloads within AWS\n",
    "    * **Batch Computing** - used when vast amounts of compute power across a cluster of compute resources is needed to complete batch processing for a series of tasks\n",
    "    * AWS Batch seamlessly creates a cluster of compute resources which are highly scalable taking advantage of the elasticity of AWS coping with any level of batch preocessing whilst optimizing distributed workloads. All provisioning, monitoring, maintenance and management of the clusters is taken care of by AWS\n",
    "    * 5 Components of AWS Batch\n",
    "        1. Jobs - units of work (ex: executable file or an application on an EC2 instance) to be run by AWS Batch. jobs are ru on EC2 instances as containerized applications and can take on different states ( submitted, pending, running, failed etc)\n",
    "        2. Job Definitions - define specific parameters for the jobs themselves and dictate how the job will run and with what configuration. (ex: how many CPUs for the container, what data volumes to use, IAM roles, mount points)\n",
    "        3. Job queues - are scheduled placement of jobs in a job queue until they run. there can be multiple queues with different priorities. can be on-demand or spot.\n",
    "        4. Job Scheduling - the job scheduler takes care of when jobs should be run and from which compute environment\n",
    "        5. Compute environments - contain the compute resources to carry ut the job.\n",
    "            * Managed environments - the service handles provisioning and creates an ECS Cluster\n",
    "            * Unmanaged environments - provisioned by the user for greater customizability (but requires greater administration and maintenance)\n",
    "    * AWS Batch is a great resource if you need to run multiple jobs in parallel\n",
    "9. Amazon Lightsail - essentially a VPS backed by AWS infrastructure, much like an EC2 instance but without as many configurable steps throughout its creation. Commonly used to host simple websites, small applications and blogs. Lightsail provides a lightweight solution for small rpojects and use cases which can be deployed quickly and cost effectively in just a few clicks\n",
    "\n",
    "### Summary  \n",
    "* Compute resources can be considered the brains and processing power required by applications and systems to carry out computational tasks via a series of instructions\n",
    "* AWS Compute is primarily: EC2, ECS, ECR, EKS, Elastic Beanstalk, Lambda, Batch & Lightsail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d951f1db-919b-4e1e-a9e6-fab539ce9c0a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[AWS Cloud Practitioner Exam Guide](https://d1.awsstatic.com/training-and-certification/docs-cloud-practitioner/AWS-Certified-Cloud-Practitioner_Exam-Guide.pdf)\n",
    "\n",
    "## Cloud Concepts (CLF-C01)\n",
    "This course will give an understanding of:\n",
    "* Key cloud concepts such as scalability, elasticity and security\n",
    "* Cloud deployment models such as public, provate and hybrid\n",
    "* Cloud service models including IaaS, PaaS and SaaS\n",
    "\n",
    "### What is Cloud Computing?\n",
    "* **Cloud Computing** - is a remote virtual pool od on-demand shared resources offering Compute, Storage, Database and Network services that can be rapidly deployed at scale\n",
    "* **Virtualization** - allows multiple virtual machines to run on on essentially a seperate operating system and applications while eing physically installed on the same server\n",
    "* **Shared hardware** - a key concept in virtualization and the cloud\n",
    "* **Hypervisor** - a piece of softwareused to create virtualized environments allowing multiple VMs to be installed on the same host. the hypervisor sits between the physical hardware and the virtual machines and creates a shared pool of virtual hardware resources for each of them to access. All VMs see the same hardware. However, they make requests to the hardware via the hypervisor which makes sure that hardware resources get shared between all the VMs as needed and configured.\n",
    "* Benefits of Virtualization\n",
    "    * Reduced capital expenditure (less hardware required)\n",
    "    * Reduced operating costs (less space, power, cooling required as in a traditional data center)\n",
    "    * Smaller footprint (less physical space needed)\n",
    "    * Optimization of resources (cloud vendor and consumer benefit)\n",
    "* Core foundation services of Cloud computing\n",
    "    * Compute - the 'brains' to process your workload. comparable to CPUs and RAM of on-premisis environments\n",
    "    * Storage - allow user to store/save data. comparable to Server hard disks, Network Attached Storage (NAS) and high speed Storage Area Network (SAN)\n",
    "    * Database - allow user to store structured sets of data used by applications. Comparable to SQL server, MySQL etc.\n",
    "    * Network - provide the connectivity to allow compute / storage / database to communicate. Comparable to routers to route traffic between network switches and firewalls to allow/deny traffic to/from an environment\n",
    "    \n",
    "### Cloud Deployment Models  \n",
    "* **Public** - a vender makes available a shared infrastructure of Compute, Storage, Database and Network services which can be provisioned on demand by the public. The user never sees the hardware let alone knows the exact physical location of their data. Hwever, they may specify a region of service which will aid in minimizing data latency. backend and physical maintenance (power, cooling and hardware maintenance) are handled by the vender\n",
    "* **Private** - infrastructure is privately hosted, managed and owned by the entity using it. This gives more direct control to the entity and is ideal for those who wish to keep a tighter grasp of security control. In this case, the hardware is usually held in private. This is different from typical on-premise server farms because cloud principals are applied (virtualization to create a pool of shared compute and network resources). This is more demanding thanthe public cloud approach because of the overhead and maintenance costs which must be assumed by the user.\n",
    "* **Hybrid** - makes use of public and private clouds. is great for handling burst traffic or disaster recovery. a hybrid model is established when a network link is configured between a private cloud and a public cloud. Hybrid clouds are normally short lived: test/dev purposes or transition states from public to private\n",
    "\n",
    "### Key Cloud Concepts\n",
    "* **On-Demand Resourcing** - when you want to provision a resource within the cloud, it's almost immediately available for use\n",
    "* **Scalability** - rabidly scale your environment up/dwn & in/out\n",
    "    * up/down - altering the power and performance of an instance by adding/subtracting compute resources (e.g. CPU or memory)\n",
    "    * in/out - add/remove the number of instances used as compute resources\n",
    "* **Economy of Scale** - Due to the huge scale of resources public cloud offering provide, this offers exceptionally low resource costs to the end user\n",
    "* **Felxibility and Elasticity** - the amount of choice available allows the user to fully customize exactly how the environment should be: amount of resources, how long resources should run and at what scale\n",
    "* **Growth** - cloud computing offers the ability to grow using a wide range of resources and services; much less growth contstraints compared to a classic on-promices environment\n",
    "* **Utility Based Metering** - you oly pay for what you use when you use them.\n",
    "* **Shared Infrastructure** - hosts within the cloud are virtualized; multiple tenants can run on the same hardware. this significantly reduces the amount of physical hardware and supporting resources required. this contributes to the economy od sclae and makes things cheaper for the customer.\n",
    "* **Highly Available** - many services and infrastructure are replicated across different zones and regions. Having data copied to multiple different places automatically ensures the durability and availability of the data and services without needing to specifically set up these redundancies.\n",
    "* **Security** - considered to bemore secure than an onsite data center, because cloud vender have to adher to global compliance programs across many industries and also by applying the shared responsibility model\n",
    "[Shared Responsibility Model](https://cloudacademy.com/blog/aws-shared-responsibility-model-security/)\n",
    "\n",
    "### Cloud Service Models\n",
    "Different levels and manageability and customization over the users use case\n",
    "* **IaaS** - Infrastructure as a Service gives the highest level of customization. Allows the user to be the architect of their own protion of the cloud: configure their own virtual network, configure instances from the OS and up.\n",
    "* **PaaS** - Platform as a Service gives access to a framework that sits on top of the operating system and up. The underlying architecture, host hardware, network components and OS are managed by the vendor. PaaS is great for developers who can focus on designing an app that will sit on top of the PaaS. ex: Heroku\n",
    "* **SaaS** _ Software as a Service (lowest level of service model) allows for the delivery of an application that can be widely accessed and distributed. These are usually very simple in design and appeal to a wider ausience. No requirements to install any software for use. ex: Gmail\n",
    "IaaS>>PaaS>>SaaS\n",
    "\n",
    "### Common use cases of Cloud Computing\n",
    "1. Migration of Production Services: migrate production services from an existing on-premise solution into the cloud\n",
    "2. Traffis Bursting: the public cloud can be used to scale a network and resources up to manage and handle additional traffic during peak use periods.\n",
    "3. Backup / Dissaster Relief: built-in resiliancy ad durability makes the cloud a greater solution for backup requirements than local on-promice solutions. Cloud has nearly unlimited storage space with built in data management lifecycle policies at a very low cost\n",
    "4. Web Hosting - hosting web services on the cloud is advantageous because of the clouds ability to load balance across multiple instances, scale up/down quickly and automatically to reflect changes in traffic.\n",
    "    * CDN: a set of systems which redirect traffic to the closest caching server. CDNs can reduce latency\n",
    "    * DNS services can help manage demand on web servers by redirecting traffice to load balancers for redistribution.\n",
    "5. Test/Dev Environments: public cloud offers users ability to spin up servers on-demand and sht them down when finished. Would would not be financially viable to have these resources around on-premisis\n",
    "6. Prook of Concept: can implement proof of concept desins to help build successful business cases to present to senior management\n",
    "7. Big Data/Data Manipulation: the cloud makes it much easier to manage and manipulate big data. Having an environment to work with big data allows the user to focus on the analysis and processing without the need to worry about the maintenance and underlying architecture.\n",
    "\n",
    "### How Data Center architecture is reflected in the Cloud\n",
    "The data center as a whole and its architecture can be logically broken down as follows:\n",
    "* Location: within each region, Cloud venders may have at least 2 datacenters in different geographic locations for added resiliancy; this is not practical for every individual user to enact\n",
    "* Physical Security: it is the vender's responsibility to ensure it is implementing and achieving the correct certification and governance regarding security\n",
    "* Mechanical & Electrical Infrastructure: Computer Room Air Conditioning (CRAC) includes generators, uninterupted power supplies, AC, fire suppression. In the cloud, it is the vendors responsibility to ensure that they are implementing the correct capacity, resiliancy and testing to ensure their infrastructure\n",
    "* Network Infrastructure (switches, routers, firewalls): networking operates at a software level (no options to instal a physical switch or router). However, you are able to implementcontrols and configurations that simulate these effects (e.g. create a virtual network segmented to different IP addresses allowing the user to deploy resources as required AWS: Virtual Private Clouds, Azure: VNets)\n",
    "* Servers: the Cloud equivalent == instances (VMs). Just as there is a variety of types of servers for on-premisis facilities, there are different types of VMs to suite different needs (e.g. hosting databases vs processing big data)\n",
    "* Storage (NAS, SAN, Block Storage, Backup) is fantastic in the Cloud as it's often regarded as unlimited, hugely scalable annd highly durable. The cloud has a variety of storage types to suite needs. The AWS equivalent of a SAN is Elastic Block Storage (EBS) which offers persistent block level storage and can be detached from one instance and re-attached to another instance.\n",
    "\n",
    "### Is the Cloud right for your business?\n",
    "Where is the business going? What are you trying to achieve? What are the business objectives?\n",
    "* On-Demand Resourcing. for on-site, it can takes weeks or more just to physically get the resources needed to scale up\n",
    "* Scalability. scaling is resource intensive, but using the Cloud eliminates most of the burden and lessens the cost on the user. [Philips Health AWS case study](https://aws.amazon.com/solutions/case-studies/philips-healthsuite-case-study/), [Airbnb case study](https://aws.amazon.com/solutions/case-studies/airbnb-case-study/)\n",
    "* Economies of Scale. get shared wholesale costs on Compute, Storage & Network resources. Savings get passed to end users\n",
    "* Flexibility & Elasticity. can have as many or as few resources as desired without having to guess the capacity up front. Auto-scaled resources with customized thresholds can shrink or expand resources elastically to meet customer demand. [Unilever case study](https://aws.amazon.com/solutions/case-studies/unilever/)\n",
    "* Growth. Cloud rapid deployment allows resources to scale to meet demand.\n",
    "* Utility Based Metering. Only pay fr resources for the time they are used as opposed to running, maintaining equipment 24x7.\n",
    "* Shared Infrastructure. Virtualized environments with multiple tnenats greatly reduces the amount of hardware required.\n",
    "* Highly available. offsite replication is built into storage by design.\n",
    "* Security. Public Cloud security is help to a very high standard and has to meet many global standards, compliance and certifications. Vendors provide security 'of' the cloud while end user provide security 'in' the cloud.\n",
    "\n",
    "### Cloud Business Benefits\n",
    "What else can the Cloud bring me as a business?\n",
    "* Innovation Potential: new and innovative cloud tools are developed all the time. moving to the cloud allows the user to experiment with these approaches in a way that might not be possible for businesses working on top of out-dated data architectures.\n",
    "* Swift go to Market: instant resource availability to push product development reduces time to deploy. [Netflix case study](https://aws.amazon.com/solutions/case-studies/netflix/)\n",
    "* Carbon footprint: shared resources helps reduce carbon emission\n",
    "* Remote Access: opens opportunities for support/management of resources. removes physical constraints on where/how resources are accessed (can outsource to cheap labor (sad face))\n",
    "* Reduce risks: minimize loss or corruption of customer data\n",
    "* Collaboration & Business agility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19932977-d4bb-4a3e-b9c3-41b59ead8291",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## AWS Global Infrastructure: Availability Zones, Regions, Edge Locations, Regional Edge Caches\n",
    "\n",
    "### Availability Zones (AZs) - are the physical data centers of AWS. \n",
    "* multiple data centers located close together can form a single AZ.\n",
    "* each AZ will have at least one other AZ geographically located within the same area (city) that are interconnected with low latency private fiber optic connections and powered by seperate sources. \n",
    "* these links are used to replicate data for resiliency purposes\n",
    "\n",
    "### Regions - localized geographical grouping of multiple AZs which each contain multiple (at least 2) data centers\n",
    "* having global regions allows for compliance with regulations, laws, and governance relating to data storage\n",
    "* regions have a specific 'friendly' and code naming convention\n",
    "* if you  have multiple AWS accounts and you try to coordinate resources within the same AZ (by codename) this may not necessarily mea those resources are physically located within the same AZ\n",
    "\n",
    "### Edge Locations - Edge locations are AWS sites deployed in major cities / high pop density areas across the globe.\n",
    "* are primarily used by end users who are accessing and using your services\n",
    "* ex: if your website hosts instaces inOhio, but a user accesses services from the UK, the user would be redirected to an edge location nearby where cached data could be read and latency reduced\n",
    "\n",
    "### Regional Edge Caches - a new  type of edge location.\n",
    "* has larger cache width than other edge locations. when data is requested at the Edge location that is no longer available, the edge location can retrieve from the regional edge cache instead of the origin server, which would have a higher latency "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3c5cd8-dfb0-4846-95d4-216b3c0b2a7c",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Compute\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This section covers topics that fall under Technology ('Domain 3') in the official AWS Certified Cloud Practitioner exam blueprint, which accounts for 33% of the exam content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5683f4f2-6c89-44dc-a493-8f19ba9d5241",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## AWS Compute Fundamentals\n",
    "\n",
    "### What is Compute in AWS?\n",
    "**Compute** resources can e considered the brains and processing power required by applications and systems to carry out computational tasks via a series of instructions. AWS Cloud compute resources are analogous to common server components such as CPU and RAM.  \n",
    "Within AWS there are a number (and range) of different services and features that offer Compute power:\n",
    "* one extreme: compute services can utilize hundereds of EC2 instances (virttual servers) which may be used continuously for months or even years processing milliions of instructions\n",
    "* other extreme: may only utilize a few hundred milliseconds of compute resource to execute just a few lines of code within AWS Lambda before relinquishing that compute power\n",
    "Indeed, compute resources can be consumed in different quantities, for different lengths of time, across a range of categories, offering a wide scope of performance and benefit optioins\n",
    "\n",
    "[AWS Cloud Compute Index](http://aws.amazon.com/products/compute/) gives a number of different scenarios and suggestions of what Cloud comute resources might be the best fit.\n",
    "\n",
    "### EC2 Elastic Compute Cloud\n",
    "EC2 will likely be the first  and most common compute service used in AWS.  \n",
    "EC2 allows you to deploy virtual servers within your AWS environment.  Most people will require an EC2 instance within their environment as a part of at least one of their solutions.  \n",
    "The EC2 service can be broken down into the following components:\n",
    "* Amazon Machine Images (AMIs) - An AMI is an image baseline with an OS and applications along with custon configurations. AMIs are essentially templates of preconfigured EC2 instances which allow you to quickly launch a new EC2 instance based on the configurstion within the AMI -this means you don't need to install an OS or other supporting software needed for the template use case. When configuing an EC2 instance, selecting an AMI is the first decision choice to make. The AMI catalog lists several different groupungs: Quickstart (45 most common), AWS marketplace (thousands of preconfigured AMIs) & community AMI (ex: Ubuntu configured for model training w/GPUs). Can also make your own AMI from an instance that you configured yourself. This saves you the trouble of manually reloading the same applications to a default Amazon (Linux) AMI. Once made, just launch your custon AMI as the base instance. Very handy for autoscaling.\n",
    "* Instance Types - Once you have selected your AMI, you then select an instance type. Selecting an instance type defines the size of the instance based on a number of parameters (although key params: vCPUs, Memory, Instance Storage, Network Performance):\n",
    "    * Instance Parameters:\n",
    "        * ECU - number of EC2 compute units\n",
    "        * vCPUs - number of virtual CPUs used on the instance\n",
    "        * Physical Processor - processing speed of the instance\n",
    "        * Clock Speed in gigahurtz\n",
    "        * Memory associated with the instance\n",
    "        * Instance storage\n",
    "        * EBS Optimized storage (yes/no)\n",
    "        * Network Performance - rate of data transfer\n",
    "        * IPv6 Support\n",
    "        * Processor architecture\n",
    "        * AES-Ni - encryption instructions (yes/no)\n",
    "        * AVX - supports advanced vector extensions (yes/no)\n",
    "        * Turbo - intel turbo boost?\n",
    "    * Instance Types: different instance types are categorized into different family types that offer distinct performnace benefits.\n",
    "        * Microinstances - minimal cost due to low CPU & mempry power\n",
    "        * General purpose - a balance of compute, storage & memory\n",
    "        * Compute optimized - greater focus on compute for high performance \n",
    "        * FPGA instances - filled  programmable gate arrays for massively parallelized work\n",
    "        * GPU instances - graphics processing applications\n",
    "        * Memory optimized - for large scale in memory applications\n",
    "        * Storage optimized - for applications with specific disk I/O and storage capacity requirements\n",
    "* Instance Purchasing Options\n",
    "    * On-Demand Instances - can be launched at any time for a flat rate. Can run as long/short as needed. Pay by the second & stop paying when the instance stops. These are typically used for short-term applications e.g. testing and development environments.\n",
    "    * Reserved Instances - purchase a set period of time (1 or 3 years) for reduced cost. Best applied to long term, predictable workloads\n",
    "        * All upfront: pay all for the 1 or 3 year arrangement\n",
    "        * Partial upfront: pay partial on a reservation for a smaller discount\n",
    "        * No upfront: get the smallest discount \n",
    "    * Scheduled Instances - pay for reservations on a recurring schedule, either daily, weekly or monthly. You can set up a schedule to run at a set time frame without committing to a 1 or 3 year reservation. However, if you don't use the instance, you will still be charged. \n",
    "    * Spot Instances - Bid for unused EC2 Compute resources. However, there is no guarentee on the period of time. Just need to outbit the AWS spot price which fluctuates with demand. If the AWS price goes above your bit price, you'll get a 2min warning before the enstance is removed from your environment. Bonus: you might score a large EC2 at a very low price, just be sure that you run processes that can be interrupted without harm (batch processes)\n",
    "    * On-Demand Capacity Reservations - reserve capacity based on different attributes such as instance type, platform, tenancy.\n",
    "* Tenancy - relates to the underlying host your EC2 instance will reside on, so essentially the physical server within an AWS Data Center. If you don't need to address any compliance or security issues that require dedicated tenancy, best to go with shared tenancy to reduce overall costs.\n",
    "    * Shared Tenancy - EC2 instance is launched on any available host with the required resources. The same host can be used by multiple customers. AWS security mechanisms prevent one EC2 instance from accessing another on the same host\n",
    "    * Dedicated Instances - Hosted on hardware that no other customer can access. May be required to meet regional/organization compliance. These dedicated instances incurr additional charges\n",
    "    * Dedicated Hosts - Additional visibility abd controol on the physical host. allows to use the same dedicated host to multiple instances.\n",
    "* User Data - allows you to enter commands what will run during the first boot cycle of the instance. Helpful to pull down additional resources and get your environment set up faster. Also get OS updates\n",
    "* Storage Options - selecting storage for your EC2 instance will depend on the instance selected, what you intend to use the instance for and how critical the data is.\n",
    "    * Persistent Storage - available by attaching EBS volumes. EBS volumes are seperated from the EC2 insance and the volumes are attached via AWS network components. Not dissimilar to attaching a flash drive to you computer. Data is automatically duplicated for resiliancy. You can disconnect the colume fro the EC2 instance and still mainttain the data. Can also implement encryptions and take backup storage snapshots\n",
    "    * Ephemeral Storage (temporary) - created by EC2 instances using local storage. Physically attached to the underlying host (similar to a PC's hardrive). Data will remain intact after reboot. However, will NOT persist once the instance is stopped. You cannot detatch ephemeral storage from the instance.\n",
    "* Security - during creation of your EC2 instance you will be asked to select a Security Group for your instance. basically a firewall to protect access (in/out) of the resource. [For more on instance level security](https://cloudacademy.com/blog/aws-security-groups-instance-level-security/)\n",
    "    * at the end of creating your EC2 instance, you will need to create an existing key pair or create a new one. The key pair are Public and Private Keys. The purpose of the keys is to encrypt the login information for Linux and Windows EC2 instances, and tthen decrypt the same information allowing you to authenticate onto the instance.\n",
    "    * The Pubic key is held and kept by AWS, the Private key is the user's responsibility to keep and ensure that it is not lost. It is possible to use the same key pair for different instances. \n",
    "    * It is the user's responsibility to maintain and install the latest OS and security patches released by the OS vendor as dictated witthin the AWS shared responsibility model.\n",
    "* Status Checks - help monitor the AWS systems requirements used to keep the instance running\n",
    "    * System Status Checks - issue with the underlying host (network, power issues, out of our control & up to AWS. best recourse: restart)\n",
    "    * Instance Status Checks - issue with the configuration of the EC2 instance (kernel, memory, configuration corrupt data etc. requires more troubleshooting)\n",
    "\n",
    "### ECS Elastic Container Service\n",
    "* This service allows the user to run Docker-enabled applications packaged as containers across a cluster of EC2 instances without requiring you to manage a complex and administratively heavy cluster management system. \n",
    "* The burden of managing your own cluster management system is abstracted with the ECS service by passing that responsibility over to AWS, specifically through the use of AWS Fargate.\n",
    "* **AWS Fargate** - is an engine used to enable ECS to run containers without having to manage and provision instances and clusters for containers.\n",
    "* **Docker** is a piece of software that allows the user to automate the installation and distribution of applications inside Linux Containers. [Introduction to Docker](https://cloudacademy.com/course/introduction-to-docker-2/course-intro-1/)\n",
    "* **Containers** hold everythiing an application need to run from within its container package (i.e. system libraries, code, system tools, run time etc.).[Basic use of Containers in Production](https://cloudacademy.com/course/basics-of-using-containers-in-production/introduction-83/) The container does not hold an OS (unlike VMs), because of this, the container is more light weight than spinning up a whole VM. The container is decoupled from the OS making them very portable and scalable\n",
    "With Amazon ECS there is no need to install any management or monitoring software for your cluster  \n",
    "* Launching an ECS Cluster\n",
    "    - Fargate Launch: requires far less configuration: just requires you to specify the CPU and memory required, define networking and IAM policies, in addition to you having to package your application into containers.\n",
    "    - EC2 Launch: has a far greater range of configuration options (more work but more customizable) the user is responsible for patching and scaling the unstances and can specify type and how many containers should be in a cluster.\n",
    "* Monitoring Clusters\n",
    "    - monitoring clusters is taken care of by Amazon CloudWatch.\n",
    "    - the user can easily create alarms based off of these metrics, providing you notification of when specific events occur, such as a when the cluster size scales up or down.\n",
    "* Amazon ECS Cluster\n",
    "    - An Amazon ECS cluster is comprised of a collection of EC2 instances\n",
    "    - Features/concepts familiar from EC2 translate to ECS: security groups, elastic load balancing, auto sclaing etc\n",
    "    - although ECS is a cluster of EC2 instances, the ECS instance still operates very similar to a single EC2 instance\n",
    "    - clusters act as a resource pool: aggregating resources such as CPU and memory\n",
    "    - Clusters are dynamically scalable and multiple instances can be used. can sart with 1 and dynamically scale to thousands\n",
    "    - multiple instance types can be used within the same cluster if required\n",
    "    - clusters can only scale within a single region (Amazon ECS is region specific)\n",
    "    - Containers can be schedules to be deployed across your cluster\n",
    "    - Instances within the cluster also have a Docker daemon and an ECS agent, so Amazon ECS commands can be translated in the Docker commands\n",
    "\n",
    "\n",
    "### ECR Elastic Container Registry\n",
    "* ECR provides a secure location to store and manage Docker images. This is a fully managed service, so the user doesn't have to provision any infrastructure to create the registry of docker images.\n",
    "* Service is typically used by developers to push, pull and manage their library of docker images in a central and secure location.\n",
    "* Components used in ECR:\n",
    "    - Registry: the object that allows the user to host and store Docker images as well as create image repositories. the default registry: `http://aws_account_id.dkr.ecr.region.amazonaws.com` Your account will have both read/write access by default to any images you create within the registry and any repositories. Access to your registry and images can be controlled via IAM policies in addiion to repository policies for more customized sercurity management. Because the Docker client is technically outside of AWS, before the client can access your ECR registry, it needs to be authenticated as an AWS user via an Auth token.\n",
    "    - Authorization tokens: to begin the authorization process so the Docker client and ECR registry can communicate, run the get-login command using the AWS CLI: `aws ecr get-login-password --region region --no-include-email` This will produce an output response which will be a docker login command: `docker login -u AWS -p password https://aws_account_id.dkr.ecr.region.amazonaws.com` this output gets copypasta'ed to the user's Docker CLI which will take it from there. The result of this will generate an Authorization toekn that will be valid for 12hrs, afterwhich the process will need to be repeated.\n",
    "    - Repository: objects within the registry which allow the user to group together and secure different docker images. There can be multiple repositories for a given registry that allow the user to organize and manage docker images into different categories.\n",
    "    - Repository Policy: different [IAM managed policies](https://cloudacademy.com/course/overview-of-aws-identity-and-access-management-iam/introduction-81/) to help control access to ECR. ex: AmazonEC2ContainerRegistryFullAccess, AmazonEC2ContainerRegistryPowerUser, AmazonEC2ContainerRegistryReadOnly. Repository policies are resource-based policies: must be associated with a principal to determine who has access and what permissions they have, access is granted through access to the ecr:GetAuthorizationToken API call, then repositories policies control what actions the user can perform for each repository.\n",
    "    - Image: Once the registry is configured & docker client is authenticated, docker images can be stored in the required repositories.\n",
    "        - pushing a docker image: [docker push command](https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html)\n",
    "        - retrieve an image: [docker pull command](https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html)\n",
    "\n",
    "\n",
    "### EKS Elastic Container Service for Kubernetes\n",
    "* [**Kubernetes**](https://cloudacademy.com/course/introduction-to-kubernetes/introduction/) is an open source container orchestration tool designed to automate, deploy, scale and operate containerized applications. It can grow from tens, thousands or even millions of containers. is container runtime (docker vs rocket) agnostic\n",
    "* [EKS](https://cloudacademy.com/course/introduction-to-aws-eks/course-introduction/) is a manged service allowing the user to run Kubernetes across AWS infrastructure without having to take care of provisioning and running the Kubernetes maagement infrastructure (the control plane). Rather, the user only needs to provision the worker nodes.\n",
    "* Kubernetes Control Plane: a number of different APIs, the kubelet processes and the Kubernetes Master. The control plane schedules containers onto nodes (schedule: the decision process of placing containers onto nodes in accordance to declared compute requirements). The control plane tracks the state of all kubernetes objects by monitoring the objects\n",
    "* What EKS does: AWS becomes respinsible for provisioning, scaling and managing the control plane, and they do this by utilizing multiple availability zones for addtional resilience.\n",
    "* Nodes: Kubernetes clusters are composed of nodes. Cluster refers to the aggregate of nodes. A node is a worker machine in kubernetes which runs as an on-demand EC2 instance and includes software to run containers. for security: each node requires an MI. Once the worker nodes are provisioned they can connect to the EKS using an endpoint\n",
    "* Working with EKS\n",
    "    1. Create an EKS Service Role: need to configure and create an IAM service role that allows EKS to provision and configure specific resources with the following permission policies: AmazonEKSServicePolicy AmazonEKSClusterPolicy\n",
    "    2. Create an EKS Cluster Virtual Private Cloud (VPC): create and run a CloudFormation stack which will configure a new VPC to use with EKS\n",
    "    3. Install kubectl and the AWS-IAM-Authenticator\n",
    "        - [Kubectl](https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html) is a command line utility for Kubernetes\n",
    "        - [IAM-Authenticator](https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html) is required to authenticate with the EKS cluster\n",
    "    4. Create an EKC Cluster: use the details from the VPC created in step 1&2\n",
    "    5. Configure kubectl for EKS: `update-kubeconfig` via AWS CLI to create a kubeconfig file for your EKS cluster\n",
    "    6. Provision and configure worker nodes: once the EKS cluster is active, launch worker nodes using CloudFormation\n",
    "    7. Configure the Worker Node to join the EKS Cluster: use the configuration map & update the ARN of instance role with the NodeInstanceRole generated in step 6\n",
    "    8. Deploy!\n",
    "\n",
    "### AWS Elastic Beanstalk\n",
    "AWS Elastic Beanstalk is an AWS managed service that takes your uploaded code of your web application and automatically provisions and deploys the required resources within AWS to make the web application opertional including: EC2, Auto Scaling, application health-monitoring and Elastic Load Balancing in addition to capacity provisioning\n",
    "* Elastic Beanstalk is an ideal service for engineers who may not have the familiarity or the necessary skills within AWS to deploy, provision, monitor and scale the correct environment to run the developed applications. The resposibility is passed on to AWS Elastic Beanstalk to deploy the correct infrastructure to run the uploaded code. Maintenance tasks can be performed from within the Elastic Beanstalk dashboard.\n",
    "* Elastic Beanstalk is compatible with many languages\n",
    "* Elastic Beanstalk is free to use, but ou pay for whatever services get provisioned as a result.\n",
    "Elastic Beanstalk Core Components:\n",
    "* Application Version: a very specific reference to a section of deployable code. will pint typically to S3 to where the deployable code may reside.\n",
    "* Environment: an application version that has been deployed on AWS resources, which are configured and provisioned by AWS Elastic Beanstalk. At this stage, the application is deployed as a solution and becomes operational whitin you environment. The environment is comprised of ALL the resources created by Elastic Beanstalk and not just an EC2 instance with your uploaded code.\n",
    "* Environment Configurations: collection of parameters and settings that dictate how an environment will have its resources provisioned by Elastic Beanstalk and how these resources will behave.\n",
    "* Environment Tier: how EB provisions resources based on what your application is designed to do.\n",
    "    - Web server environment: handles http requests. services typically inclue Route 53, Elastic Load Balancer, Auto Scaling, EC2, Security Groups etc.\n",
    "    - Worker environment: does nothandle http requests but instead pulls data from, say, SQS queue. typically also includes IAM service roles, AUto Scaling and EC2\n",
    "* Configuration Template: This is the template that provides the baseline for creating a new, unique, environment configuration\n",
    "* Platform: Platform is a culmination of components in which you can build your application upon using Elastic Beanstalk: the OS, programming language, server type, components of Elastic Beanstalk itself\n",
    "* Applictions: An application is a collection of different elements, such as environments, environment cofiguations and application versions. can be deployed across one of the two environment tiers\n",
    "Elastic Beanstalk Workflow. a very simple workflow process for yor application deployment and ongoing management\n",
    "1. Create Application\n",
    "2. Upload version as well as info about configuation. this creates the Environment Configuration\n",
    "3. Launch the Environment with appropriate resources to run the appliction\n",
    "4. Manage Environment and any new versions etc.\n",
    "\n",
    "\n",
    "### AWS Lambda\n",
    "AWS Lambda is a serverless compute service that allows you to run your application code without having to manage EC2 instances  \n",
    "**Serverless** means that you do not need to worry about provisioning and managing your own compute resources to run your own code, instead this is managed and provisioned by AWS. The Lambda service does require compute resources to run your code requests, but because the AWS user does not need to be concerned with managing this compute power or where it is provisioned from, it is considered 'serverless' from the user perspective.  \n",
    "Lambda bonus: \n",
    "* if you don't have to spend time provisioning (operating, managing, patching, securing) an EC2 instance, then you have more time to focus on building the application.\n",
    "* Cost Optimized: you only ever have to pay for compute power when Lambda is in use via Lambda Functions. AWS Lambda charges compute power per 100ms of use only when your code is running, in addition to the number of times your code runs.\n",
    "Working with AWS Lambda:\n",
    "1. You can either upload your code in Lambda, or writ it with the code editors Lambdas provides. Supported languages: Node.js, Java, C#, Python, Go, PowerShell, Ruby\n",
    "2. Configure your Lambda frunctions to execute upon specific triggets from supported event sources\n",
    "3. Once the specific trigger is initiated, Lambda will run your code (as per your Lambda function) using only the required compute power as defined.\n",
    "4. AWS recods the compute time in milliseconds and the quantity of Lambda functions run to ascertain the cost of the service.\n",
    "Components of AWS Lambda:\n",
    "* The Lambda Function: code the Lambda will invoke per defined triggers\n",
    "* Event Sources: AWS services that can be used to trigger your lambda functions [Using AWS Lambda with other services](https://docs.aws.amazon.com/lambda/latest/dg/lambda-services.html#supported-event-source-s3)\n",
    "* Trigger: an operation from an event source that causes the function to invoke (ex: an S3 'PUT' request)\n",
    "* Downstream Resources: resources that are required during the execution of your Lambda Funcion. These aren't the resources that trigger the Lambda, rather they are the resources required once the Lambda has been triggered and used once the code in the Lambda is executed.\n",
    "* Log streams: can add log statements to help identify and troubleshoot issues with the Lambda function. the log stream is essentially a sequence of events that all come from the same functtion and are recorded by CloudWatch.\n",
    "Creating Lambda Functions\n",
    "* Selecting a Blueprint: AWS Lambda provides many blueprints ex: S3-get-object - an S3 trigger tthat retrieves metadata\n",
    "* Configure Triggers: define triggers for the Lambda. ex: specifying the S3 bucket for your function\n",
    "* Configure Lambda Function: upload code or edit in-line. define the required resources, maximum execution timeout, IAM Role and Handler Name\n",
    "**AWS Lambda** is a highly scalable serverless service coupled with fantastic cost optimization compared to EC2 as you are only charges for Compute power while the code is running and for the number of functions called\n",
    "\n",
    "### AWS Batch\n",
    "AWS Batch is used to manage and run batch computing workloads within AWS.  \n",
    "* **Batch Comouting** - used in specialist use cases which require a vast amount of compute power across a cluster of compute resources to complete batch processing executing a series of tasks. \n",
    "* AWS Batch romoves a lot of the constraints that make local batch processing difficult such as appropriating/maintaining the resources/hardware, administration activities and maintenance tasks.  \n",
    "* AWS Batch allows the user to create a custer of compute resources which is highly scalable taking advantage of the elasticity of AWS which can cope with any level of batch processing whilst optimizing the distribution of the workloads. All provisionng, monitoring, maintenance and management of the clusters is taken care of by AWS. (no software to install on your machine)\n",
    "AWS Batch Components\n",
    "1. Jobs - a job is a unit of work to be run by AWS Batch. can be an executable file, an application within an ECS cluster or shell script. Jobs run on EC2 instances as a containerized application. Jobs can have different states: 'Submitted', 'Pending', 'Running' etc.\n",
    "2. Job Definitions - define specific parameters for the Jobs themselves and dictate how the Job will run and with what configuration. Ex: how many CPUs to use for the container, which data volumes to use, which IAM roles, mount points.\n",
    "3. Job Queues - jobs that are scheduled are placed in a Job Queue until they run. there can be multiple queues with different priorities, on-demand and spot instances are supported. Furthermore, AWS Batch can bid on spotinstances for yyou.\n",
    "4. Job Scheduling - the job scheduler takes care of when a job should be run and from which compute environment. Typically will operate on a first in first out basis. the scheduler makes sure that higher priority queues are run first.\n",
    "5. Compute Environments - environments containing the compute resources to carry out a job.\n",
    "    - Managed Environments - the service will handle the provisionng, scaling and termination of Compute instances. the environment is created in an ECS cluster\n",
    "    - Unmanaged Environments - these are environments provisioned, managed and maintained by the user. Gives greater customization but requires greater administration and maintenance such as creating the necessary Amazon ECS cluster\n",
    "* If you have a requirement to run multiple jobs in parallel using Batch computing, for example, to analyze financial risk models, perform media transcoding or engineering simulations, then AWS Batch would be a perfect solution.\n",
    "\n",
    "### Amazon Lightsail\n",
    "Amazon Lightsail is essentially a VPS backed by AWS infrastructure, much like an EC2 instance but without as many configurable steps throughout its creation. \n",
    "* It has been designed to be simple, quick and very easy to use at a low cost point for small scale use cases by small businesses or for single users.\n",
    "    - commonly used to host simple websites, small applications and blogs\n",
    "    - you can run multiple Lightsail instances together allowing them to communicate\n",
    "    - it is possible to connect it to other AWS resources and to your existing VPC running within AWS via a peer connection\n",
    "Deploying a Lightsail instance\n",
    "* a Lightsail instance can be deployed from a single page with just a few simple configuration options.\n",
    "* Amazon Lighsail can be accessed either via the AWS console under the Compute category, or directly to the [homepage of AWS Lightsail](https://aws.amazon.com/free/compute/lightsail/) which sits outside of the AWS Management Console\n",
    "* very simple 1-page configuration. pricing is on-demand (only charged when in use)\n",
    "* need to add a unique name to the Lightsail instance and are encouraged to use key-value pairs to organize resources\n",
    "Monitoring options:\n",
    "* Connect - using ssh\n",
    "* Storage - montor disk storage\n",
    "* Metrics - graphical views of the instance (CPU intialization etc)\n",
    "* Networking - very simple view of IP address and a simple firewall to sellect acceptable ports\n",
    "* Snapshots - a simle way to backup your instance.\n",
    "* Tags - organize key-value tags to organize resource\n",
    "* History - whe configuration changes occur\n",
    "Amazon Lightsail provides a lightweight solution for small projects and use cases which can be deployed quickly and cost effectively in just a few clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6611da95-d6e3-4b39-89ea-74ec815c99ef",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Elastic Load Balancing\n",
    "\n",
    "### What is an Elastic Load Balancer?\n",
    "The main function of ELB is to help manage and control the flow of inbound requests destined to a group of targets by distributing these requests evenly across the targeted resource group. Targets could be a fleet of EC2 instances, Lambdas, a range of IP addresses or even Containers. Targets defined within ELB could be situated across different Availability Zones, or all placed within a single AZ.  \n",
    "* Scenario:: \n",
    "    * Suppose you have created a new application which is currently residing on a single EC2 instance within your environemnt that is being accessed by a number of users.\n",
    "        - It might work. it could provide a service for multiple users, but this infrastructure brings some challenges\n",
    "        - What if the instance fails? then the application is down for everyone.\n",
    "        - what if there is a sudden spike in traffic? the instance might not be able to handle the additional load based on performance limitations\n",
    "    * To strengthen your infrastructure and help remediate these challenges, you shoud introduce an ELB and additional instances running your application\n",
    "        - AWS ELB will act as the point for receiving incoming traffic from users and evenly distributes traffic across a greater number of instances.\n",
    "        - The ELS is highly available as this is a managed service provided by AWS\n",
    "        - if there is a problem and one of the instances fails, ELB will automatically detect the failure based on defined metrics and will divert any traffic to the remaining 2 healthy instances\n",
    "        - if there is a surge in traffic, the additionaly instances will help distribute the surge load.\n",
    "One of the main advantages of using ELB is the fact that it is managed by AWS, and is by definition, elastic.\n",
    "   - It will automatically scale to meet your incoming traffic needs by scaling both up and down. \n",
    "   - if you were doing this by yourself, then you would need to worry about scaling a load balancer and finding a way to enforce high availability\n",
    "   - With an AWS ELB, you can create your load balancer and enable dynamic scaling with just a few clicks\n",
    "Load Balancer Types\n",
    "1. Application Load Balancer - for web applications running the HTTP or HTTPS protocols. operates at request level. Advanced routing TLS termination and visibility features targeted at application architectures\n",
    "2. Network Load Balancer - operates on connection level, routing traffic to targets within your VPC for ultra-high performance while maintaining very low laitencies. handles millions of requests per second\n",
    "3. Classic Load Balancer - used for applications that were built in the EC2 classic environment. Operates at the connection and request level.\n",
    "ELB Components  \n",
    "You ELB can contain 1 or more listeners, each listener can contain 1 or more rules, each rule can contain 1 or more conditions, and all conditions in the rule equal an action.\n",
    "* Listeners = for every load balancer, you must configure at least one listener. The listener defines ow your inbound connections are routed to you target groups based on port and protocols set as conditions\n",
    "* Target Groups = a target group is a group of resources that you want your ELS to route requests to. You can configure your ELB with a number of different target groups, each associated with a different listener configuration and associated rules.\n",
    "* Rules = are associated to each listener that you have configured within your ELB. Rules help to define how an incoming request gets routed to which target group. Example: IF Source IP is XXXXX and Http request method is PUT THEN Forward to Group1\n",
    "* Health Checks = is performed against the resources defined within the target group. Thes health checks allow the ELB to contactt each target using a specific protocol to receive a response\n",
    "* Internet-Facing ELS = the nodes of the ELB are accessible via the internet and so have a public DNS name that can be resiolved to its public IP address, in addition to an internal IP address. This allows the ELB to serve incoming requests from the internet before distributing and routing the traffic to your target groups.\n",
    "* Internal ELB = An internal ELb only has an internal IP address, this means that it can only serve requests that originate from within the VPC.\n",
    "* ELB Nodes = for each Availability Zone selected, an ELB node will be placed withini that AZ. You need to ensure that you have an ELB node associated to any AZ that you want to route traffic. The nodes are used by the ELB to balance traffic to your target groups.\n",
    "* Cross-Zone Load Balancing = depending on which ELB option you select, you may have the option of enabling and implementing Cross-Zone load balancing within your environment. When cross-zone load balancing is disabled, each ELB in it's associated AZ will distribute traffic within the targets in its AZ only. However, when cross-zone load balancing is enabled, the ELBs will distribute all incoming traffic evenly between all targets in the AZs.\n",
    "\n",
    "\n",
    "### SSL Server Certificates\n",
    "When using HTTPS as a listener\n",
    "* HTTPS allows an encrypted communication channel to be set up between clients initiating the request and your ALB (application load balancer). To allow your ALB to recieve encrypted traffic over HTTPS it will need a server certificate and an associted security policy. SSL (Secure Sockets Layer) is a cryptographic protocol, much like TLS (Transport Layer Security). Both SSL and TLS are used interchangeably when discussing certificates on your ALB.\n",
    "* There server certificste used by the ALB is an X.509 certificte, which is a digital ID provisioned by a Certificate Authority such as AWS Certificate Manager (ACM). The certificate is used to terminate the encrypted connection received from the remote client, and then the request is decrypted and forwarded to the resources in the ELB target group\n",
    "* When you select HTTPS as your listener, you will be asked to select a certificate using 1 or 4 different options:\n",
    "    * ACM = choose or load an ACM certificate (recommended). can create/provision SSL/TLS server certificates to be used within your AWS environment across different services\n",
    "    * IAM = choose or load an IAM to use a 3rd party option. IAM is used as your certificate manager when deploying you ELBs in regions that are not supported by ACM [Managing service certificates in IAM](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html)\n",
    "[General Info on AWS Certificates](https://docs.aws.amazon.com/acm/latest/userguide/acm-overview.html)  \n",
    "\n",
    "\n",
    "\n",
    "### Application Load Balancers\n",
    "In the [OSB layer model](https://en.wikipedia.org/wiki/OSI_model), Application Load Balancers sit at layer 7, the Application layer.  \n",
    "**ApplicationLayer** - serves as the interface for users and application processes to access network services. application processes include: http, ftp, smtp and nfs. [Cloud Academy class on OSI model](https://cloudacademy.com/course/osi-and-tcp-ip-networking-models/osi-and-tcp-ip-networking-models/)  \n",
    "* Chose an Application Load Balancer when you need a flexible feature set for youe web applications with HTTP or HTTPS traffic. Operating at the request level, Application Load Balancers provide advanced routing and visibility feeatures targeted at application architectures, including microservices and containers.\n",
    "* First thing to do is to configure **Target Groups** - a target group is simply a group of resources that you want your ELB to route requests to. You might want to configure different target groups depending on the nature of your requests.\n",
    "* How to set up target groups from console, go to EC2, go to Load Balancers, go to Target Group. Create Target Group & fill in page. Target type can be instance, IP, or Lambda. also need to select web protocol (ex: HTTP), Port (ex: 80), and the appropriate VPC.  In Target group: Webservers, got to Targets to associate Targets with the newly created Target Group\n",
    "* Configuring the ALB: from console, go to EC2, go to Load Balancers, go to Create Load balancer, go to Application Load Balancer: give name define listeners, select Availability Zone, Configure Security Settings, Configure Security Groups, Configure Routing, Register the Targets and thenReview the configuration. Create!\n",
    "* adding new rules. Click on the listener, select '+' and are given option to insert rule. Select to add a rule. add a condition (ex: a source IP) then take an action (ex: a different target group). This let you use youe ALB to direct traffic.\n",
    "\n",
    "### Network Load Balancers\n",
    "The principles between the ALB and the Network load balancer are the same. However, where the ALB works at the application layer (OSI \\#7) analyzing the HTTP header to direct traffic, the **Network Load Balancer** operates at OSI level \\#4 enabling the NLB to balance requests purely based upon the TCP protocol. \n",
    "* Choose an NLB when you need ultra-high performance, TLS offloading at scale, centralized certificate deployment, support for UDP, and static IP addresses for your application. Operating at the connection level, NLBs are capable of handling millions of requests per second securely while maintaining ultra-low latencies.\n",
    "* If you application logic requires a static IP address, then the NLB will be your choice of elastic load balancer\n",
    "* Cross-Zone Load Balancing is different for the NLB compared to the ALB.\n",
    "    - the ALB has cross-zone load balancing always enabled\n",
    "    - the NLB can have cross-zone load balancing enabled or disabled.\n",
    "        * The NLB node uses an algorithm which uses details based on the TCP sequence, the protocol, source port, source IP, destination port and destination IP to select the target in the zone to process the request.\n",
    "        * when the TCP connection is established with a target host then that connection will remain open with that target for the duration of the request\n",
    "* Configuring an NLB: from Console go to EC2, go to Load Banlacing, select load balancer, create load balancer, select network load balancer. Configure name, listeners, protocol and availability zone(s), configure security settings, configure Routing, register the Targets and review the configuration. Create!\n",
    "\n",
    "### Classic Load Balancers\n",
    "The Classic Load Balancer supports the TCP, SSL/TLS, HTTP and HTTPS protocols  \n",
    "It is considered best practice to **use the ALB over the Classic Load Balancer** unless you have an existing application running in the EC2-Classic network.\n",
    "* **EC2 Classic** - this legacy platform enabled users to deploy EC2 instances in a single, flat network shared with other customers, instead of inside a VPC. EC2-Classic is no longer supported for newer AWS accounts.\n",
    "* Although the CLB doesn't provide as many features as the ALB, it does offer the following which the ALB lacks:\n",
    "    * Supports EC2-Classic\n",
    "    * Supports TCP and SSL listeners\n",
    "    * Supports sticky sessions using application generated cookies\n",
    "* Configuring a CLB: form console, go toEC2, go to Load balancers, create a new load balancer, select CLB previous generation, do basic configuration (listener, AZs), assign security groups, configure security settings, configure health checks, add EC2 instances (NO TARGET GROUPS), add tags, and review. Create!\n",
    "\n",
    "[**Comparing Load Balancers**](https://aws.amazon.com/elasticloadbalancing/features/#compare). We see that the ALB is much more feature rich, however, the NLB supports tome significant differences to that of the ALB, such as support for static IPs, EIPs, and preserving source IP addresses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e019059a-fed8-41bf-9a72-248c9025c6a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "## EC2 Auto Scaling\n",
    "\n",
    "\n",
    "### EC2 Auto Scaling\n",
    "\n",
    "\n",
    "### Components of EC2 Auto Scaling\n",
    "\n",
    "\n",
    "### Using ELB and Auto Scaling Together\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9231d60e-1dcc-4790-965c-7b79eef12dd1",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "***\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## Other Resources\n",
    "\n",
    "1. [Crushing the AWS Cloud Practitioner Exam](https://www.capitalone.com/tech/cloud/crushing-the-aws-ccp-exam/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
