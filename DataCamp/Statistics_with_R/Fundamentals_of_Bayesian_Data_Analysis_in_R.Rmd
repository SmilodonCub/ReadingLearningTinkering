---
title: 'Fundamentals of Bayesian Data Analysis in `R`'
subtitle: 'DataCamp: Statistics with `R`'
author: 'Bonnie Cooper'
output:
  rmdformats::downcute
---

![](https://www.kdnuggets.com/wp-content/uploads/datacamp-logo.png){width=150%}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}

library( dplyr )
library( ggplot2 )
library( gridExtra )
library( tidyverse )
library( ggridges )
library( ggExtra )
library( lattice )
library( RColorBrewer )
```


## What is Bayesian Inference Work?

### A First Taste of Bayes

Bayesian Inference in a Nutshell:  
A method for figuring out unobservable quantities given known facts that uses probability to describe the uncertainty over what the values of the unknown quantities could be.  

A very interesting summary of Alan Turing's use of Bayesian inference to crack the enigma code is given. (use the decrypted message to find the probable setting of the enigma machine wheels) A more full treatment of the topic can be found here: [Alan Turing and Enigma Statistics](http://www.mathcomp.leeds.ac.uk/turing2012/Images/Turing_Statistics.pdf)

Bayesian Data Analysis:  

* The use of bayesian inference to learn from data (the known facts) inference about parameters.  
* Can be used for hypothesis testing, linear regressin, etc.
* It's real power: Is flexible and allows you to construct problem-specific models

Bayesian Data Analysis: a tool to make sense of your data

### Let's Try Some Bayesian Data Analysis
Bayesian inferece == Probabilistic inference.  

Probability:  

* A number from 0 to 1
* A statement about the certainty / uncertainty of an event
* 1 is complete certainty that a case is `TRUE`
* 0 is complete certainty that a case is `FALSE`
* Not only for binary yes / no events, but can be applied to continuous variables

The role of probability distributions in Bauesian data analysis is to represent uncertainty, and the role of Bayesian inference is to update probability distributions to reflect what has been learned from data.  

Let's look at a brief illustrative example,  
**A Bayesian model for the proportion of success**  
ex: What is the proportion ofsuccessful treatments with a new drug?  

* `prop_model( data )`
* `data` is a vector of failures represented by 1s and 0s
* There is an unknown underlying proportion of success
* If the data point is a success is only affected by the proportion of success
* Prior to seeing any data, any underlying proportion of success is equally likely
* The result is a probability distribution that represents what the model knows about the underlying proportion of success

The output of prop_model is a plot showing what the model learns about the underlying proportion of success from each data point in the order you entered them. At n=0 there is no data, and all the model knows is that it's equally probable that the proportion of success is anything from 0% to 100%. At n=4 all data has been added, and the model knows a little bit more.  
will build toward implementing `prop_model()` during this course.

...but here is the code from the course author [Rasmus Bååth](https://www.fil.lu.se/person/RasmusBaath)
```{r}
#The prop_model function - Rasmus Bååth R code

# This function takes a number of successes and failuers coded as a TRUE/FALSE
# or 0/1 vector. This should be given as the data argument.
# The result is a visualization of the how a Beta-Binomial
# model gradualy learns the underlying proportion of successes 
# using this data. The function also returns a sample from the
# posterior distribution that can be further manipulated and inspected.
# The default prior is a Beta(1,1) distribution, but this can be set using the
# prior_prop argument.

# Make sure the packages tidyverse and ggridges are installed, otherwise run:
# install.packages(c("tidyverse", "ggridges"))

# Example usage:
# data <- c(TRUE, FALSE, TRUE, TRUE, FALSE, TRUE, TRUE)
# prop_model(data)
prop_model <- function(data = c(), prior_prop = c(1, 1), n_draws = 10000,
                       gr_name="Proportion graph") {
  #library(tidyverse)
 
  data <- as.logical(data)
  # data_indices decides what densities to plot between the prior and the posterior
  # For 20 datapoints and less we're plotting all of them.
  data_indices <- round(seq(0, length(data), length.out = min(length(data) + 1, 40)))
  
  # dens_curves will be a data frame with the x & y coordinates for the 
  # denities to plot where x = proportion_success and y = probability
  proportion_success <- c(0, seq(0, 1, length.out = 100), 1)
  dens_curves <- map_dfr(data_indices, function(i) {
    value <- ifelse(i == 0, "Prior", ifelse(data[i], "Success", "Failure"))
    label <- paste0("n=", i)
    probability <- dbeta(proportion_success,
                         prior_prop[1] + sum(data[seq_len(i)]),
                         prior_prop[2] + sum(!data[seq_len(i)]))
    probability <- probability / max(probability)
    data_frame(value, label, proportion_success, probability)
  })
  # Turning label and value into factors with the right ordering for the plot
  dens_curves$label <- fct_rev(factor(dens_curves$label, levels =  paste0("n=", data_indices )))
  dens_curves$value <- factor(dens_curves$value, levels = c("Prior", "Success", "Failure"))
  
  graph_label <- paste("Prior likelihood distribution Beta(a =", 
                       as.character(prior_prop[1]),", b =",
                                    as.character(prior_prop[2]),")") 
  
  p <- ggplot(dens_curves, aes(x = proportion_success, y = label,
                               height = probability, fill = value)) +
    ggridges::geom_density_ridges(stat="identity", color = "white", alpha = 0.8,
                                  panel_scaling = TRUE, size = 1) +
    scale_y_discrete("", expand = c(0.01, 0)) +
    scale_x_continuous("Proportion of success") +
    scale_fill_manual(values = hcl(120 * 2:0 + 15, 100, 65), name = "", drop = FALSE,
                      labels =  c("Prior   ", "Success   ", "Failure   ")) +
    ggtitle(paste0(gr_name, ": ", sum(data),  " successes, ", sum(!data), " failures"),
            subtitle = graph_label) +
    labs(caption = "based on Rasmus Bååth R code") +
    theme_light() +
    theme(legend.position = "top")
  print(p)
  
  # Returning a sample from the posterior distribution that can be further 
  # manipulated and inspected
  posterior_sample <- rbeta(n_draws, prior_prop[1] + sum(data), prior_prop[2] + sum(!data))
  invisible(posterior_sample)
}
```

Zombie apocalypse: let us say there is a new drug to treat zombieism. It has never been tested before, but the results from this pilot test have two recoveries out of 4 subjects. Here, `prop_model` plots the successive posterior probabilities that the zombie antidote will be successful:
```{r}
data <- c( 1, 0, 0, 1 )
prop_model( data )
```

Next we observe several more subjects, however, they are all failures, leaving just 2 successes out of 13 subjects. Here is how the posterior probability changes with the new case information:
```{r}
data <- c( 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0 )
prop_model( data )
```

What if we observe a lot more?
```{r}
data <- rbinom( 50, 1, 0.2 )
prop_model( data )
```

Compare this experimental zombie drug's success with that of the currently used drug which has a 7% success rate:
```{r}
data2 <- rbinom( 50, 1, 0.07 )
posterior2 <- prop_model( data2 )
head( posterior2 )
```
Subjective obseration time: the more information collected about a variable, the closer the observed prior distribution approaches the 'true' probability of a success.


* Take a **prior probability distribution** use observations from the data to update a **posterior probability distribution**
* **prior** - a probability distribution that represents what the model knows before seeing the data
* **posterior** - a probability distribution that represents what the model knows after having seen the data.

```{r}
data = c(1, 0, 0, 1, 0, 0,
         0, 0, 0, 0, 0, 0, 0)
         
# Extract and explore the posterior
posterior <- prop_model( data )
head( posterior )
```
plot the posterior distribution:
```{r}
hist( posterior, breaks = 30, xlim = c( 0,1 ), col = 'palegreen4' )
```

Calculate a few descriptive statistics for the posterior distribution:
```{r}
# Calculate the median
median(posterior)

# Calculate the credible 90% interval
quantile(posterior, c(0.05, 0.95))

# Calculate the probability that the success rate is greater than 0.7%
sum( posterior > 0.07 )/length(posterior)
```

Now let's try to generate a similar plot with ggplot:
```{r}
post_df <- data.frame( 'posterior' = posterior )

ggplot( post_df, aes( x = posterior ) ) +
  geom_histogram( fill = 'palegreen4', color = "#e9ecef" ) +
  ggtitle( 'Posterior Probability Distribution' )
```

and display some summary statistics:
```{r}
summary( post_df )
```


How to interpret these summary statistics?  
    "Given the data of two cured and 11 relapsed zombies, there is a 90% probability that the drug cures between 6% and 39% of treated zombies. Further, there is a 93% probability that this new drug is more effective that the current zombie drug."  
    
Compare the 2 posterior probability distributions in an overlapped histogram:
```{r}
post_df <- data.frame( 'posterior' = posterior, 'posterior2' = posterior2 )
post_long <- post_df %>%
  pivot_longer( cols = everything(), names_to = 'dist', values_to = 'prob' ) %>%
  ggplot( aes( x = prob, fill = dist ) ) +
  geom_histogram( color = "#e9ecef" ) +
  scale_fill_manual(values=c("palegreen4", "#404080"))
post_long
```



## How does Bayesian Inference Work?

### The Parts Needed for Bayesian Inference

Bayesian Inference:  

* $\mbox{Bayesian Inference } = \mbox{Data } + \mbox{ a Generative Model } + \mbox{ Priors }$
* **Generative Model**: can be a computer program, mathematical expression or a set of rules that can be fed parameters values to be used to simulate data.

```{r}
#Paramters
prop_success <- 0.15
n_zombies <- 13
#simulate the data with rbinom()
data <- rbinom( n_zombies, 1, prop_success )
data
```

This similar function was presented in the course video:
```{r}
data <- c()
for( zombie in 1 : n_zombies ) {
  data[ zombie ] <- runif( 1, min = 0, max = 1 ) < prop_success
}
data <- as.numeric( data )
data
```
This function gives an idea of the logic for the prevous code. However, using `rbinom()` is more efficient. Now run the generative model where the success is estimated at 42% and the test is done with 100 zombies.
```{r}
prop_success <- 0.42
n_zombies <- 100
#simulate the data with rbinom()
data <- rbinom( 1, n_zombies, prop_success )
data
```
37 zombies got cured in this run of the generative model. How about if it is run 200 times?
```{r}
data <- rbinom( 200, n_zombies, prop_success )
data
```
Above are the results of 200 simulations of the binomial generative model with the given parameters. 
Show some summary stats for this distribution...
```{r}
summary( data )
mean( data )
```

### Using a Generative Model
apply our generative model to a relatively large number of simulated trials. The results will for the probability distribution for our process:
```{r}
cured_zombies <- rbinom( n = 100000, size = 100, prob = 0.07 )
post_df <- data.frame( 'Cured_Zombies' = cured_zombies )

ggplot( post_df, aes( x = Cured_Zombies ) ) +
  geom_histogram( binwidth = 1, fill = 'palegreen4', color = "#e9ecef" ) +
  ggtitle( 'Cured Zombies Probability Distribution' )
```

However, it is more often the case that we have data, and do not need a generative model to create it. However, we do not know the parameters that drive the process. Bayesian Inference is used to invert the probability to work backwards and learn about the underlying process given the data at hand.

**New Situations**: instead of the zombie example, let's look at clickthrough rates....  
How many visitors to a site?  
To get more visitors to your website you are considering paying for an ad to be shown 100 times on a popular social media site. According to the social media site, their ads get clicked on 10% of the time.
```{r}
# Fill in the parameters
n_samples <- 100000
n_ads_shown <- 100
proportion_clicks <- 0.1
n_visitors <- rbinom(n_samples, size = n_ads_shown, 
                     prob = proportion_clicks)

post_df <- data.frame( 'Site_Visits' = n_visitors )

ggplot( post_df, aes( x = Site_Visits ) ) +
  geom_histogram( binwidth = 1, fill = 'cadetblue', color = "#e9ecef" ) +
  ggtitle( 'Number of Visitors Distribution' )
```


### Prepresenting Uncertainty with Priors

And now to address the Priors.
**Prior Probability Distribution**: represents how uncertain the model is about the parameters before seeing any data. Ideally, this uncertainty should reflect our uncertainty. The reason it is called a **prior** is because it represents the uncertainty *before* having included information about the data.  

You're not so sure that your ad will get clicked on exactly 10% of the time. Instead of assigning proportion_clicks a single value you are now going to assign it a large number of values drawn from a probability distribution.

```{r}
n_samples <- 100000
n_ads_shown <- 100
# model a uniform probability distribution that ranges from 0 to 0.2 to represent the uncertainty of our value for this prior
proportion_clicks <- runif( n = n_samples, min = 0.0, max = 0.2 )
n_visitors <- rbinom( n_samples, n_ads_shown, proportion_clicks )
```

Visualize the probability distribution for the prior, proportion_clicks.
```{r}
pclick_df <- data.frame( 'probability_click' = proportion_clicks )

pclick_plot <- ggplot( pclick_df, aes( x = probability_click ) ) +
  geom_histogram( fill = 'cadetblue', color = "#e9ecef" ) +
  ggtitle( 'Probability Distribution of the Prior' )

nvis_df <- data.frame( 'Site_Visits' = n_visitors )

nvis_plot <- ggplot( nvis_df, aes( x = Site_Visits ) ) +
  geom_histogram( binwidth = 1, fill = 'cadetblue', color = "#e9ecef" ) +
  ggtitle( 'Number of Visitors Distribution' )

grid.arrange( pclick_plot, nvis_plot, ncol = 2 )
```
The envelope of the n-visitors data from this generative model is much different. The added uncertainty for the prior increases the uncertainty in the number of visitors the site will recieve.


### Bayesian Models and Conditioning
What if we send our add out in to the wilds of the market place and get some data on it's performance. Let's say 13 out of 100 users who were shown the ad followed through and clicked to visit the website. With this data, we can now apply some Bayesian Inference....  

```{r}
#this dataframe represents the joint probability distribution over proportion_clicks and n_visitors together.
prior_df <- data.frame( 'proportion_clicks' = proportion_clicks, 'n_visitors' = n_visitors )

#visualize as a scatterplot....
prior_plot <- ggplot( prior_df, aes( x = n_visitors, y = proportion_clicks ) ) +
  geom_point( alpha = 1/10 ) +
  #geom_jitter(alpha = 1/10 ) +
  theme( legend.position = 'none' )
prior_plot <- ggMarginal( prior_plot, type = 'histogram', 
                          xparams = list(  bins=20 ),
                          yparams = list(  bins=20))
prior_plot
```
The histograms show the marginal distributions. These are the same probability distributions that were described and visualized in the previous exercises.  

There is a clear trend in the data: the higher the underlying proportion of clicks, the more visitors there will be to the website

From this joint probability data, we can come up wih probability distributions for specific conditional situations. For example, If we know a conditional value for `proportion_clicks` = 0.10, then we can describe the distribution of `n_visitors`

```{r}
prior10_df <- prior_df %>%
  mutate( p10 = round( proportion_clicks, 2) == 0.1 ) %>%
  filter( p10 == TRUE ) %>%
  select( c( n_visitors, proportion_clicks ) )

prior_plot <- ggplot( prior_df, aes( x = n_visitors, y = proportion_clicks ) ) +
  geom_point( alpha = 1/10 ) +
  geom_point(data = prior10_df, aes(x = n_visitors, y = proportion_clicks, color = 'red' ) ) +
  theme( legend.position = 'none' )
prior_plot <- ggMarginal( prior_plot, type = 'histogram', 
                          xparams = list(  bins=20 ),
                          yparams = list(  bins=20))
prior_plot
```

And now to plot this distribution:
```{r}
ggplot( prior10_df, aes( x = n_visitors ) ) +
  geom_histogram( binwidth = 1, fill = 'red', color = "#e9ecef" ) +
  ggtitle( 'Probability Distribution of n_visitors given proportion_clicks = 0.1' )
```
If we shift the `proportion_clicks`, the distribution of `n_visitors` shifts accordingly.
Additionally, we can condition the `proportion_clicks` on a given `n_visitors`  

This demonstrations brings us to **the essence of Bayesian Inference**:  
Bayesian inference is conditioning on data, in order to learn about parameter values

Let's condition on the data we collected were we got 13 visits from 100 presentations of the ad. Find the **posterior** distribution for the proportion of clicks given there were 13 visitors. It is called a posterior, because it represents the uncertainty *after* having included information from the observed data.
```{r}
head( prior_df )
#condition the joint distribution with the observation that there were 13 visitors
# Create the posterior data frame
posterior <- prior_df[prior_df$n_visitors == 13, ]

# Visualize posterior proportion clicks
hist( posterior$proportion_clicks )
```
Now we can use this new estimate of the prior distribution to compute `n_visits` if we rerun the ad campaign:

```{r}
# Assign posterior to a new variable called prior
prior <- posterior

# Take a look at the first rows in prior
head(prior)

# Replace prior$n_visitors with a new sample and visualize the result
n_samples <-  nrow(prior)
n_ads_shown <- 100
prior$n_visitors <- rbinom(n_samples, size = n_ads_shown,
                           prob = prior$proportion_clicks)
hist(prior$n_visitors)

# Calculate the probability that you will get 5 or more visitors
sum(prior$n_visitors >= 5) / length(prior$n_visitors)
```
From the observation of 13/100 visits, we find that it is very probable (99% probable) that future add campaigns will generate at least 5 visits.


## Why use Bayesian Data Analysis?

### Four Good Things with Bayes

Bayes is very Flexible!  

1. Can include information sources in addition to the data
  + Background Information
  + Expert Opinion
  + Common Knowledge
  + ex: suppose you ask a vendor what the median and range of success has been for previous ad campaigns.
2. Can make many comparisons between groups or data sets
  + Can test different experimental conditions
  + Can easily find the probable difference between treatment groups
  + ex: suppose you have two different treatment groups
3. Can use the result of Bayesian Analysis to do Decision Analysis
  + **Decision Analysis**: take the result of a statistical analysis and post-process it to apply to a process of interest.
  + the posterior distributions are not of principal interest, rather an outcome to meet a goal (e.g. highest return/clickthrough)
4. Can change the underlying statistical model
  + if new data indicates the need for a change e.g. from a binomial model to include a new variable(s)
5. Bayes is optimal in the smol world that is the model
6. In Bayesian data analysis there is a separation between model and computation

**Example**: suppose you ask a vendor what the median and range of success has been for previous ad campaigns. The vendor responds that most ads get 5% clicks with a range of 2-8%. This information can be incorporated by updating the **priors** of a bayesian statistical model.  

we will use the `beta` distibution with `rbeta()` as a generative model for our informed pior
```{r}
# Explore using the rbeta function
beta_sample <- rbeta(n = 1000000, shape1 = 1, shape2 = 1)

# Visualize the results
hist( beta_sample )
```
The above distibution is an **uninformative prior** because it gives an equally likely probability for all values from 0 to 1.  
`rbeta()` takes two shape parameters that must be positive. The larger the `shape1` parameter, the closer the mean of the distribution is the 1. The larger the `shape2` parameter, the closer the mean of the distribution is to 0.

```{r}
# Modify the parameters
beta_sample1 <- data.frame( 'dist' = rbeta(n = 1000000, shape1 = 100, shape2 = 20) )
# Visualize the results
close21 <- ggplot( beta_sample1, aes( x = dist ) ) +
  geom_histogram( fill = 'cadetblue', color = "#e9ecef" ) +
  ggtitle( 'rbeta(nshape1 = 100, shape2 = 20)' )
# Modify the parameters
beta_sample2 <- data.frame( 'dist' = rbeta(n = 1000000, shape1 = 20, shape2 = 100) )
# Visualize the results
close20 <- ggplot( beta_sample2, aes( x = dist ) ) +
  geom_histogram( fill = 'cadetblue', color = "#e9ecef" ) +
  ggtitle( 'rbeta(nshape1 = 20, shape2 = 100)' )

grid.arrange( close21, close20, ncol = 2 )
```

There are many values that could be used to construct a reasonable prior distribution based on the information given to us, but here is a good suggestion:
```{r}
# Explore using the rbeta function
beta_sample <- rbeta(n = 1000000, shape1 = 5, shape2 = 95)

# Visualize the results
hist( beta_sample )
```

Now for some excitement!: Let's update the model developed in the previous chapter with our new, informed prior distribution for `proportion_clicks`

**Old Prior**:
```{r}
n_draws <- 100000
n_ads_shown <- 100

# Change the prior on proportion_clicks
proportion_clicks <- 
  runif(n_draws, min = 0.0, max = 0.2)
n_visitors <- 
  rbinom(n_draws, size = n_ads_shown, 
         prob = proportion_clicks)
prior <- 
  data.frame(proportion_clicks, n_visitors)
posterior <- 
  prior[prior$n_visitors == 13, ]

# This plots the prior and the posterior in the same plot
par(mfcol = c(2, 1))
hist(prior$proportion_clicks, 
     xlim = c(0, 0.25))
hist(posterior$proportion_clicks, 
     xlim = c(0, 0.25))
```

**New Prior**
```{r}
n_draws <- 100000
n_ads_shown <- 100

# Change the prior on proportion_clicks
proportion_clicks <- 
  rbeta(n_draws, shape1 = 5, shape2 = 95)
n_visitors <- 
  rbinom(n_draws, size = n_ads_shown, 
         prob = proportion_clicks)
prior <- 
  data.frame(proportion_clicks, n_visitors)
posterior <- 
  prior[prior$n_visitors == 13, ]

# This plots the prior and the posterior in the same plot
par(mfcol = c(2, 1))
hist(prior$proportion_clicks, 
     xlim = c(0, 0.25))
hist(posterior$proportion_clicks, 
     xlim = c(0, 0.25))
```
The `proportion_clicks` prior is Dead! Long live the `proportion_clicks` prior!  
Note the change in the prior distribution, but also the shift towards the left of the posterior. Indeed, the information given in the informaed prior tames the more enthusiastic response in the small data sample we have recieved with 13% clicks. The posterior distribution is shaped by both the observed data and the prior information. Which posterior should we use? The naive model favors the data we have collected. Whereas the informed posterior incorporates the outside information we have gathered. If the information is correct, then the informed posterior should lead to beter estimates of future outcomes.


### Contrasts and Comparisons
Comparing Video and Text ads  

```{r}
posterior <- data.frame( 'video_prop' = rbeta(n = 1000000, shape1 = 13, shape2 = 87 ),
                         'text_prop' = rbeta(n = 1000000, shape1 = 6, shape2 = 94 ) ) %>%
  mutate( 'prop_diff' = video_prop - text_prop )
head( posterior )
```
The new feature, `prop_diff` describes the probability difference between the two conditions

visualize:
```{r}
posterior_long <- posterior %>%
  pivot_longer( cols = everything(), names_to = 'dist', values_to = 'val' )

ggplot(posterior_long , 
       aes(x=val, fill=dist)) + geom_histogram(alpha=0.4, position="identity")
```
```{r}
# Define parameters
n_draws <- 100000
n_ads_shown <- 100
proportion_clicks <- runif(n_draws, min = 0.0, max = 0.2)
n_visitors <- rbinom(n = n_draws, size = n_ads_shown, 
                     prob = proportion_clicks)
prior <- data.frame(proportion_clicks, n_visitors)

# Create the posteriors for video and text ads
posterior_video <- prior[prior$n_visitors == 13, ]
posterior_text <- prior[prior$n_visitors == 6, ]

# Visualize the posteriors
hist(posterior_video$proportion_clicks, xlim = c(0, 0.25))
hist(posterior_text$proportion_clicks, xlim = c(0, 0.25))
```
```{r}
posterior <- data.frame(
    video_prop = posterior_video$proportion_clicks[1:4000],
    text_prop  = posterior_text$proportion_click[1:4000])
    
# Calculate the posterior difference: video_prop - text_prop
posterior$prop_diff = posterior$video_prop - posterior$text_prop 

# Visualize prop_diff
hist(posterior$prop_diff, xlim = c(0, 0.25))
```

```{r}
# Calculate the median of prop_diff
median(posterior$prop_diff)

# Calculate the proportion
# That is, calculate the proportion of samples in posterior$prop_diff that are more than zero.
sum( posterior$prop_diff > 0 ) / length( posterior$prop_diff )
```

### Decision Analysis

Bayesian data analysis makes it pretty easy to compare and contrast parameter estimates.

**Decision Analysis**: using the posteriors to apply to a process. e.g.: now that we know the posteriors, which will be more lucrative video or text add campaign?
```{r}
video_cost <- 0.25 #per video ad
text_cost <- 0.05 #per text ad
visitor_spend <- 2.53 #return for each visitor to site

posterior$video_profit <- posterior$video_prop * visitor_spend - video_cost
posterior$text_profit <- posterior$text_prop * visitor_spend - text_cost
posterior$profit_diff <- posterior$video_profit - posterior$text_profit

glimpse( posterior )
```
Which type of add will give the most money?

```{r}
posterior_long <- posterior %>%
  select( c( video_profit, text_profit ) ) %>%
  pivot_longer( cols = everything(), names_to = 'dist', values_to = 'val' )

ggplot(posterior_long , 
       aes(x=val, fill=dist)) + geom_histogram(alpha=0.4, position="identity")
```
```{r}
ggplot( posterior, aes( x = profit_diff ) ) +
  geom_histogram() +
  geom_vline( xintercept = 0, color = 'red' ) +
  geom_vline( xintercept = median( posterior$profit_diff ),
              color = 'blue' ) +
  annotate( geom = 'text', 
            x = median( posterior$profit_diff ) - 0.01,
            y = 150, label = 'Median Value', 
            color = 'blue', angle = 90 )
```
```{r}
# Calculate a "best guess" for the difference in profits
median(posterior$profit_diff)

# Calculate the probability that text ads are better than video ads
sum( posterior$profit_diff < 0 ) / length( posterior$profit_diff )
```
In conclusion, even though text ads get a lower proportion of clicks, they are also much cheaper. And, as you have calculated, there is a 63% probability that text ads are better.


### Change Anything and Everything

Let us say that we can also show a banner ad on a website. We have tested this out and received 19 clicks for a 24 hour period. What is a good generative model to use here?

**Poisson Distribution**:  

* takes 1 parameter: the mean number of events per time unit
* `rpois` samples the Poisson distibution
```{r}
#let's say the mean clicks is 20/day
n_clicks <- rpois( n = 100000, lambda = 20 )
hist( n_clicks )
```

Explore the Poisson Distribution more before we move on with Bayes
```{r}
# Simulate from a Poisson distribution and visualize the result
x  <- rpois(n = 10000, lambda = 3)
hist( x )

#Let's say that you run an ice cream stand and on cloudy days you on average sell 11.5 ice creams. It's a cloudy day.
x  <- rpois(n = 10000, lambda = 11.5)
hist( x )

#It's still a cloudy day, and unfortunately, you won't break even unless you sell 15 or more ice creams.
# Calculate the probability of break-even
sum( x >= 15 )/length( x )
```

Let's change the generative model to a poisson mod:
```{r}
# Change the model according to instructions
n_draws <- 100000
mean_clicks <- runif(n_draws, min = 0, max = 80)
n_visitors <- rpois(n = n_draws, mean_clicks)

prior <- data.frame(mean_clicks, n_visitors)
posterior <- prior[prior$n_visitors == 19, ]

# Visualize mean_clicks
hist( prior$mean_clicks )
hist( posterior$mean_clicks)
```

### Bayes is Optimal, kind of....
Bayes is optimal in the small world of the model.


## Bayesian Inference with Bayes' Theorem

### Probability Rules
fitting Bayesian models more efficiently

Probability Theory  

* Probability
  + a number between 0 and 1
  + A statement of certainty/uncertainty
  
* Mathematical Notation
  + P(n = 1) is a probability
  + P(n) is a probability distribution
  + P(n = 1|m = 1) is a conditional probability
  + P(n|m = 1) is a conditional probability distribution
  
* Manipulating (independent) Probabilities
  + The Sum Rule
    * p( 1 or 2 or 2 ) = 1/6 + 1/6 + 1/6 = 0.5
  + The Product Rule
    * p( 6 and 6 ) = 1/6 * 1/6 = 1/36 = 2.8%


### Calculating Likelihoods
we simulated using the `rbinom`, `rpois` etc. functions. now we can calculate using the `d` functions like `dbinom` and `pois`

```{r}
#calculate P(n_visitors = 13 | prob_success = 10%)
dbinom( 13, size = 100, prob = 0.1)

#calculate P(n_visitors = 13 or n_visitors = 14 | prob_success = 10 )
dbinom( 13, size = 100, prob = 0.1 ) + dbinom( 14, size = 100, prob = 0.1 )
```
Calculating probability distributions
```{r}
#calculate P( n_visitors | prob_success = 0.1 )
n_visitors <- seq( 0, 100, by = 1 )
probability <- dbinom( n_visitors, size = 100, prob = 0.1 )
plot( n_visitors, probability, type = 'h' )
```
Probability Density.. Continuous Distributions
```{r}
dunif( x = 0.12, min = 0, max = 0.2 )
```
```{r}
# Explore using dbinom to calculate probability distributions
n_ads_shown <- 100
proportion_clicks <- 0.1
n_visitors <- seq(0, 100)
prob <- dbinom(n_visitors, 
    size = n_ads_shown, prob = proportion_clicks)

# Plot the distribution
plot( n_visitors, prob, type = 'h')

# Change the code according to the instructions
n_ads_shown <- 100
proportion_clicks <- seq(0, 1, by = 0.01)
n_visitors <- 13
prob <- dbinom(n_visitors, 
    size = n_ads_shown, prob = proportion_clicks)

plot(proportion_clicks, prob, type = "h")
```

### Bayesian Calculation
Bayesian Inference by Calculation. Instead of simulating probabilities, we will directly calculate them in `R`.

```{r}
n_ads_shown <- 100
n_visitors <- seq( 0, 100, by = 1 )
proportion_clicks <- seq( 0, 1, 0.01 )
pars <-  expand.grid( proportion_clicks = proportion_clicks,
                   n_visitors = n_visitors ) 
pars$prior <- dunif( pars$proportion_clicks, min = 0, max = 0.2 )
pars$likelihood <- dbinom( pars$n_visitors, size = n_ads_shown,
                           prob = pars$proportion_clicks)
pars$probability <- pars$likelihood * pars$prior
pars$probability <- pars$probability / sum( pars$probability )

glimpse( pars )
```
```{r}
ggplot( pars, aes( x = n_visitors, y = proportion_clicks,
                   fill = probability ) ) +
  geom_tile() +
  xlim( c( 0, 35 ) ) +
  ylim( c( 0, 0.2 ) ) +
  scale_fill_gradient(low="white", high="darkblue")
```

```{r}
pars13 <- pars[ pars$n_visitors == 13, ]
pars13$probability <- pars13$probability / sum( pars$ probability )

ggplot( pars13, aes( x = n_visitors, y = proportion_clicks,
                   fill = probability ) ) +
  geom_tile() +
  xlim( c( 0, 35 ) ) +
  ylim( c( 0, 0.2 ) ) +
  scale_fill_gradient(low="white", high="darkblue")
```
```{r}
ggplot( pars13, aes( x = proportion_clicks, y = probability ) ) +
  geom_area() +
  xlim( c( 0, 0.3 ) )
```

A conditional shortcut: You can directly condition on the data, no need to first create the joint distribution.

```{r}
# Simplify the code below by directly conditioning on the data
n_ads_shown <- 100
proportion_clicks <- seq(0, 1, by = 0.01)
n_visitors <- 6
pars <- expand.grid(proportion_clicks = proportion_clicks,
                    n_visitors = n_visitors)
pars$prior <- dunif(pars$proportion_clicks, min = 0, max = 0.2)
pars$likelihood <- dbinom(pars$n_visitors, 
    size = n_ads_shown, prob = pars$proportion_clicks)
pars$probability <- pars$likelihood * pars$prior
pars$probability <- pars$probability / sum(pars$probability)
pars$probability <- pars$probability / sum(pars$probability)
plot(pars$proportion_clicks, pars$probability, type = "h")
```


### Bayes' Theorem
This is an implementation of Bayes' Theorem:
```{r}
pars$probability <- pars$likelihood * pars$prior
pars$probability <- pars$probability / sum( pars$probability )
```

Now to write this in proper notation:

$$P(\theta|D) = \frac{\mbox{P}(D|\theta) \cdot \mbox{P}(\theta)}{\sum \mbox{P}(D|\theta) \cdot \mbox{P}(\theta)}$$
where:  

* $\theta$ = paramters
* $D$ = the Data

The probability of the different parameter values given some data equals the likelihood of the relative probability of the data given the different parameter values multiplied by the prior probability of the different parameter values before seeing the data normalized by the total sum of the likelihood weighted by the prior.  

In the previous chapter, we used simulation to find the solution. In this chapter we calculated the probability using an algorithm called **grid approximation**. However, there are many more algorithms to fit Bayesian models, some being more efficient than others.  

tilda notation $\sim \longrightarrow \mbox{is distributed as }$ 

## More Parameters, More Data, and More Bayes...

### The Temperature in a Normal Lake
```{r}
temp <- c( 19, 23, 20, 17, 23 )
#f = 66, 73, 68, 63, 73
```

Use the **Normal** distribution as a generative model for values centered about a mean value.

```{r}
rnorm( n = 5, mean =  20, sd = 2 )
```
use `dnorm()` to look at how likely an outcome is given some fixed parameters
```{r}
like <- dnorm( x = temp, mean = 20, sd = 2 )
like
```
the results are the relative likelihoods of the data points.

```{r}
#log likelihood
log(like)
```

using `rnorm` & `dnorm`:
```{r}
# Assign mu and sigma
mu <- 3500
sigma <- 600

par(mfrow = c( 1,2 ) ) 
weight_distr <- rnorm(n = 100000, mean = mu, sd = sigma)
p1 <- hist(weight_distr, 60, xlim = c(0, 6000), col = "lightgreen")

# Create weight
weight <- seq(0, 6000, by = 100)

# Calculate likelihood
likelihood <- dnorm(weight, mu, sigma)

# Plot the distribution of weight
p2 <- plot( x = weight,
y = likelihood,
type = 'h' )
```



### A Bayesian Model of Water Temperature

so, we have out temps: temp = 19, 23, 20, 17, 23  
and we have out generative model: $\mbox{temp}_i \sim \mbox{Normal}(\mu,\sigma)$

Now we need to our prior distributions...
our informed guess about water temperatures is that we can expect the following standard deviation and mean:  
$\sigma \sim \mbox{Uniform}(\mbox{min: }0,\mbox{max: }10)$  
$\mu \sim \mbox{Normal}(\mbox{mean: }18,\mbox{sd: }5)$  

we could have used many other priors, but this is our best guess for now.

Let's fit our new normal model using the grid approximation method we used earlier

```{r}
temp <- c( 19, 23, 20, 17, 23 )
mu <- seq( 8, 30, by = 0.5 )
sigma <- seq( 0.1, 10, by = 0.3 )
pars <- expand.grid( mu = mu, sigma = sigma )

plot( pars, pch = 19, main = 'The Parameter Space' )
```

```{r}
#the priors
pars$mu_prior <- dnorm( pars$mu, mean = 18, sd = 5 )
pars$sigma_prior <- dunif( pars$sigma, min = 0, max = 10 )
pars$prior <- pars$mu_prior * pars$sigma_prior
for( i in 1:nrow( pars ) ) {
  likelihoods <- dnorm( temp, pars$mu[i], pars$sigma[i])
  pars$likelihood[i] <- prod( likelihoods )
}

#calculate the posterior probability
pars$probability <- pars$likelihood * pars$prior
pars$probability <- pars$probability / sum( pars$probability )

summary( pars )
```

now to visualize:
```{r}
ggplot( pars, aes( x = mu, y = sigma,
                   fill = probability ) ) +
  geom_tile() +
  xlim( c( 10, 30 ) ) +
  ylim( c( 0, 10 ) ) +
  scale_fill_gradient(low="white", high="darkblue")
```
From this result, we would expect the most probable temperature to be around 19-22 degrees C.  

Now to try with the zombie exercise

```{r}
# The IQ of a bunch of zombies
iq <- c(55, 44, 34, 18, 51, 40, 40, 49, 48, 46)
# Defining the parameter grid
pars <- expand.grid(mu = seq(0, 150, length.out = 100), 
                    sigma = seq(0.1, 50, length.out = 100))
# Defining and calculating the prior density for each parameter combination
pars$mu_prior <- dnorm(pars$mu, mean = 100, sd = 100)
pars$sigma_prior <- dunif(pars$sigma, min = 0.1, max = 50)
pars$prior <- pars$mu_prior * pars$sigma_prior
# Calculating the likelihood for each parameter combination
for(i in 1:nrow(pars)) {
  likelihoods <- dnorm(iq, pars$mu[i], pars$sigma[i])
  pars$likelihood[i] <- prod(likelihoods)
}
# Calculate the probability of each parameter combination
pars$probability <- pars$likelihood * pars$prior
pars$probability <- pars$probability / sum( pars$probability )

# Visualize
ggplot( pars, aes( x = mu, y = sigma,
                   fill = probability ) ) +
  geom_tile() +
  xlim( c( 30, 60 ) ) +
  ylim( c( 5, 20 ) ) +
  scale_fill_gradient(low="white", high="darkblue")
```
Generate the same visualization as in the course using the `levelplot` from the `lattice` library

```{r}
coul <- colorRampPalette(brewer.pal(8, "PuRd"))(25)
levelplot(probability ~ mu * sigma, data = pars, , col.regions = coul)
```
From this visualization we see that a credible interval of Zombie IQ is a mean of 42 +/- 10


### Answering the Question: Should I have a beach party?



### A Practical Tool: BEST



### What hae you learned? What did we miss?



<br><br><br>