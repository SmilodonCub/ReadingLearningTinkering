---
title: 'Linear Algebra for Data Science in `R`'
subtitle: 'DataCamp: Statistics with `R`'
author: 'Bonnie Cooper'
output:
  rmdformats::downcute
---

![](https://www.kdnuggets.com/wp-content/uploads/datacamp-logo.png){width=150%}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}

library( dplyr )
library( MASS )
library( ggplot2 )
```


## Introduction to Linear Algebra

### Motivations

linear operations on mathematical objects such as vectors, matrices and tensors.  

**Vectors**: storing univariate data.  
generating vectors in `R`
```{r}
#replicating values
rep( c( 1,2,3 ), 4 )
rep( c( 1,2,3 ), each = 4 )
```
```{r}
#generating sequences of values
seq( 2,8, by = 2 )
seq( 0, 55, by = 5 )
```
```{r}
#or just specify values
z <- c( 1,5,-2,4 )
z
```
```{r}
#assigning values of a vector
z[2] <- 7
z
```

**Matrices** - Storing Tables of Data.
can be thought of as a superposition of vectors.  
generating matrices in `R`:
```{r}
matrix( 5,3,4 )
```
```{r}
#can also specify values:
matrix( c( 1,-1,2,3,2,-2 ), nrow =2, ncol = 3, byrow = TRUE )
Z <- matrix( c( 1,-1,2,3,2,-2 ), nrow =2, ncol = 3, byrow = FALSE )
Z
```
can also update values with by assignment:
```{r}
Z[ 2,1 ] <- 100
Z
```

Algebra of Vectors:
```{r}
x <- seq( 1, 7, by = 1)
y <- seq( 2, 14, by = 2 )
z <- c( 1, 1, 2 )
# Add x to y and print
print(x + y)

# Multiply z by 2 and print
print(2 * z)

# Multiply x and y by each other and print
print(x * y)
```

Creating Matrices
```{r}
# Create a matrix of all 1's and all 2's that are 2 by 3 and 3 by 2, respectively
matrix(1, nrow = 2, ncol = 3)

print(matrix(2, nrow = 3, ncol = 2))

A <- matrix( 1, nrow = 2, ncol = 2 )
# Create a matrix B and changing the byrow designation.
B <- matrix(c(1, 2, 3, 2), nrow = 2, ncol = 2, byrow = FALSE)
B <- matrix(c(1, 2, 3, 2), nrow = 2, ncol = 2, byrow = TRUE)

# Add A to the previously-created matrix B
A + B
```


### Matrix-Vector Operations
an operation with the following dimensions (n,m)*(m,1) = (n,1) size vector.

consider the following matrix:
```{r}
A = matrix(c(1, 2, 3, -1, 0, 3), nrow = 2, ncol = 3, byrow = TRUE)
A
```
This is a 2x3 matrix. therefore, it can be multiplied by a 3x1 vector


### Matrix-Matrix Calculations
an operation with the following dimensions (n,m)*(m,o) = (n,o) size matrix.  
Matrix multiplication is not cummutative (order matters). Also, %*% $\neq$ *

```{r}
A <- matrix( c( 1,2,2,1 ), nrow = 2, ncol = 2, byrow = TRUE )
A
I <- diag( 2 )
I
I%*%A
A%*%I
```
The Identity matrix returns a mirror image of the input.

Some important matrix concepts:  
1. Square Matrices
2. The Matrix Inverse
3. Singular Matrices
4. Diagonal and triangular matrices

```{r}
A <- matrix( c( 1,2,-1,2 ), nrow = 2, ncol = 2, byrow = TRUE )

# Take the inverse of the 2 by 2 identity matrix
solve(diag( 2 ))

# Take the inverse of the matrix A
Ainv <- solve(A)

# Multiply A inverse by A
Ainv%*%A

# Multiply A by its inverse
A%*%Ainv
```


## Matrix-Vector Equations

### Motivation for Solving Matrix-Vector Equations
can one vector be described as a linear combination of other vectors?
$$A\vec{x} = \vec{b}$$
can the vector $\vec{b}$ be described as a linear combination of the columns of $A$?  

The Massy Matrix:

$$\left( \begin{array}{cc} 4 & -1 & -1 & -1 & -1 \\ -1 & 4 & -1 & -1 & -1 \\ -1 & -1 & 4 & -1 & -1 \\ -1 & -1 & -1 & 4 & -1 \\ -1 & -1 & -1 & -1 & 4 \end{array} \right)
\left( \begin{array}{cc} r_{JH} \\ r_{FM} \\ r_G \\ r_D \\ r_{McD} \end{array} \right) = \left( \begin{array}{cc} 103 \\ 28 \\ 15 \\ -40 \\ -106 
 \end{array} \right)$$

The rankings of the team are the coefficients that describe the linear combination of rows from the Massy matrix that produce the net point differential for the teams

Let's have a look using some real data:

```{r}
url <- 'https://assets.datacamp.com/production/repositories/2654/datasets/6bfadc8a2147bddbbaedafc8e21b8576cb4364ce/WNBA_Data_2017_M.csv'
WNBA_df <- read.csv( url )
WNBA_mmat <- data.matrix( WNBA_df )
url <- 'https://assets.datacamp.com/production/repositories/2654/datasets/4e20e9adfd6514bd5b1bfb1464cd6da9fbbadfe9/WNBA_Data_2017_f.csv'
WNBA_pd <- read.csv( url )
```

```{r}
# Print the Massey Matrix M
print( WNBA_mmat )

# Print the vector of point differentials f 
print( WNBA_pd )

# Find the sum of the first column of M
sum(WNBA_mmat[,1])

# Find the sum of the vector f
sum(WNBA_pd)
```

### Matrix-Vector Equations - Some Theory

What conditions are needed for Matrix-Vector equations to have a unique solution?  
I $A$ is an $n*n$ square matrix, then the following conditions are equivalent and imply a unique solution:  

* The matrix $A$ must have an inverse
* The determinant of $A$ is nonzero
* The rows and columns of $A$ form a basis for the set of all vectors with $n$ elements
* the homogeneous equation $A\vec{x}=\vec{0}$ has just the trivial ($\vec{x}=0$) solution

Does the matrix have an inverse:
```{r}
solve( A )
```

Is the determinant > 0?
```{r}
det( A )
```


Try it with the WNBA Massy Matrix:

```{r}
solve( WNBA_mmat )
```

what is the determinant?
```{r}
det( WNBA_mmat )
```

### Solving Matrix-Vector Equations
it's a whole lot simpler with `R`:
```{r}
A <- matrix( c( 1,-2,0,4 ), nrow = 2, ncol = 2, byrow = TRUE )
b <- matrix( c( 1,-2 ), nrow = 2, ncol = 1, byrow = TRUE )
x <- solve( A )%*%b
x
```
check the solution. does $A\vec{x}$ give you back $b$?
```{r}
res <- A%*%x
b == res
```
back to the 2017 WNBA Rating
```{r}
WNBA_pd$Differential[13] <- 0

#now to solve for the ratings vector
# Solve for r and rename column
ratings <- solve(WNBA_mmat)%*%WNBA_pd$Differential
colnames(ratings) <- "Rating"
ratings_df <- data.frame( ratings )

# Print r
head( ratings_df,5 )
```
Which team is the best?
```{r}
head( arrange(ratings_df, -Rating), 5 )
```

### Other Considerations for Matrix-Vector Equations
Options for non-square matrices:  

* Row Reduction
* Least Squares
* Single Value Decomposition
* Generalized / Pseudo-Inverse (when not invertible)

```{r}
#ginv() to find the generalized inverse
A <- matrix( c( 2, 3, -1, 4, 1, 7 ), nrow = 3, ncol = 2, byrow = TRUE )
ginv( A )
```
can think of as serving the same purpose as `solve()` for an mat w/no inverse.
```{r}
ginv( A )%*%A
```
that's almost the identity matrix....
```{r}
A%*%ginv( A )
```

Now to solve the WNBA ratings...
```{r}
# Find the rating vector the conventional way
ratings <- solve(WNBA_mmat)%*%WNBA_pd$Differential
colnames(ratings) <- "Rating"
ratings_df <- data.frame( ratings )

# Find the rating vector using ginv
r <- ginv(WNBA_mmat) %*% WNBA_pd$Differential
colnames(r) <- "Rating"
print(r)
```
## Eigenvalues and Eigenvectors


### Intro to Eigenvalues and Eigenvectors
Eigenvectors take collections of objects and subset a selection to approximate the data

Matrix Multiplications: rotations, reflections, dilations, contractions, projections & combinations. Eigenvalue/vector operations can represent complex data by decomposing to the sum of simpler operations


### Definition of Eigenvalues and Eigenvectors
For a matrix A, the scalar $\lambda$ is an eigenvalue of A with associated eigenvector $\hat{v} \neq \hat{0}$ if the following is true:
$$A\hat{v}= \lambda \hat{v}$$
In other words: the matrix multiplication $A\hat{v}$, a matrix-vector operation, produces the same vector as $\lambda \hat{v}$ a scalar-vector operation.

An example:
```{r}
A <- matrix( c( 2, 3, 0, 1 ), nrow = 2, ncol = 2, byrow = TRUE )
A
A %*% c(1,0) == 2*c(1,0)
```
We can say that 2 is an eigenvalue of $A$ with the associated eigenvector $\hat{v} = (1,0)^T$  
These two can be called an eigenpair of the matrix $A$  

Eigenvectors are all about direction and not magnitude. An eigenvector of $A$ is a vector that points in its own (or complete opposite) direction upon multiplication by $A$.

```{r}
A <- matrix( c( -1, 2, 4, 0, 7, 12, 0, 0, -4 ), nrow = 3, ncol = 3, byrow = TRUE )
# Show that 7 is an eigenvalue for A
A%*%c(0.2425356, 0.9701425, 0) - 7*c(0.2425356, 0.9701425, 0)

# Show that -4 is an eigenvalue for A
A%*%c(-0.3789810, -0.6821657, 0.6253186) - -4*c(-0.3789810, -0.6821657, 0.6253186)

# Show that -1 is an eigenvalue for A
A%*%c(1,0,0) - -1*c(1,0,0)

# Show that double an eigenvector is still an eigenvector
A%*%((2)*c(0.2425356, 0.9701425, 0)) - 7*(2)*c(0.2425356, 0.9701425, 0)

# Show half of an eigenvector is still an eigenvector
A%*%((0.5)*c(0.2425356, 0.9701425, 0)) - 7*(0.5)*c(0.2425356, 0.9701425, 0)
```

### Computing Eigenvalues and Eigenvectors in `R`
Solving Eigenvalue/Eigenvector Problems

Properties:  

* an n by n matrix has up to n eigenvalues
* even is A is a matrix of all real numbers, some or even all of its eigenvalues could be complex numbers
* all complex eigenvalues must come in conjugate pairs

Very simple to solve in `R`
```{r}
E <- eigen( A )
E
```

```{r}
A <- matrix( c( 1, 2, 1, 1 ), nrow = 2, ncol = 2, byrow = TRUE )

# Compute the eigenvalues of A and store in Lambda
Lambda <- eigen(A)

# Print eigenvalues
print(Lambda$values[1])
print(Lambda$values[2])

# Verify that these numbers satisfy the conditions of being an eigenvalue
det(Lambda$values[1]*diag(2) - A)
det(Lambda$values[2]*diag(2) - A)

# Print eigenvectors
print(Lambda$vectors[, 1])
print(Lambda$vectors[, 2])

# Verify that these eigenvectors & their associated eigenvalues satisfy Av - lambda v = 0
Lambda$values[1]*Lambda$vectors[, 1] - A%*%Lambda$vectors[, 1]
Lambda$values[2]*Lambda$vectors[, 2] - A%*%Lambda$vectors[, 2]
```

### Some More on Eigenvalues and Eigenvectors
If all the eigenvalues are distinct, then the eigenvectors form a basis for the set of n-dimensional vectors.

Eigenpairs turn matrix multiplication into a linear combination of scalar multiplications. can see this by applying the matrix A to $\vec{x}$ and using the fact that $A\vec{v}_j = \lambda_j \vec{v}_j$:
$$A\vec{x} = c_1\lambda_1 \vec{v}_1 + c_2\lambda_2 \vec{v}_2 + \cdots + c_n\lambda_n \vec{v}_n$$
Markov Model for Allele Frequency
```{r}
A <- matrix( c( 0.98, 0.005, 0.005, 0.01,
                0.005, 0.98, 0.01, 0.005,
                0.005, 0.01, 0.98, 0.005,
                0.01, 0.005, 0.005, 0.98), nrow = 4, ncol = 4, byrow = TRUE )
A
```
```{r}
# This code iterates mutation 1000 times
x <- c(1, 0, 0, 0)
for (j in 1:1000) {x <- A%*%x}

# Print x
print(x)

# Print and scale the first eigenvector of M
Lambda <- eigen(A)
v1 <- Lambda$vectors[, 1]/sum(Lambda$vectors[, 1])

# Print v1
print(v1)
```

## Principal Component Analysis

### Intro to the Idea of PCA

```{r}
url <- 'https://assets.datacamp.com/production/repositories/2654/datasets/760dae913f682ba6b2758207280138662ddedc0d/DataCampCombine.csv'
combine <- read.csv( url )
glimpse( combine )
```
```{r}
ggplot( combine, aes( x = shuttle, y = forty ) ) +
  geom_point( )
```

Removing redundant features from data. Highly correlated features adds redundancy which detracts from the explanatory value of the model.

**Principal Component Analysis**  

* One of the more useful methods from applied linear algebra
* Non-parametric way of extracting meaningful information from confusing data sets
* Uncovers hidden, low-dimensional structures that underlie your data
* These structures are more-easily visualized and are often interpretable to content experts

```{r}
ggplot( combine, aes( x = three_cone, y = forty ) ) +
  geom_point( )

# Find the correlation between variables forty and three_cone
cor(combine$forty, combine$three_cone)
```
```{r}
ggplot( combine, aes( x = vertical, y = broad_jump ) ) +
  geom_point( )

# Find the correlation between variables forty and three_cone
cor(combine$forty, combine$three_cone)
```
### The Linear Algebra Behind PCA

* the ith element of the diagonal of $\frac{A^TA}{n-1}$ is the variance of the ith column of the matrix
* the eigenvalues of $\frac{A^TA}{n-1}$ are real, and the corresponding eigenvectors are orthogonal or point in distinct directions
* the total variance of the data set is the sum of the eigenvalues of $\frac{A^TA}{n-1}$
* these eigenvectors are called the principal components of the data set in the matrix A
* the direction that an eigenvector points can explain eigenvalues of the total variance in the data set. if an eigenvalue or subset of eigenvalues explains a significant amount of the total variance, there is an opportunity for dimension reduction.

```{r}
# Extract columns 5-12 of combine
A <- combine[, 5:12]

# Make A into a matrix
A <- as.matrix(A)

#find the means of each column
meanA <- colMeans( A )

# Subtract the mean of each column
A <- sweep( A, 2, meanA )
head(A)
```

```{r}
# Create matrix B from equation in instructions
B <- t(A)%*%A/(nrow(A) - 1)

# Compare 1st element of the 1st column of B to the variance of the first column of A
B[1,1]
var(A[, 1])

# Compare 1st element of 2nd column of B to the 1st element of the 2nd row of B to the covariance between the first two columns of A
B[1, 2]
B[2, 1]
cov(A[, 1], A[, 2])
```
```{r}
# Find eigenvalues of B
V <- eigen( B )

# Print eigenvalues
V$values

# Print eigenvectors
V$vectors

# Roughly how much of the variability in the dataset can be explained by the first principal component?

V$values[1] / sum( V$values )
```
That's right. A great deal of the variability in the athletic profile of future NFL players can be attributed to one linear combination of the data!

### Performing PCA in `R`
```{r}
combine_pca <- combine %>% 
  dplyr::select( height:shuttle ) %>% 
  prcomp()

summary( combine_pca )
```
extract PC of interest. here we take the first 2 which account for 98.6% of the variance

```{r}
combine_pca_res <- cbind( combine[, 1:4 ], combine_pca$x[,1:2])
head( combine_pca_res )
```

```{r}
ggplot( combine_pca_res, aes( x = PC1, y = PC2, color = position ) ) +
  geom_point()
```

Subset the data for a certain position
```{r}
# Subset combine only to "WR"
combine_WR <- subset(combine, position == "WR")

# Scale columns 5-12 of combine_WR
B <- scale(combine_WR[, 5:12])

# Print the first 6 rows of the data
head( B )

# Summarize the principal component analysis
summary(prcomp( B ))
```




<br><br><br>