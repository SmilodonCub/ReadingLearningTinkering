---
title: 'Inference for Categorical Data in `R`'
subtitle: 'DataCamp: Statistics with `R`'
author: 'Bonnie Cooper'
output:
  rmdformats::downcute
---

![](https://www.kdnuggets.com/wp-content/uploads/datacamp-logo.png){width=150%}
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, message=FALSE}

library( dplyr )
library( ggplot2 )
library( gridExtra )
library( infer )
library( tidyr )
library( broom )
library( gapminder )
library( tidyverse )
library( benford.analysis )
```


## Inference for a Single Parameter


### The General Social Survey
```{r}
load( file = '/home/bonzilla/Documents/ReadingLearningTinkering/DataCamp/Statistics_with_R/gss.RData' )
glimpse( gss )
```
Let's visualize the distribution for the feature `happy`:
```{r}
gss2016 <- filter( gss, year == 2016 )

p1 <- ggplot( gss2016, aes( x = happy ) ) +
  geom_bar()

gss2016 <- gss2016 %>%
  mutate( Happy = case_when( happy == "VERY HAPPY" ~ "HAPPY",
                            happy == "PRETTY HAPPY" ~ "HAPPY",
                            happy == "NOT TOO HAPPY" ~ "UNHAPPY",)) %>%
  drop_na( Happy )

p2 <- ggplot( gss2016, aes( x = Happy ) ) +
  geom_bar()

grid.arrange( p1, p2, ncol = 2 )
```
find the proportion of `PRETTY HAPPY` people:
```{r}
p_hats <- gss2016 %>%
  group_by( Happy ) %>%
  summarise( count = n() ) %>%
  mutate( prop = count / sum( count ) )
p_hats
```

### Bootstrap

95% Confidence interval. estimate by adding / subtracting $2\cdot SE$. Find the $SE$ by bootstrapping:  

* `specify()`
* `generate()`
* `calculate()`

```{r}
boot <- gss2016 %>%
  specify( response = Happy,
           success = "HAPPY" ) %>%
  generate( reps = 500,
            type = "bootstrap" ) %>%
  calculate( stat = "prop" )

bootplot <- ggplot( boot, aes( x = stat ) ) +
  geom_density()

bootplot
```

find the standard deviation 

```{r}
SE <- boot %>% summarize( sd( stat ) ) %>% pull()
SE
```
Now we can find our confidence interval by adding / subtracting 'SE' from 'p_hat':
```{r}
CI <- c( p_hats$prop[1] - 2*SE, p_hats$prop[1] + 2*SE)
CI
```
We can be 95% confident that the number of happy Americans is somewhere between 82.6% <->85.3%

```{r}
bootplot + 
  geom_vline( xintercept = CI[1], color = 'red' ) +
  geom_vline( xintercept = CI[2], color = 'red' )
```

how much confidence people had in the scientific community in 2016? Let's take a look at this survey question...
```{r}
# Plot distribution of consci
ggplot(gss2016, aes(x = consci )) +
  # Add a bar layer
  geom_bar()
```
```{r}
# Compute proportion of high conf
p_hat <- gss2016 %>%
  group_by( consci ) %>%
  summarise( count = n() ) %>%
  mutate( prop = count / sum( count ) )
p_hat
```

```{r}
boot1 <- gss2016 %>%
  mutate( ConSci = case_when( consci == "A GREAT DEAL" ~ "High",
                            consci == "ONLY SOME" ~ "Low",
                            consci == "HARDLY ANY" ~ "Low",)) %>%
  drop_na( ConSci ) %>%
  specify(response = ConSci, success = "High") %>%
  generate(reps = 1, type = "bootstrap") 


boot1 %>%
  dplyr::summarize(prop_high = mean( ConSci == "High" )) %>%
  pull()
```
bootstrap many times and construct CIs
```{r}
# Create bootstrap distribution for proportion with High conf
boot_dist <- gss2016 %>%
  mutate( ConSci = case_when( consci == "A GREAT DEAL" ~ "High",
                            consci == "ONLY SOME" ~ "Low",
                            consci == "HARDLY ANY" ~ "Low",)) %>%
  drop_na( ConSci ) %>%
  # Specify the response and success
  specify(response = ConSci, success = "High") %>%
  # Generate 500 bootstrap reps
  generate(reps = 500, type = "bootstrap") %>%
  # Calculate proportions
  calculate(stat = "prop")
#boot_dist

# Compute estimate of SE
SE <- boot_dist %>%
  summarize(se = sd( stat )) %>%
  pull()
#SE

# Plot bootstrap distribution of stat
ggplot( boot_dist, aes( x = stat )) +
  # Add density layer
  geom_density() +
  geom_vline( xintercept = mean( boot_dist$stat ) - 2*SE, color = 'red' ) +
  geom_vline( xintercept = mean( boot_dist$stat ) + 2*SE, color = 'red' )

```


### Interpreting a Confidence Interval
Interpretation: "We're 95% confident that the true proportion is somewhere in this interval." Or, if we drew up 100 sample estimates and their CIs, 95 would contain the true value of the proportion.  

The width of CIs are affected by 3 things:  

* `n` the number of samples. SE decreasing with increasing number of sample
* $\alpha$ the confidence level. Higher confidence == wider CI 
* `p` the value of the parameter. SEs are highest when estimating proportions close to 0.5.

**SE**: Standard Error: the less data you have to make an estimate, the more uncertainty there will be as to the value of that estimate


### The Approximation Shortcut
a shortcut that uses the Normal Distribution. 

is the sample sufficiently large?
```{r}
p_hat <- gss2016 %>%
  mutate( Happy = case_when( happy == "VERY HAPPY" ~ "HAPPY",
                            happy == "PRETTY HAPPY" ~ "HAPPY",
                            happy == "NOT TOO HAPPY" ~ "UNHAPPY",)) %>%
  drop_na( Happy ) %>%
  summarize( mean( Happy == "HAPPY" ) ) %>%
  pull()

n <- nrow( gss2016 )

#these 2 criteria both need to be greater than 10:
c( n * p_hat, n*(1-p_hat))
```
Shortcut SE:
```{r}
SE_approx <- sqrt( p_hat * ( 1 - p_hat ) / n )
SE_approx
```

Does the normal distribution describe our data?
```{r}
ggplot( boot, aes( x = stat ) ) +
          geom_density() +
  stat_function( fun = dnorm,
                 color = 'purple',
                 args = list( mean = p_hat,
                              sd = SE_approx ) )
```
The approximation methods describe the data reasonably when normality criteria are met.


## Proportions: Testing and Power

### Hypothesis Test for a Proportion

* Confidence Interval - Captures how much uncertainty there is in the estimate. is formed using the Standard Error.  
* Hypothesis Test - What estimates would you observe if the ground truth holds a particular value?  
  + Using the `infer` library, we can use the `hypothesize()` function
  + the sampling distribution gives us the uncertainty in the data when the $H_0$ is true
  
Demonstrate this with the survey data: Do half of Americans favor capital punishment?  

```{r}
gss2016 %>%
  drop_na( cappun ) %>%
  ggplot( aes( x = cappun ) ) +
  geom_bar()
```
```{r}
p_hat <- gss2016 %>%
  drop_na( cappun ) %>%
  dplyr::summarize( mean( cappun == 'FAVOR' ) ) %>%
  pull()
p_hat 
```
Is this data consistent with a world where only half of Americans favor the death penalty?  
look at a distribution of the null hypothesis $H_0$:
```{r}
null <- gss2016 %>%
  drop_na( cappun ) %>%
  specify( response = cappun, succes = 'FAVOR' ) %>%
  hypothesize( null = 'point', p = 0.5 ) %>%
  generate( reps = 500, type = 'simulate' ) %>%
  calculate( stat = 'prop' )

ggplot( null, aes( x = stat ) ) +
  geom_density() +
  geom_vline( xintercept = p_hat, color = 'red' )
```

The proportion is well above the null distribution. We can quantify this with the p-value which gives the proportion of $H_0$ values $\geq$ the estimated proportion from the data, $\hat{p}$
```{r}
null %>%
  summarise( mean( stat > p_hat ) ) %>%
  pull() * 2
```

Hypothesis test:

* Null hypothesis: theory about the state of the world
* Null distribution: distribution of test statistics assuming the nul hypothesis is true
* p-value: a measure of consistency between the null hypothesis and your observation
  + high p-val: $\mbox{p-value }\gt\alpha$ observation is consistent with the null
  + low p-val: $\mbox{p-value }\lt\alpha$  observation is not consistent with the null

Let's try the same analysis with another feature.
Do 75% of Americans believe in an afterlife?
```{r}
gss2016_al <- gss2016 %>%
  drop_na( postlife )
# Using `gss2016`, plot postlife
ggplot( gss2016_al, aes( x = postlife ) ) +
  # Add bar layer
  geom_bar()
```
calculate the population estimate:
```{r}
# Calculate and save proportion that believe
p_hat <- gss2016_al %>%
  dplyr::summarize(prop_yes = mean(  postlife == 'YES')) %>%
  pull()

# See the result
p_hat
```
Our sample $\hat{p}=$ `r p_hat` and we would like to test our result against the claim that 75% of Americans believe in an afterlife. This is interpreted as a point null hypothesis that the population proportion has the value 75%.  
Here we generate data under the null hypothesis:

1 example simulation for p = 0.75
```{r}
# From previous step
sim1 <- gss2016_al %>%
  specify(response = postlife, success = "YES") %>%
  hypothesize(null = "point", p = 0.75) %>%
  generate(reps = 1, type = "simulate")

# Using sim1, plot postlife
ggplot( sim1, aes( x = postlife )) +
  # Add bar layer
  geom_bar()
```
find the simulated proportion
```{r}
# Compute proportion that believe
sim1 %>%
  summarise(prop_yes = mean(postlife == "YES")) %>%
  pull()
```
Now that we've seen an example simulation, let's do this many many times so that we can build a distribution of expected values assuming the true proportion is 75% Yes:

```{r}
# Generate null distribution
null <- gss2016_al %>%
  specify(response = postlife, success = "YES") %>%
  hypothesize(null = "point", p = 0.75) %>%
  generate(reps = 1000, type = "simulate") %>%
  # Calculate proportions
  calculate(stat = 'prop')
```

Now to visualize the null distribution with our sample estimate, $\hat{p}$
```{r}
# Visualize null distribution
ggplot( null, aes( x = stat )) +
  # Add density layer
  geom_density() +
  # Add line at observed
  geom_vline(xintercept = p_hat, color = "red")
```
Calculate the p-value:
```{r}
null %>%
  summarize(
    # Compute the one-tailed p-value
    one_tailed_pval = mean(stat >= p_hat),
    # Compute the two-tailed p-value
    two_tailed_pval = 2 * one_tailed_pval
  ) %>%
  pull(two_tailed_pval)
```

The videos fail to reject the null hypothesis for both of the features used above. However, the videos also subset to use only 150 records from the `gss2016` dataframe. 
Here we'll subset to see if we get the same result:
```{r}
gss2016_al150 <- gss2016_al %>%
  sample_n( 150 )

# Calculate and save proportion that believe
p_hat <- gss2016_al %>%
  dplyr::summarize(prop_yes = mean(  postlife == 'YES')) %>%
  pull()

# Generate null distribution
null <- gss2016_al150 %>%
  specify(response = postlife, success = "YES") %>%
  hypothesize(null = "point", p = 0.75) %>%
  generate(reps = 1000, type = "simulate") %>%
  # Calculate proportions
  calculate(stat = 'prop')

# Visualize null distribution
ggplot( null, aes( x = stat )) +
  # Add density layer
  geom_density() +
  # Add line at observed
  geom_vline(xintercept = p_hat, color = "red")

null %>%
  summarize(
    # Compute the one-tailed p-value
    one_tailed_pval = mean(stat >= p_hat),
    # Compute the two-tailed p-value
    two_tailed_pval = 2 * one_tailed_pval
  ) %>%
  pull(two_tailed_pval)
```

that's similar to the result from the DataCamp videos. we could replicate the exact result by using the `id` values to subset the records from `gss2016`. I'm not sure why the DataCamp course uses less data than is available. maybe we'll find out as the course progresses....


### Intervals for Differences
A question in two variables. e.g.: Do men and women believe at different rates?

Let $p$ be the proportion that believe in life after death  

* $H_0 \mbox{ : } p_{female} - p{male} = 0$
* $H_0 \mbox{ : } p_{female} - p{male} \neq 0$

```{r}
p1 <- ggplot( gss2016, aes( x = sex, fill = postlife ) ) +
  geom_bar()
p2 <- ggplot( gss2016, aes( x = sex, fill = postlife ) ) +
  geom_bar( position = 'fill' )

grid.arrange( p1, p2, ncol = 2 )
```
men and women appear to have different proportions. but is this significant?

calculate the difference in proportions for the 2 genders:
```{r}
p_hats <- gss2016 %>%
  group_by( sex ) %>%
  summarise( mean( postlife == 'YES', na.rm = TRUE ) ) %>%
  pull()
d_hat <- diff( p_hats )
d_hat
```
Now to generate some $H_0$ data:  

* $H_0 \mbox{ : } p_{female} - p_{male} = 0$
* There is no association between belief in the afterlife and the sex of the subject
* The variable `postlife` is independent of the variable `sex`

Here we generate the $H_0$ distribution by permutation:
```{r}
null <- gss2016 %>% 
  drop_na( c( postlife, sex ) ) %>%
  specify( postlife ~ sex, success = 'YES' ) %>%
  hypothesize( null = 'independence' ) %>%
  generate( reps = 500, type = 'permute' ) %>%
  calculate( stat = 'diff in props', order = c('FEMALE','MALE' ) )
```

let's visualize the distribution
```{r}
ggplot( null, aes( x = stat ) ) +
  geom_density() +
  geom_vline( xintercept = d_hat, color = 'red' )
```
These data suggest that there is a statistically significant difference between the sexes in the belief of life after death. furthermore, by the sign of the difference value we can conclude that more females than males believe in life after death.  

Let's run through the same analysis with a different feature, `cappun`: Is there a difference between males and females when it comes to the proportion of those who support the death penalty for people convicted of murder.

```{r}
gss2016_cappun <- gss2016 %>%
  drop_na( cappun )
# Plot distribution of sex filled by cappun
ggplot(gss2016_cappun, aes(x = sex, fill = cappun)) +
  # Add bar layer
  geom_bar(position = "fill")
```
find the sample difference of the $\hat{p}$s
```{r}
# Compute two proportions
p_hats <- gss2016_cappun %>%
  # Group by sex
  group_by( sex ) %>%
  # Calculate proportion that FAVOR
  summarize(prop_favor = mean( cappun == 'FAVOR')) %>%
  pull()

# Compute difference in proportions
d_hat <- diff( p_hats )

# See the result
d_hat
```
**NOTE**: R will do things alphabetically unless it is told otherwise. Therefore, `diff()` takes the difference of females - males.

Great. Now to calculate some $H_0$ data for a distribution
```{r}
# Create null distribution
null <- gss2016_cappun %>%
  # Specify the response and explanatory as well as the success
  specify(cappun ~ sex, success = "FAVOR") %>%
  # Set up null hypothesis
  hypothesize(null = "independence") %>%
  # Generate 500 reps by permutation
  generate(reps = 500, type = "permute") %>%
  # Calculate the statistics
  calculate(stat = 'diff in props', order = c("FEMALE", "MALE"))

# Visualize null
ggplot( null, aes( x= stat)) +
  # Add density layer
  geom_density() +
  # Add red vertical line at obs stat
  geom_vline(xintercept = d_hat, color = "red")
```
calculate the p-val:
```{r}
# Compute two-tailed p-value
null %>%
  summarise(
    one_tailed_pval = mean(stat <= d_hat),
    two_tailed_pval = one_tailed_pval * 2
  ) %>%
  pull(two_tailed_pval)
```
The p-value is approximately 0. therefore, the observed difference is statistically significantly different from the null hypothesis. Therefore, we can reject $H_0$. **Note**: this is a different result that given in the course exercise, because the couse subsets the data to 150 records whereas the full dataset is used for the above calculations. again, I have no idea why DataCamp course does this.

To illustrate the difference between hypothesis testing and confidence intervals, lets take a look at how to bootstrap to find a CI for an extimate:
```{r}
# Create the bootstrap distribution
boot <- gss2016_cappun %>%
  # Specify the variables and success
  specify( cappun ~ sex, success = "FAVOR") %>%
  # Generate 500 bootstrap reps
  generate( reps = 500, type = 'bootstrap' ) %>%
  # Calculate statistics
  calculate(stat = "diff in props", order = c("FEMALE", "MALE"))

    
# Compute the standard error
SE <- boot %>%
  summarize(se = sd( stat )) %>%
  pull()
  
# Form the CI (lower, upper)
c(d_hat - 2*SE, d_hat + 2*SE)
```

The CI does not span zero. Therefore, zero is not a plausible value; this agrees with the previous hypothesis test.


### Statistical Errors
What is the probability that you will reject a true null hypothesis?  
What is the probability that you will fail to reject a false null hypothesis? 
can add a feature with a random fair cointoss to evaluate independence of features



## Comparing Many Parameters: Independence


### Contingency Tables
```{r}
gss2016 %>%
  select( partyid, natarms ) %>%
  glimpse()
```
```{r}
gss2016 %>%
  drop_na( partyid, natarms ) %>%
  ggplot( aes( x = partyid, fill = natarms ) ) +
  geom_bar( position = 'fill' ) +
  theme(axis.text.x=element_text(angle=45,hjust=1)) 
```
let's try to clean the data the way the course does. remap the `partid` values to just 4 values
```{r}
gss2016_parties <- gss2016 %>%
  select( partyid, natarms ) %>%
  drop_na( partyid, natarms ) %>%
  mutate( party = case_when( partyid == "INDEPENDENT" ~ "Ind",
                            partyid == "IND,NEAR DEM" ~ "Ind",
                            partyid == "IND,NEAR REP" ~ "Ind",
                            partyid == 'NOT STR DEMOCRAT' ~ 'Dem',
                            partyid == 'STRONG DEMOCRAT' ~ 'Dem',
                            partyid == 'NOT STR REPUBLICAN' ~ 'Rep',
                            partyid == 'STRONG REPUBLICAN' ~ 'Rep',
                            partyid == 'OTHER PARTY' ~ 'Oth' ),
          party = as.factor( party )) 
gss2016_parties %>%
  ggplot( aes( x = party, fill = natarms ) ) +
  geom_bar(  ) +
  theme(axis.text.x=element_text(angle=45,hjust=1)) 
```
Well, that's as close as I can come without details of how DataCamp processed the dataframe.  
The course uses a smaller subset of the data, this code uses the whole set

now to visualize as a contingency table:
```{r}
glimpse( gss2016_parties )
tab <- gss2016_parties %>%
  select( natarms, party ) %>%
  table()
tab
```
```{r}
colSums( tab )
```

We can see that the `Oth` category is much smaller than the others.  

Can shift back to a dataframe form for plotting with the `tidy()` function:
```{r}
tab %>%
  tidy() %>%
  uncount( n ) %>%
  head()
```

Now do a similar visualization to look at how people from different political parties think about spending for space exploration:
```{r}
# Subset data
gss_party <- gss2016 %>%
  select( partyid, natspac ) %>%
  drop_na( partyid, natspac ) %>%
  mutate( party = case_when( partyid == "INDEPENDENT" ~ "Ind",
                            partyid == "IND,NEAR DEM" ~ "Ind",
                            partyid == "IND,NEAR REP" ~ "Ind",
                            partyid == 'NOT STR DEMOCRAT' ~ 'Dem',
                            partyid == 'STRONG DEMOCRAT' ~ 'Dem',
                            partyid == 'NOT STR REPUBLICAN' ~ 'Rep',
                            partyid == 'STRONG REPUBLICAN' ~ 'Rep',
                            partyid == 'OTHER PARTY' ~ 'Oth' ),
          party = as.factor( party )) %>%
  # Filter out the "Oth"
  filter( party != 'Oth' )

# Visualize distribution 
partyspac <- gss_party %>%
  ggplot( aes( x = party, fill = natspac ) ) +
  # Add bar layer of counts
  geom_bar( position = 'fill') +
  geom_text(aes(label = ..count..), stat = "count", position = "fill")
partyspac
```
This plot shows the filled histogram along with the contingency table values.


### Chi-Squared Test Statistic

compare the two plots for the opinions on funding for military or space exploration by party affiliation:
```{r}
partyarms <- gss2016_parties %>%
  filter( party != 'Oth' ) %>%
  ggplot( aes( x = party, fill = natarms ) ) +
  geom_bar( position = 'fill' ) +
  theme(axis.text.x=element_text(angle=45,hjust=1)) +
  geom_text(aes(label = ..count..), stat = "count", position = "fill")

grid.arrange( partyarms, partyspac, ncol = 2 )
```
How different are these distributions from a null distribution which shows no difference between party affiliation and opinions on spending. We need the expected counts if the variables are independent of one another.  

The $\chi^2$ statistic summarizes the squared & normalized distances of the sample proportions from the expected value.

How to find the expected values for the null hypothesis?....permutation
```{r}
#an example permutation
perm_1 <- gss2016_parties %>%
  filter( party != 'Oth' ) %>%
  specify(natarms ~ party) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1, type = "permute")
  
# Visualize permuted data
ggplot( perm_1, aes( x = party, fill = natarms ) ) +
  # Add bar layer
  geom_bar( position = 'fill' ) +
  geom_text(aes(label = ..count..), stat = "count", position = "fill")
```

compute the $\chi^2$-statistic:
```{r}
chi_obs_arms <- gss2016_parties %>% chisq_stat( natarms ~ party )
gss_party %>% chisq_stat( natspac ~ party )
chi_obs_spac <- 1.32606
```

lets observe many more permutations:
```{r}
# Create null
null_spac <- gss_party %>%
  specify(natspac ~ party) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 500, type = "permute") %>%
  calculate(stat = "Chisq")

# Visualize null
ggplot( null_spac, aes( x = stat ) ) +
  # Add density layer
  geom_density() +
  # Add vertical line at obs stat
  geom_vline( xintercept = chi_obs_spac, color = 'red' ) +
  theme_classic()
```
```{r}
# Create null that natarms and party are indep
null_arms <- gss2016_parties %>%
  specify(natarms ~ party) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 500, type = "permute") %>%
  calculate(stat = "Chisq")
  
# Visualize null
ggplot(null_arms, aes(x = stat)) +
  # Add density layer
  geom_density() +
  # Add vertical red line at obs stat
  geom_vline(xintercept = chi_obs_arms, color = "red")
```
The `natarms` statistic deviates way further from the null distribution compared to the `natspac` statistic. It seems that spending on the military is a partisan issue, while spending on space exploration is not.


### Alternate Method: the Chi-Squared Distribution
an approximation method for $\chi^2$  

* statistics: $\hat{x}^2$
* shape is determined by the degrees of freedom
* $df = (nrows  -1) \cdot (ncols-1)$

visualize the permutation distribution w/the approximation:

```{r}
ggplot( null_spac, aes( x = stat ) ) +
  geom_density() +
  stat_function(
    fun = dchisq,
    args = list( df = 4 ),
    color = 'blue'
  ) +
  geom_vline( xintercept = chi_obs_spac, color = 'red' )
```
The two distributions are very close.  

calculate the p-value:
```{r}
1 - pchisq( chi_obs_spac, df = 4 )
```

When to use $\chi^2$:  

* when the counts is $\geq$ 5 for each cell of the contingency table
* degrees of freedom, $df \geq 2$


Does this data set provide evidence of an association between happiness and geography?
```{r}
# Visualize distribution of region and happy
ggplot( gss2016, aes( x = region, fill = happy ) ) +
  # Add bar layer of proportions
  geom_bar( position = 'fill' ) +
  theme(axis.text.x=element_text(angle=45,hjust=1))
  
# Calculate and save observed statistic
chi_obs <- gss2016 %>%
  chisq_stat( happy ~ region)

# See the result
chi_obs
```



### Intervals for the Chi-Squared Distribution
They don't exist. Don't do them.

## Comparing Many Parameters: Goodness of Fit

### Case Study: election fraud

Benford's lay: the first digit law  
In a free and fair election, the vote counts should follow Benford's Law.  
If the election was fraudulent, then the vote counts should not follow Benford's law.
```{r}
firstgap <- gapminder %>%
  filter( year == 2007 ) %>%
  select( country, pop ) %>%
  mutate( strpop = as.character( pop ),
          strpop = as.numeric( str_sub( strpop, 1, 1 ) ) )

head( firstgap )

ggplot( firstgap, aes( x = strpop ) ) +
  geom_bar() +
  ggtitle( 'Country populations' ) +
  xlab( 'first digit' ) +
  theme_classic()
```

Take a look at the voting data for an election in `iran`
```{r}
url <- 'https://assets.datacamp.com/production/repositories/1703/datasets/a777b2366f4e576da5d58fda42f8337332acd3ae/iran.csv'
iran <- read.csv( url )
glimpse( iran )
```
```{r}
# Compute and save candidate totals
totals <- iran %>%
  summarise(ahmadinejad = sum( ahmadinejad ),
            rezai = sum( rezai),
            karrubi = sum( karrubi ),
            mousavi = sum(mousavi))

# Gather data
gathered_totals <- totals %>%
  gather(key = 'candidate', value = 'votes') %>%
  arrange( desc( votes ) )

# Inspect gathered totals
gathered_totals

# Plot total votes for each candidate
ggplot( gathered_totals, aes( x = reorder( candidate, -votes ), y = votes )) +
  # Add col layer
  geom_col( stat = 'identity' ) +
  xlab( 'candidate' ) +
  theme_classic()
```

Now to break the votes down by province:
```{r}
# Construct province-level dataset
province_totals <- iran %>%
  # Group by province
  group_by( province ) %>%
  # Sum up votes for top two candidates
  summarise( ahmadinejad = sum( ahmadinejad ), mousavi = sum( mousavi ) ) 

# Filter for won provinces won by #2
province_totals %>%
  filter(mousavi > ahmadinejad)
```
a couple of provinces were won by Mousavi.


### Goodness of Fit

Let's take a look at the first digits for the total counts of votes by region:
```{r}
glimpse( iran )
# Create first_digit variable
iran <- iran %>%
  mutate( first_digit = as.character( total_votes_cast ),
          first_digit = as.factor( str_sub( first_digit, 1, 1 ) ) )
  
# Check if get_first worked
iran %>%
  select( total_votes_cast, first_digit )

# Construct bar plot
ggplot( iran, aes( x = first_digit )) +
  # Add bar layer
  geom_bar() +
  theme_classic()
```

```{r}
bfd_iran <- benford( iran$total_votes_cast)
plot( bfd_iran )
```
There are some deviations from benford's law (many of the data points are above the expected value given by the dashed red lines)

```{r}
bfd_iran
```
Let's used the $\chi^2$ test to evaluate the goodness of fit for benford's law to our data.
Here's an example of finding the $\chi^2$ to compare the uniformity of party affiliation:
```{r}
gss2016_parties_noOth <- gss2016_parties %>%
  filter( party != 'Oth' ) %>%
  droplevels()

gss2016_parties_noOth %>%
  ggplot( aes( x = party )) +
  geom_bar() +
  geom_hline( yintercept =  1350/3, color = 'goldenrod', size = 2 )
```
```{r}
tab <- gss2016_parties_noOth %>%
  select( party ) %>%
  table()
tab

p_uniform <- c( Dem = 1/3, Ind = 1/3, Rep = 1/3 )
chisq.test( tab, p = p_uniform )$stat
```
How to test this?...try simulating the null hypothesis
```{r}
sims <- gss2016_parties_noOth %>%
  specify( response = party ) %>%
  hypothesise( null = 'point', p = p_uniform ) %>%
  generate( reps = 1, type = 'simulate' )
glimpse( sims )

ggplot( sims, aes( x = party) ) +
  geom_bar()
```
This plot looks a lot closer to the null. How let's permute many more times to generate a null distribution:

```{r}
p_benford <- c( `1` = 0.30103000, `2` = 0.17609126, `3` = 0.12493874,
                `4` = 0.09691001, `5` = 0.07918125, `6` = 0.06694679,
                `7` = 0.05799195, `8` = 0.05115252, `9` = 0.04575749 )

# Compute observed stat
chi_obs_stat <- iran %>%
  chisq_stat(response = first_digit, p = p_benford)
#chi_obs_stat

# Form null distribution
null <- iran %>%
  # Specify the response
  specify( response = first_digit ) %>%
  # Set up the null hypothesis
  hypothesize( null = 'point', p = p_benford ) %>%
  # Generate 500 reps
  generate( reps = 500, type = 'simulate' ) %>%
  # Calculate statistics
  calculate( stat = 'Chisq' )
```

```{r}
# Compute degrees of freedom
degrees_of_freedom <- iran %>%
  # Pull out first_digit vector
  pull("first_digit") %>%
  nlevels() -1


# Plot both null dists
ggplot( null, aes( x = stat )) +
  # Add density layer
  geom_density() +
  # Add vertical line at obs stat
  geom_vline( xintercept = chi_obs_stat) +
  # Overlay chisq approx
  stat_function(fun = dchisq, args = list(df = degrees_of_freedom), color = "blue")
```
```{r}
# Permutation p-value
null %>% summarize(pval = mean( stat > chi_obs_stat ))
  
# Approximation p-value
1 - pchisq( chi_obs_stat, df = degrees_of_freedom )
```
The low p-value suggests that the results are inconsistent with benson's law.


### And now to US

Let's compare our analysis for the Iranian election with data from a US election

```{r}
url <- 'https://assets.datacamp.com/production/repositories/1703/datasets/3e73a6c4432671bff5e6f05d340ac1ee41f2ba76/iowa.csv'
iowa <- read.csv( url, colClasses = c( 'factor', 'factor', 'factor', 'factor', 'numeric' ) )
glimpse( iowa )
```

Let's look at the Iowa votes totals for R/D presidential/vpres candidates by county:
```{r}
# Get Iowa county vote totals
iowa_county <- iowa %>%
  # Filter for rep/dem
  filter(candidate %in% c("Donald Trump / Mike Pence", "Hillary Clinton / Tim Kaine")) %>%
  # Group by county
  group_by( county ) %>%
  # Compute total votes in each county
  summarise(dem_rep_votes = sum( votes ))
  
# See the result
iowa_county
```
Now add a column with the first digit for each vote total:
```{r}
# Create first_digit variable
iowa_county <- iowa_county %>%
  mutate( first_digit = as.character( dem_rep_votes ),
          first_digit = as.factor( str_sub( first_digit, 1, 1 ) ) )

# Using iowa_county, plot first_digit
ggplot( iowa_county, aes( x = first_digit ) ) +
  # Add bar layer
  geom_bar()
```
Now to evaluate the diviation of the iowa data from the benford prediction with a $\chi^2$ hypothesis test:

```{r}
# Compute observed stat
chi_obs_stat <- iowa_county %>%
  chisq_stat(response = first_digit, p = p_benford)

# Form null distribution
null <- iowa_county %>%
  # Specify response
  specify( response = first_digit ) %>%
  # Set up null
  hypothesize( null = 'point', p = p_benford ) %>%
  # Generate 500 reps
  generate( reps = 5000, type = 'simulate' ) %>%
  # Calculate statistics
  calculate( stat = 'Chisq' )

# Visualize null stat
ggplot( null, aes( x = stat ) ) +
  # Add density layer
  geom_density() +
  # Add vertical line at observed stat
  geom_vline( xintercept = chi_obs_stat )
```
calculate the p-value:
```{r}
# Permutation p-value
null %>% summarize(pval = mean( stat > chi_obs_stat ))

# Compute degrees of freedom
degrees_of_freedom <- iowa_county %>%
  # Pull out first_digit vector
  pull("first_digit") %>%
  nlevels() -1

# Approximation p-value
1 - pchisq( chi_obs_stat, df = degrees_of_freedom )
```
The low p-value indicates that if in fact there was election fraud in Iowa, there would be a very small probability of observing this data or more extreme. Because the observed statistic is far into the tails of the null distribution, this indicates that your data is inconsistent with the null hypothesis that Benford's Law applies to vote totals in Iowa.


### Electrion Fraud in Iran and Iowa: debrief
What went wrong?
maybe Benfords Law doesn't apply to vote totals?
probably didn't have enough data









<br><br><br>

