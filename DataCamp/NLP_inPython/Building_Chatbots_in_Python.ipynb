{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Building Chatbots in Python\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Conversational Software\n",
    "\n",
    "* Implementing smalltalk, ELIZA style\n",
    "* How to use regex and ML to extract meaning from free-form text\n",
    "* build chat bots to helpdo simple tasks such as query a database\n",
    "* **statefulness** - How to handle keeping track of the state of a conversation\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECHOBOT\n",
    "\n",
    "we'd like to implement a simple chatbot that just echos a user's input\n",
    "\n",
    "    def respond( message ):\n",
    "        return 'I can hear you! you said: {}'.format( message )\n",
    "    def send_message( message ):\n",
    "        # calls respond() tp get response\n",
    "\n",
    "it's weird and unnatural for immediate responses, so it might be better to add a delay\n",
    "\n",
    "    import time\n",
    "    time.sleep( 0.5 )\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : hello\n",
      "BOT : I can hear you! You said: hello\n"
     ]
    }
   ],
   "source": [
    "bot_template = \"BOT : {0}\"\n",
    "user_template = \"USER : {0}\"\n",
    "\n",
    "# Define a function that responds to a user's message: respond\n",
    "def respond(message):\n",
    "    # Concatenate the user's message to the end of a standard bot respone\n",
    "    bot_message = \"I can hear you! You said: \" + message\n",
    "    # Return the result\n",
    "    return bot_message\n",
    "\n",
    "# Define a function that sends a message to the bot: send_message\n",
    "def send_message( message ):\n",
    "    # Print user_template including the user_message\n",
    "    print(user_template.format(message))\n",
    "    # Get the bot's response to the message\n",
    "    response = respond( message )\n",
    "    # Print the bot template including the bot's response.\n",
    "    print(bot_template.format(response))\n",
    "\n",
    "# Send a message to the bot\n",
    "send_message(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Creating a Personality\n",
    "\n",
    "Why would you want to add some personnality?  \n",
    "\n",
    "* personality is one thing that makes a chatbot different from a command line app\n",
    "* personnality makes chatbots more accessible & engaging to hoomans\n",
    "* users will expect it!\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "smalltalk\n",
    "\n",
    "    responses = {\n",
    "        \"what's your name?\" : [\"my name is EchoBot\",\n",
    "                               \"they call me Echobot\",\n",
    "                               \"the name's Bot, Echo Bot\"],\n",
    "        \"what's the weather today?\" : \"it's {} today\".format( weather_today )\n",
    "    }\n",
    "    \n",
    "    weather_today = 'cloudy'\n",
    "    \n",
    "    import random\n",
    "    def respond( message ):\n",
    "        if message in response:\n",
    "            return random.choice( responses[ message ] )\n",
    "            \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is Greg'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define variables\n",
    "name = \"Greg\"\n",
    "weather = \"cloudy\"\n",
    "\n",
    "# Define a dictionary with the predefined responses\n",
    "responses = {\n",
    "  \"what's your name?\": \"my name is {0}\".format(name),\n",
    "  \"what's today's weather?\": \"the weather is {0}\".format(weather),\n",
    "  \"default\": \"default message\"\n",
    "}\n",
    "\n",
    "# Return the matching response if there is one, default otherwise\n",
    "def respond( message ):\n",
    "    # Check if the message is in the responses\n",
    "    if message in responses:\n",
    "        # Return the matching message\n",
    "        bot_message = responses[message]\n",
    "    else:\n",
    "        # Return the \"default\" message\n",
    "        bot_message = responses[\"default\"]\n",
    "    return bot_message\n",
    "\n",
    "respond( \"what's your name?\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : what's today's weather?\n",
      "BOT : the weather is cloudy\n",
      "None\n",
      "USER : what's your name?\n",
      "BOT : my name is Greg\n",
      "None\n",
      "USER : what's your favorite color?\n",
      "BOT : default message\n"
     ]
    }
   ],
   "source": [
    "print( send_message( \"what's today's weather?\" ) )\n",
    "print( send_message( \"what's your name?\" ) )\n",
    "send_message( \"what's your favorite color?\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "name = \"Gremlin\"\n",
    "weather = \"sunny\"\n",
    "\n",
    "# Define a dictionary containing a list of responses for each message\n",
    "responses = {\n",
    "  \"what's your name?\": [\n",
    "      \"my name is {0}\".format(name),\n",
    "      \"they call me {0}\".format(name),\n",
    "      \"I go by {0}\".format(name)\n",
    "   ],\n",
    "  \"what's today's weather?\": [\n",
    "      \"the weather is {0}\".format(weather),\n",
    "      \"it's {0} today\".format(weather)\n",
    "    ],\n",
    "  \"default\": [\"default message\"]\n",
    "}\n",
    "\n",
    "# Use random.choice() to choose a matching response\n",
    "def respond(message):\n",
    "    # Check if the message is in the responses\n",
    "    if message in responses:\n",
    "        # Return a random matching response\n",
    "        bot_message = random.choice(responses[message])\n",
    "    else:\n",
    "        # Return a random \"default\" response\n",
    "        bot_message = random.choice(responses[\"default\"])\n",
    "    return bot_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : what's your name?\n",
      "BOT : my name is Gremlin\n",
      "None\n",
      "USER : what's your name?\n",
      "BOT : I go by Gremlin\n",
      "None\n",
      "USER : what's your name?\n",
      "BOT : I go by Gremlin\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print( send_message(\"what's your name?\") )\n",
    "print( send_message(\"what's your name?\") )\n",
    "print( send_message(\"what's your name?\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : what's today's weather?\n",
      "BOT : you tell me!\n",
      "None\n",
      "USER : what's today's weather?\n",
      "BOT : I don't know :(\n",
      "None\n",
      "USER : I love building chatbots\n",
      "BOT : tell me more!\n",
      "None\n",
      "USER : I love building chatbots\n",
      "BOT : I find that extremely interesting\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define a dictionary containing a list of responses for each message\n",
    "responses = {\n",
    "  \"question\": [\n",
    "      \"I don't know :(\", 'you tell me!'\n",
    "   ],\n",
    "  \"statement\": [\n",
    "      'tell me more!',\n",
    " 'why do you think that?',\n",
    " 'how long have you felt this way?',\n",
    " 'I find that extremely interesting',\n",
    " 'can you back that up?',\n",
    " 'oh wow!',\n",
    " ':)'\n",
    "  ]\n",
    "}\n",
    "\n",
    "def respond(message):\n",
    "    # Check for a question mark\n",
    "    if message.endswith( '?' ):\n",
    "        # Return a random question\n",
    "        return random.choice(responses[\"question\"])\n",
    "    # Return a random statement\n",
    "    return random.choice(responses[\"statement\"])\n",
    "\n",
    "\n",
    "# Send messages ending in a question mark\n",
    "print( send_message(\"what's today's weather?\") )\n",
    "print( send_message(\"what's today's weather?\") )\n",
    "\n",
    "# Send messages which don't end with a question mark\n",
    "print( send_message(\"I love building chatbots\") )\n",
    "print( send_message(\"I love building chatbots\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Text Processing with Regex\n",
    "\n",
    "* match messages against known patterns\n",
    "* extract key phrases\n",
    "* transform sentences grammatically\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string matches\n"
     ]
    }
   ],
   "source": [
    "#finding pattern matches\n",
    "import re\n",
    "pattern = 'do you remember .*'\n",
    "message = 'do you remember when you ate strawberries in the garden?'\n",
    "match = re.search( pattern, message )\n",
    "\n",
    "if match:\n",
    "    print( 'string matches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if bots took over the world\n",
      "bots took over the world\n"
     ]
    }
   ],
   "source": [
    "#groups\n",
    "pattern = 'if (.*)'\n",
    "message = 'what would happen if bots took over the world'\n",
    "match = re.search( pattern, message )\n",
    "print( match.group(0) )\n",
    "print( match.group(1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you walk your dog'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grammatical transformation\n",
    "def swap_pronouns( phrase ):\n",
    "    if 'I' in phrase:\n",
    "        phrase = re.sub( 'I', 'you', phrase )\n",
    "    if 'my' in phrase:\n",
    "        phrase = re.sub( 'my', 'your', phrase )\n",
    "    return phrase\n",
    "    \n",
    "swap_pronouns( 'I walk my dog' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "putting the two processes together would mean something similar to the following pseudocode:\n",
    "\n",
    "    pattern = 'do you remember (.*)'\n",
    "    message = 'do you remember when you ate strawberries in the garden'\n",
    "    phrase = re.search( pattern, message ).group(1)\n",
    "    response = choose_response( pattern )\n",
    "    phrase = swap_pronouns( phrase )\n",
    "    response.format( phrase )\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('What about your last birthday', 'your last birthday')\n",
      "your last birthday\n",
      "when me went to florida\n",
      "i had your own castle\n",
      "\n",
      "Sending messages...\n",
      "USER : do you remember your last birthday\n",
      "BOT : Why haven't you been able to forget your last birthday\n",
      "USER : do you think humans should be worried about AI\n",
      "BOT : if humans should be worried about AI? Absolutely.\n",
      "USER : I want a robot friend\n",
      "BOT : What would it mean if you got a robot friend\n",
      "USER : what if you could be anything you wanted\n",
      "BOT : Do you really think it's likely that you could be anything you wanted\n"
     ]
    }
   ],
   "source": [
    "rules  = {\n",
    "   'do you think (.*)': ['if {0}? Absolutely.', 'No chance'\n",
    "   ],\n",
    "   'do you remember (.*)': ['Did you think I would forget {0}',\n",
    " \"Why haven't you been able to forget {0}\",\n",
    " 'What about {0}',\n",
    " 'Yes .. and?'\n",
    "  ],\n",
    "   'I want (.*)': ['What would it mean if you got {0}',\n",
    " 'Why do you want {0}',\n",
    " \"What's stopping you from getting {0}\"  \n",
    "  ],\n",
    "   'if (.*)': [\"Do you really think it's likely that {0}\",\n",
    " 'Do you wish that {0}',\n",
    " 'What do you think about {0}',\n",
    " 'Really--if {0}'    \n",
    "   ] \n",
    "}\n",
    "\n",
    "# Define match_rule()\n",
    "def match_rule(rules, message):\n",
    "    response, phrase = \"default\", None\n",
    "    \n",
    "    # Iterate over the rules dictionary\n",
    "    for pattern, responses in rules.items():\n",
    "        # Create a match object\n",
    "        match = re.search( pattern, message)\n",
    "        if match is not None:\n",
    "            # Choose a random response\n",
    "            response = random.choice( responses )\n",
    "            if '{0}' in response:\n",
    "                phrase = match.group(1)\n",
    "                #print( phrase)\n",
    "    # Return the response and phrase\n",
    "    return response.format(phrase), phrase\n",
    "\n",
    "# Define replace_pronouns()\n",
    "def replace_pronouns(message):\n",
    "\n",
    "    message = message.lower()\n",
    "    if 'me' in message:\n",
    "        # Replace 'me' with 'you'\n",
    "        return re.sub( 'me', 'you', message )\n",
    "    if 'my' in message:\n",
    "        # Replace 'my' with 'your'\n",
    "        return re.sub( 'my', 'your', message )\n",
    "    if 'your' in message:\n",
    "        # Replace 'your' with 'my'\n",
    "        return re.sub( 'your', 'my', message )\n",
    "    if 'you' in message:\n",
    "        # Replace 'you' with 'me'\n",
    "        return re.sub( 'you', 'me', message )\n",
    "\n",
    "    return message\n",
    "\n",
    "# Define respond()\n",
    "def respond(message):\n",
    "    # Call match_rule\n",
    "    response, phrase = match_rule( rules, message )\n",
    "    if '{0}' in response:\n",
    "        # Replace the pronouns in the phrase\n",
    "        phrase = replace_pronouns( phrase )\n",
    "        # Include the phrase in the response\n",
    "        response = response.format( phrase )\n",
    "    return response\n",
    "\n",
    "\n",
    "# Test match_rule\n",
    "print(match_rule(rules, \"do you remember your last birthday\"))\n",
    "# Test replace_pronouns()\n",
    "print(replace_pronouns(\"my last birthday\"))\n",
    "print(replace_pronouns(\"when you went to Florida\"))\n",
    "print(replace_pronouns(\"I had my own castle\"))\n",
    "\n",
    "print( '' )\n",
    "print( 'Sending messages...')\n",
    "send_message(\"do you remember your last birthday\")\n",
    "send_message(\"do you think humans should be worried about AI\")\n",
    "send_message(\"I want a robot friend\")\n",
    "send_message(\"what if you could be anything you wanted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Understanding Intents and Entities\n",
    "\n",
    "**NLU** - Natural Language Understanding is usually concerned with transforming natural text to a structured data format within a particulr domain\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: finite_state_machine Pages: 1 -->\n",
       "<svg width=\"576pt\" height=\"162pt\"\n",
       " viewBox=\"0.00 0.00 576.00 161.52\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(.5617 .5617) rotate(0) translate(4 283.529)\">\n",
       "<title>finite_state_machine</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-283.529 1021.3716,-283.529 1021.3716,4 -4,4\"/>\n",
       "<!-- I&#39;m looking for a \n",
       "Mexican restaurant \n",
       "in the center of town -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>I&#39;m looking for a \n",
       "Mexican restaurant \n",
       "in the center of town</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"122.0868\" cy=\"-139.7645\" rx=\"118.1709\" ry=\"118.1709\"/>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"122.0868\" cy=\"-139.7645\" rx=\"122.1737\" ry=\"122.1737\"/>\n",
       "<text text-anchor=\"middle\" x=\"122.0868\" y=\"-151.0645\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">I&#39;m looking for a </text>\n",
       "<text text-anchor=\"middle\" x=\"122.0868\" y=\"-136.0645\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Mexican restaurant </text>\n",
       "<text text-anchor=\"middle\" x=\"122.0868\" y=\"-121.0645\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">in the center of town</text>\n",
       "</g>\n",
       "<!-- NLU -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>NLU</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"313.3207\" cy=\"-139.7645\" rx=\"33.2948\" ry=\"33.2948\"/>\n",
       "<text text-anchor=\"middle\" x=\"313.3207\" y=\"-136.0645\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">NLU</text>\n",
       "</g>\n",
       "<!-- I&#39;m looking for a \n",
       "Mexican restaurant \n",
       "in the center of town&#45;&gt;NLU -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>I&#39;m looking for a \n",
       "Mexican restaurant \n",
       "in the center of town&#45;&gt;NLU</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M244.2809,-139.7645C253.3406,-139.7645 262.0546,-139.7645 270.0605,-139.7645\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"270.1469,-143.2646 280.1468,-139.7645 270.1468,-136.2646 270.1469,-143.2646\"/>\n",
       "</g>\n",
       "<!-- Sure! what about Pepe&#39;s\n",
       "burritos on Main St -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Sure! what about Pepe&#39;s\n",
       "burritos on Main St</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"877.6071\" cy=\"-139.7645\" rx=\"135.5361\" ry=\"135.5361\"/>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"877.6071\" cy=\"-139.7645\" rx=\"139.5294\" ry=\"139.5294\"/>\n",
       "<text text-anchor=\"middle\" x=\"877.6071\" y=\"-143.5645\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Sure! what about Pepe&#39;s</text>\n",
       "<text text-anchor=\"middle\" x=\"877.6071\" y=\"-128.5645\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">burritos on Main St</text>\n",
       "</g>\n",
       "<!-- intent &amp; entities -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>intent &amp; entities</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"470.21\" cy=\"-139.7645\" rx=\"87.9851\" ry=\"87.9851\"/>\n",
       "<text text-anchor=\"middle\" x=\"470.21\" y=\"-136.0645\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">intent &amp; entities</text>\n",
       "</g>\n",
       "<!-- NLU&#45;&gt;intent &amp; entities -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>NLU&#45;&gt;intent &amp; entities</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M346.5929,-139.7645C354.4535,-139.7645 363.2321,-139.7645 372.3864,-139.7645\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"372.4415,-143.2646 382.4414,-139.7645 372.4414,-136.2646 372.4415,-143.2646\"/>\n",
       "</g>\n",
       "<!-- Database -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Database</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"647.8974\" cy=\"-139.7645\" rx=\"53.8905\" ry=\"53.8905\"/>\n",
       "<text text-anchor=\"middle\" x=\"647.8974\" y=\"-136.0645\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Database</text>\n",
       "</g>\n",
       "<!-- intent &amp; entities&#45;&gt;Database -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>intent &amp; entities&#45;&gt;Database</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M558.0126,-139.7645C566.715,-139.7645 575.4307,-139.7645 583.8257,-139.7645\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"583.8843,-143.2646 593.8843,-139.7645 583.8842,-136.2646 583.8843,-143.2646\"/>\n",
       "</g>\n",
       "<!-- Database&#45;&gt;Sure! what about Pepe&#39;s\n",
       "burritos on Main St -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>Database&#45;&gt;Sure! what about Pepe&#39;s\n",
       "burritos on Main St</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M702.0445,-139.7645C710.0434,-139.7645 718.5936,-139.7645 727.4542,-139.7645\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"727.5772,-143.2646 737.5772,-139.7645 727.5771,-136.2646 727.5772,-143.2646\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7fd0f56c0b90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphviz\n",
    "\n",
    "f = graphviz.Digraph('finite_state_machine', filename='fsm.gv')\n",
    "f.attr(rankdir='LR', size='8,5')\n",
    "\n",
    "f.attr('node', shape='doublecircle')\n",
    "f.node(\"\"\"I'm looking for a \n",
    "Mexican restaurant \n",
    "in the center of town\"\"\")\n",
    "f.node(\"\"\"Sure! what about Pepe's\n",
    "burritos on Main St\"\"\")\n",
    "\n",
    "\n",
    "f.attr('node', shape='circle')\n",
    "f.edge(\"\"\"I'm looking for a \n",
    "Mexican restaurant \n",
    "in the center of town\"\"\", 'NLU')\n",
    "f.edge('NLU', \"\"\"intent & entities\"\"\")\n",
    "f.edge(\"\"\"intent & entities\"\"\", 'Database')\n",
    "f.edge('Database', \"\"\"Sure! what about Pepe's\n",
    "burritos on Main St\"\"\")\n",
    "\n",
    "\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**intent** - a broad description of what a person is trying to say  \n",
    "**entities** - where/ when/ what. specific to your domain  \n",
    "**Named Entity Recognition (NER)** - broader identification of subjects\n",
    "\n",
    "Using Regex to recognize intents  \n",
    "\n",
    "* simpler than machine learning approaches\n",
    "* highly computationally efficient\n",
    "* However: can get difficult to debug as things grow in complxity\n",
    "* make use of the pipe `|` operator (equivalent to OR)\n",
    "\n",
    "For example:\n",
    "\n",
    "    re.search( r\"\\b(hellow|hey|hi)\\b\", \"hey there!\" ) is not None\n",
    "    \n",
    "here the `\\b` matches to the beginning and end of a word\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mary', 'Oxford', 'Google']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#regex to recognize intents\n",
    "pattern = re.compile( '[A-Z]{1}[a-z]*' )\n",
    "message = \"Mary is a friend of mine, she studied at Oxford and now works at Google\"\n",
    "pattern.findall( message )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'greet': re.compile('hello|hi|hey'), 'goodbye': re.compile('bye|farewell'), 'thankyou': re.compile('thank|thx')}\n",
      "USER : hello!\n",
      "BOT : Hello you! :)\n",
      "USER : bye byeee\n",
      "BOT : goodbye for now\n",
      "USER : thanks very much!\n",
      "BOT : you are very welcome\n"
     ]
    }
   ],
   "source": [
    "keywords  = {\n",
    "   'greet': ['hello', 'hi', 'hey'\n",
    "   ],\n",
    "   'goodbye': ['bye', 'farewell'\n",
    "  ],\n",
    "   'thankyou': ['thank', 'thx'    \n",
    "   ] \n",
    "}\n",
    "\n",
    "responses  = {\n",
    "   'greet': 'Hello you! :)',\n",
    "   'goodbye': 'goodbye for now',\n",
    "   'thankyou': 'you are very welcome',\n",
    "    'default': 'default message'       \n",
    "}\n",
    "\n",
    "# Define a dictionary of patterns\n",
    "patterns = {}\n",
    "\n",
    "# Iterate over the keywords dictionary\n",
    "for intent, keys in keywords.items():\n",
    "    # Create regular expressions and compile them into pattern objects\n",
    "    patterns[intent] = re.compile( '|'.join(keys) )\n",
    "    \n",
    "# Print the patterns\n",
    "print(patterns)\n",
    "\n",
    "# Define a function to find the intent of a message\n",
    "def match_intent(message):\n",
    "    matched_intent = None\n",
    "    for intent, pattern in patterns.items():\n",
    "        #print( intent, pattern )\n",
    "        # Check if the pattern occurs in the message \n",
    "        if pattern.search( message ):\n",
    "            matched_intent = intent\n",
    "            #print( intent )\n",
    "    return matched_intent\n",
    "\n",
    "# Define a respond function\n",
    "def respond(message):\n",
    "    # Call the match_intent function\n",
    "    intent = match_intent( message )\n",
    "    # Fall back to the default response\n",
    "    key = \"default\"\n",
    "    if intent in responses:\n",
    "        key = intent\n",
    "    return responses[key]\n",
    "\n",
    "# Send messages\n",
    "send_message(\"hello!\")\n",
    "send_message(\"bye byeee\")\n",
    "send_message(\"thanks very much!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER : my name is David Copperfield\n",
      "BOT : Hello, David Copperfield!\n",
      "USER : call me Ishmael\n",
      "BOT : Hello, Ishmael!\n",
      "USER : People call me Cassandra\n",
      "BOT : Hello, People Cassandra!\n"
     ]
    }
   ],
   "source": [
    "# entity extractio using regex\n",
    "# Define find_name()\n",
    "def find_name(message):\n",
    "    name = None\n",
    "    # Create a pattern for checking if the keywords occur\n",
    "    name_keyword = re.compile( '(name|call)')\n",
    "    # Create a pattern for finding capitalized words\n",
    "    name_pattern = re.compile(  '[A-Z]{1}[a-z]*' )\n",
    "    if name_keyword.search(message):\n",
    "        # Get the matching words in the string\n",
    "        name_words = name_pattern.findall( message )\n",
    "        if len(name_words) > 0:\n",
    "            # Return the name if the keywords are present\n",
    "            name = ' '.join(name_words)\n",
    "    return name\n",
    "\n",
    "# Define respond()\n",
    "def respond(message):\n",
    "    # Find the name\n",
    "    name = find_name( message )\n",
    "    if name is None:\n",
    "        return \"Hi there!\"\n",
    "    else:\n",
    "        return \"Hello, {0}!\".format(name)\n",
    "\n",
    "# Send messages\n",
    "send_message(\"my name is David Copperfield\")\n",
    "send_message(\"call me Ishmael\")\n",
    "send_message(\"People call me Cassandra\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Word Vectors \n",
    "\n",
    "**machine learning** - programs which can get better at a task by being exposed to more data\n",
    "**word vectors** - try to represent the meaning of words. words which appear in a similar context will have similar vectors. word vectors trained on data with millions of words will carry a lot of implicit meaning  \n",
    "\n",
    "Training word vectors requires a lot of data. High quality word vectors are available for anyone to use. Here we will use the `GloVe` algoirthm and the `spaCy` NLP library.  \n",
    "\n",
    "**similarity** - the direction of the word vectors  \n",
    "**distance between words** - the angle between vectors  \n",
    "\n",
    "**Cosine similarity**  \n",
    "    * 1 if the vectors have the same direction\n",
    "    * 0 if the vectors are perpendicular\n",
    "    * -1 if the vectors have opposite direction\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load( \"en_core_web_sm\" )\n",
    "print( nlp.vocab.vectors_length ) #check length of GloVe vectors in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello : [ 0.07995048 -0.35524017  0.39444777  0.1166493  -0.07528512]\n",
      "can : [ 0.44284296 -0.23842171  0.00590326 -0.28871217 -0.35360825]\n",
      "you : [ 0.5555374   1.2453554   0.89171314  0.7010913  -0.6754221 ]\n",
      "help : [ 0.7803134   0.35145187 -0.5284932   0.53522944 -0.53859246]\n",
      "me : [ 0.45454198  1.0163618  -0.57533526 -0.25329933  0.38366047]\n",
      "? : [-0.4429566   0.14937042 -0.10314497  0.8081817  -0.8367223 ]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp( 'hello can you help me?' )\n",
    "for token in doc:\n",
    "    print(\"{} : {}\".format( token, token.vector[:5] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.046753254647874186\n",
      "0.5549909794776612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bonzilla/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/bonzilla/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "# similarity with spaCy\n",
    "doc = nlp( 'cat' )\n",
    "print( doc.similarity( nlp( 'can' ) ) )\n",
    "print( doc.similarity( nlp( 'dog' ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The similarity measure reflect how close the meaning of tokens are in the context of the traininng data, over how close the words are in spelling.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentences = [' i want to fly from boston at 838 am and arrive in denver at 1110 in the morning',\n",
    " ' what flights are available from pittsburgh to baltimore on thursday morning',\n",
    " ' what is the arrival time in san francisco for the 755 am flight leaving washington',\n",
    " ' cheapest airfare from tacoma to orlando',\n",
    " ' round trip fares from pittsburgh to philadelphia under 1000 dollars',\n",
    " ' i need a flight tomorrow from columbus to minneapolis',\n",
    " ' what kind of aircraft is used on a flight from cleveland to dallas',\n",
    " ' show me the flights from pittsburgh to los angeles on thursday',\n",
    " ' all flights from boston to washington',\n",
    " ' what kind of ground transportation is available in denver',\n",
    " ' show me the flights from dallas to san francisco',\n",
    " ' show me the flights from san diego to newark by way of houston',\n",
    " ' what is the cheapest flight from boston to bwi',\n",
    " ' all flights to baltimore after 6 pm',\n",
    " ' show me the first class fares from boston to denver',\n",
    " ' show me the ground transportation in denver',\n",
    " ' all flights from denver to pittsburgh leaving after 6 pm and before 7 pm',\n",
    " ' i need information on flights for tuesday leaving baltimore for dallas dallas to boston and boston to baltimore',\n",
    " ' please give me the flights from boston to pittsburgh on thursday of next week',\n",
    " ' i would like to fly from denver to pittsburgh on united airlines',\n",
    " ' show me the flights from san diego to newark',\n",
    " ' please list all first class flights on united from denver to baltimore',\n",
    " ' what kinds of planes are used by american airlines',\n",
    " \" i'd like to have some information on a ticket from denver to pittsburgh and atlanta\",\n",
    " \" i'd like to book a flight from atlanta to denver\",\n",
    " ' which airline serves denver pittsburgh and atlanta',\n",
    " \" show me all flights from boston to pittsburgh on wednesday of next week which leave boston after 2 o'clock pm\",\n",
    " ' atlanta ground transportation',\n",
    " ' i also need service from dallas to boston arriving by noon',\n",
    " ' show me the cheapest round trip fare from baltimore to dallas']\n",
    "\n",
    "# Calculate the length of sentences\n",
    "n_sentences = len( sentences )\n",
    "\n",
    "# Initialize the array with zeros: X\n",
    "X = np.zeros((n_sentences, 96))\n",
    "\n",
    "# Iterate over the sentences\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    # Pass each each sentence to the nlp object to create a document\n",
    "    doc = nlp( sentence)\n",
    "    # Save the document's .vector attribute to the corresponding row in X\n",
    "    X[idx, :] = doc.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Intents and Classification\n",
    "\n",
    "a **classifier** to predict the intent label for a given sentence  \n",
    "fit a classifier to **training data**  \n",
    "evaluate the performance of a classifier with **test data**  \n",
    "can use **accuracy** as a measure of model performance $\\rightarrow$ the fraction of correctly predicted labels\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i want to fly from boston at 838 am and arriv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atis_flight</td>\n",
       "      <td>what flights are available from pittsburgh to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atis_flight_time</td>\n",
       "      <td>what is the arrival time in san francisco for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>cheapest airfare from tacoma to orlando</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>round trip fares from pittsburgh to philadelp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             labels                                          sentences\n",
       "0       atis_flight   i want to fly from boston at 838 am and arriv...\n",
       "1       atis_flight   what flights are available from pittsburgh to...\n",
       "2  atis_flight_time   what is the arrival time in san francisco for...\n",
       "3      atis_airfare            cheapest airfare from tacoma to orlando\n",
       "4      atis_airfare   round trip fares from pittsburgh to philadelp..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# the ATIS dataseet\n",
    "atis_df = pd.read_csv( 'atis_intents_train.csv', names = ['labels','sentences'] )\n",
    "atis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = atis_df.labels.copy()\n",
    "sentences_train = atis_df.sentences.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_shape = ( len( sentences_train ),96 )\n",
    "X_train = np.zeros( X_train_shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sentence in enumerate( sentences_train ):\n",
    "    X_train[idx,:] = nlp( sentence ).vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Nearest Neightbor Classification\n",
    "\n",
    "* Simplest Solution: \n",
    "    * look for the label that is most similar\n",
    "    * use its intent as a best guess\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nearest neighbor classification in scikit-learn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "test_message = \"i would like to find a flight from charlotte to las vegas that makes a stop in st. louis\"\n",
    "test_x = nlp( test_message ).vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'atis_flight'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the most similar using cosine similarity\n",
    "scores = [ cosine_similarity( X_train[idx,:].reshape(-1, 1), test_x.reshape(-1, 1) ) for idx in range( len( sentences_train ) ) ]\n",
    "labels_train[ np.argmax( scores ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "nearest neightbor works for many simple domains, but often the problem requires something more robust...  \n",
    "\n",
    "**Support Vector Machines/Classifier**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit( X_train, labels_train )\n",
    "#y_pred = clf.predict( test_x.reshape(-1, 1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Entity Extraction\n",
    "\n",
    "Keywords don't work for entities you haven't seen before  \n",
    "\n",
    "use the pre-built Named Entity Recognition from spaCy\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary PERSON\n",
      "Google ORG\n",
      "2009 DATE\n"
     ]
    }
   ],
   "source": [
    "doc = nlp( 'my friend Mary has worked at Google since 2009' )\n",
    "for ent in doc.ents:\n",
    "    print( ent.text, ent.label_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Roles for Entities\n",
    "\n",
    "ex: origin/destination for travel locations\n",
    "\n",
    "**parse tree** - a hierarchical structure that describes parent/child relationships  \n",
    "for example: 'a flight to Shanghai from Singapore'. to is the parent of Shanghai and from is the parent of Singapore. the sentence could be rephrased as 'a flight from Singapore to Shanghai' and the structure is unchanged.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_1 = re.compile( '.* from (.*) to (.*)')\n",
    "pattern_2 = re.compile( '.* to (.*) from (.*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[to, flight]\n",
      "[from, flight]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp( 'a flight to Shanghai from Singapore' )\n",
    "shanghai, singapore = doc[3], doc[5]\n",
    "print( list( shanghai.ancestors ) )\n",
    "print( list( singapore.ancestors ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color red belongs to item jacket\n",
      "color blue belongs to item jeans\n"
     ]
    }
   ],
   "source": [
    "doc = nlp( \"let's see that jacket in red and some blue jeans\" )\n",
    "items = [ doc[4], doc[10] ]\n",
    "colors = [ doc[6], doc[9] ]\n",
    "\n",
    "for color in colors:\n",
    "    for tok in color.ancestors:\n",
    "        if tok in items:\n",
    "            print( \"color {} belongs to item {}\".format( color, tok) )\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entity_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-5d237f3b1712>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Assign the colors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0massign_colors\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-85-5d237f3b1712>\u001b[0m in \u001b[0;36massign_colors\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Check for \"color\" entities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mentity_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"color\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0;31m# Find the parent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mfind_parent_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'entity_type' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the document\n",
    "doc = nlp( \"let's see that jacket in red and some blue jeans\" )\n",
    "\n",
    "# Iterate over parents in parse tree until an item entity is found\n",
    "def find_parent_item(word):\n",
    "    # Iterate over the word's ancestors\n",
    "    for parent in word.ancestors:\n",
    "        # Check for an \"item\" entity\n",
    "        if entity_type(parent) == \"item\":\n",
    "            return parent.text\n",
    "    return None\n",
    "\n",
    "# For all color entities, find their parent item\n",
    "def assign_colors(doc):\n",
    "    # Iterate over the document\n",
    "    for word in doc:\n",
    "        # Check for \"color\" entities\n",
    "        if entity_type(word) == \"color\":\n",
    "            # Find the parent\n",
    "            item =  find_parent_item(word)\n",
    "            print(\"item: {0} has color : {1}\".format(item, word))\n",
    "\n",
    "# Assign the colors\n",
    "assign_colors( doc )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Robust NLU with Rasa \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ruamel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-62579a22af94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import necessary modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrasa_nlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrasa_nlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRasaNLUConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrasa_nlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rasa_nlu/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrasa_nlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrasa_nlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrasa_nlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrun_evaluation\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrasa_nlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rasa_nlu/train.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrasa_nlu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrasa_nlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComponentBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrasa_nlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRasaNLUModelConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rasa_nlu/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mruamel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaml\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ruamel'"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from rasa_nlu.converters import load_data\n",
    "from rasa_nlu.config import RasaNLUConfig\n",
    "from rasa_nlu.model import Trainer\n",
    "\n",
    "# Create args dictionary\n",
    "args = {'pipeline' : 'spacy_sklearn'}\n",
    "\n",
    "# Create a configuration and trainer\n",
    "config = RasaNLUConfig( cmdline_args = args )\n",
    "trainer = Trainer( config )\n",
    "\n",
    "# Load the training data\n",
    "training_data = load_data(\"./training_data.json\")\n",
    "\n",
    "# Create an interpreter by training the model\n",
    "interpreter = trainer.train( training_data )\n",
    "\n",
    "# Test the interpreter\n",
    "print(interpreter.parse(\"I'm looking for a Mexican restaurant in the North of town\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
