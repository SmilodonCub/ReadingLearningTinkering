{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation in `Python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Source Language) $\\longrightarrow$ (Target Language)  \n",
    "\n",
    "One-hot encoded vectors:  \n",
    "\n",
    "* a sparse vector of ones and zeros\n",
    "    * 1: token is present\n",
    "    * 0: token is not present\n",
    "* vector length is determines by the size of the vocabulary\n",
    "    * vocabulary = set of tokens in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# mapping that contaains words and their corresponding indices\n",
    "word2index = { 'I':0, 'like':1, 'cats':2 }\n",
    "# converting words to IDs or indices\n",
    "words = [ 'I', 'like', 'cats' ]\n",
    "word_ids = [ word2index[w] for w in words ]\n",
    "print( word_ids )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', [1.0, 0.0, 0.0, 0.0, 0.0]), ('like', [0.0, 1.0, 0.0, 0.0, 0.0]), ('cats', [0.0, 0.0, 1.0, 0.0, 0.0])]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding with keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "onehot_1 = to_categorical( word_ids, num_classes=5 )\n",
    "print( [ (w,ohe.tolist()) for w,ohe in zip( words, onehot_1 )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# exploring the `to_categorical()` function\n",
    "def compute_onehot_length(words, word2index):\n",
    "  # Create word IDs for words\n",
    "  word_ids = [word2index[w] for w in words]\n",
    "  # Convert word IDs to onehot vectors\n",
    "  onehot = to_categorical(word_ids)\n",
    "  # Return the length of a single one-hot vector\n",
    "  return onehot.shape[1]\n",
    "\n",
    "word2index = {\"He\":0, \"drank\": 1, \"milk\": 2}\n",
    "# Compute and print onehot length of a list of words\n",
    "print(compute_onehot_length(['He','drank','milk'], word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length_1 => 9  and length_2 =>  6\n"
     ]
    }
   ],
   "source": [
    "# use the num_classes parameter to set the length of the vectors\n",
    "word2index = {'He': 6,'I': 0,'We': 3,'cats': 2,'dogs': 5,'hates': 7,'like': 4,'rabbits': 8}\n",
    "words_1 = [\"I\", \"like\", \"cats\", \"We\", \"like\", \"dogs\", \"He\", \"hates\", \"rabbits\"]\n",
    "# Call compute_onehot_length on words_1\n",
    "length_1 = compute_onehot_length(words_1, word2index)\n",
    "\n",
    "words_2 = [\"I\", \"like\", \"cats\", \"We\", \"like\", \"dogs\", \"We\", \"like\", \"cats\"]\n",
    "# Call compute_onehot_length on words_2\n",
    "length_2 = compute_onehot_length(words_2, word2index)\n",
    "\n",
    "# Print length_1 and length_2\n",
    "print(\"length_1 =>\", length_1, \" and length_2 => \", length_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Encoder Decoder Model\n",
    "\n",
    "A machine translation model works by, first, consuming words of the source language sequentially, and then, sequentially predicting the corresponding words in the target language  \n",
    "\n",
    "Input $\\longrightarrow$ **Encoder Model** $\\longrightarrow$ Context Vector $\\longrightarrow$  **Decoder Model** $\\longrightarrow$ Output\n",
    "\n",
    "**Writing the Encoder**  \n",
    "\n",
    "    def words2onehot( word_list, word2index ):\n",
    "        word_ids = [word2index[w] for w in word_list]\n",
    "        onehot = to_categorical( word_ids, 3 )\n",
    "        return onehot\n",
    "        \n",
    "    def encoder( onehot ):\n",
    "        word_ids = np.argmax( onehot, axis=1 ):\n",
    "        return word_ids \n",
    "        \n",
    "    onehot = word2onehot([\"I', 'like', 'cats']), words2index )\n",
    "    context = encoder( onehot )\n",
    "    print( context )\n",
    "    \n",
    "**Writing the Decoder**  \n",
    "\n",
    "    def decoder( context_vector ):\n",
    "        word_ids_rev = context_vector[::-1]\n",
    "        onehot_rev = to_categorical( word_ids_rev, 3 )\n",
    "        return onehot_rev\n",
    "        \n",
    "    def onehot2words( onehot, index2words):\n",
    "        ids = np.argmax( onehot, axis = 1 )\n",
    "        return [indext2word[id] for id in ids]\n",
    "        \n",
    "    onehot_rev = decoder( context )\n",
    "    reversed_words = onehot2words( onehot_rev, index2word )\n",
    "    print( reversed_words )\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', [1.0, 0.0, 0.0]), ('like', [0.0, 1.0, 0.0]), ('cats', [0.0, 0.0, 1.0])]\n"
     ]
    }
   ],
   "source": [
    "# The encoder\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "word2index = {'I': 0, 'cats': 2, 'like': 1}\n",
    "\n",
    "def words2onehot(word_list, word2index):\n",
    "  # Convert words to word IDs\n",
    "  word_ids = [word2index[w] for w in word_list]\n",
    "  # Convert word IDs to onehot vectors and return the onehot array\n",
    "  onehot = to_categorical(word_ids, num_classes=3)\n",
    "  return onehot\n",
    "\n",
    "words = [\"I\", \"like\", \"cats\"]\n",
    "# Convert words to onehot vectors using words2onehot\n",
    "onehot = words2onehot(words, word2index)\n",
    "# Print the result as (<word>, <onehot>) tuples\n",
    "print([(w,ohe.tolist()) for w,ohe in zip(words, onehot)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Encoder: Text reversing model\n",
    "def encoder(onehot):\n",
    "  # Get word IDs from onehot vectors and return the IDs\n",
    "  word_ids = np.argmax(onehot, axis=1)\n",
    "  return word_ids\n",
    "\n",
    "# Define \"We like dogs\" as words\n",
    "words = ['We','like','dogs']\n",
    "# Define the word2index dict\n",
    "word2index = {'We': 0, 'dogs': 2, 'like': 1}\n",
    "\n",
    "# Convert words to onehot vectors using words2onehot\n",
    "onehot = words2onehot(words, word2index)\n",
    "# Get the context vector by using the encoder function\n",
    "context = encoder(onehot)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dogs', 'like', 'We']\n"
     ]
    }
   ],
   "source": [
    "index2word = {0: 'We', 1: 'like', 2: 'dogs'}\n",
    "# Implementing the Decoder\n",
    "# Define the onehot2words function that returns words for a set of onehot vectors\n",
    "def onehot2words(onehot, index2word):\n",
    "  ids = np.argmax(onehot, axis=1)\n",
    "  res = [index2word[id] for id in ids]\n",
    "  return res\n",
    "# Define the decoder function that returns reversed onehot vectors\n",
    "def decoder(context_vector):\n",
    "  word_ids_rev = context_vector[::-1]\n",
    "  onehot_rev = to_categorical(word_ids_rev, num_classes=3)\n",
    "  return onehot_rev\n",
    "# Convert context to reversed onehot vectors using decoder\n",
    "onehot_rev = decoder(context)\n",
    "# Get the reversed words using the onehot2words function\n",
    "reversed_words = onehot2words(onehot_rev, index2word)\n",
    "print(reversed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Understanding Sequential Models\n",
    "\n",
    "**Time Series inputs and Sequential Models**  \n",
    "\n",
    "* sentences as time series input\n",
    "    * current word is affected by the previous words\n",
    "* The encoder/decoder uses a machine leaarning model that: \n",
    "    * **sequential model** - can learn from times series inputs \n",
    "    \n",
    "**Gated Recurrent Unit (GRU)** - sequential GRU units take in input ad pass a hidden state to the next unit until the sequence is processes. the hidden states at each unit represent the 'memory' of what the model has seen.  \n",
    "\n",
    "**`Keras` (functional API) refresher**  \n",
    "\n",
    "* `Keras` has two important objects: `Layer` and `Model` objects\n",
    "* Input Layer\n",
    "    * `inp = keras.layers.Input( shape = (...))`\n",
    "* Hidden Layer\n",
    "    * `layer = keras.layers.GRU(...)`\n",
    "* Output\n",
    "    * `out = layer( inp )`\n",
    "* Model\n",
    "    * `mode = Model( inputs=inp, outputs=out )`\n",
    "    \n",
    "**Understanding the Shape of the Data**  \n",
    "* Sequence data is 3-dimensional\n",
    "    1. **batch dimension** - the number of sequences\n",
    "    2. **time dimension** - the length of the sequences\n",
    "    3. **Input dimention** - length of the onehot vector (vocab length)\n",
    "    \n",
    "** Implementing GRUs with `Keras`**  \n",
    "\n",
    "Defining `Keras` layers:  \n",
    "\n",
    "    inp = keras.layers.Input( batchdim, timedim, inputdim ) \n",
    "    #for a model that takes arbitrary number of samples, leave out batchdim\n",
    "    gru_out, gru_state = keras.layers.GRU( 10, return_state =True )(inp)\n",
    "    #alternatively:\n",
    "    gru_out = keras.layers.GRU( 10, return_sequences=True )(inp)\n",
    "    \n",
    "Defining a `Keras` model:  \n",
    "\n",
    "    model = keras.model.Model( input=inp, outputs-gru_out )\n",
    "\n",
    "Predicting with the `Keras` model:  \n",
    "\n",
    "    x = np.random.normal( size = ( batchdim, timedim, inputdim ) )\n",
    "    y = model.predict( x )\n",
    "    print( \"shape (y) =', y.shape, \"\\ny =\\n\", y )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (y) = (2, 10) \n",
      "y = \n",
      " [[ 0.2810379   0.03663985 -0.02324818 -0.0489272   0.04935639 -0.04420675\n",
      "   0.02311645  0.20025104  0.00613629  0.00435218]\n",
      " [-0.19966927 -0.2918625   0.21411544 -0.04881026 -0.19061796  0.10403307\n",
      "  -0.21819672 -0.131522    0.06279384  0.41709882]]\n"
     ]
    }
   ],
   "source": [
    "#implement a simple model that has an input layer and a GRU layer. \n",
    "#You will then use the model to produce output values for a random input array.\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "# Define an input layer\n",
    "inp = keras.layers.Input(batch_shape=(2,3,4))\n",
    "# Define a GRU layer that takes in the input\n",
    "gru_out = keras.layers.GRU(10)(inp)\n",
    "\n",
    "# Define a model that outputs the GRU output\n",
    "model = keras.models.Model(inputs=inp, outputs=gru_out)\n",
    "\n",
    "x = np.random.normal(size=(2,3,4))\n",
    "# Get the output of the model and print the result\n",
    "y = model.predict(x)\n",
    "print(\"shape (y) =\", y.shape, \"\\ny = \\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (y1) =  (2, 10)  shape (y2) =  (5, 10)\n"
     ]
    }
   ],
   "source": [
    "#see how you can use Keras models to accept arbitrary sized batches of inputs\n",
    "\n",
    "# Define an input layer\n",
    "inp = keras.layers.Input(shape=(3,4))\n",
    "# Define a GRU layer that takes in the input\n",
    "gru_out = keras.layers.GRU(10)(inp)\n",
    "# Define a model that outputs the GRU output\n",
    "model = keras.models.Model(inputs=inp, outputs=gru_out)\n",
    "\n",
    "x1 = np.random.normal(size=(2,3,4))\n",
    "x2 = np.random.normal(size=(5,3,4))\n",
    "\n",
    "# Get the output of the model and print the result\n",
    "y1 = model.predict(x1)\n",
    "y2 = model.predict(x2)\n",
    "print(\"shape (y1) = \", y1.shape, \" shape (y2) = \", y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gru_out1.shape =  (3, 10)\n",
      "gru_out2.shape =  (3, 10)\n",
      "gru_state.shape =  (3, 10)\n",
      "gru_out3.shape =  (3, 25, 10)\n"
     ]
    }
   ],
   "source": [
    "# Define the Input layer\n",
    "inp = keras.layers.Input(batch_shape=(3,25,5))\n",
    "# Define a GRU layer that takes in inp as the input\n",
    "gru_out1 = keras.layers.GRU(10)(inp)\n",
    "print(\"gru_out1.shape = \", gru_out1.shape)\n",
    "\n",
    "# Define the second GRU and print the shape of the outputs\n",
    "gru_out2, gru_state = keras.layers.GRU(10, return_state=True)(inp)\n",
    "print(\"gru_out2.shape = \", gru_out2.shape)\n",
    "print(\"gru_state.shape = \", gru_state.shape)\n",
    "\n",
    "# Define the third GRU layer which will return all the outputs\n",
    "gru_out3 = keras.layers.GRU(10, return_sequences=True)(inp)\n",
    "print(\"gru_out3.shape = \", gru_out3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Implementing the Encoder/Decoder Model with `Keras`\n",
    "\n",
    "### Implementing the Encoder\n",
    "\n",
    "Understanding the Data:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( 'vocab_fr.txt' ) as f:\n",
    "    fr_text = f.readlines()\n",
    "    \n",
    "with open( 'vocab_en.txt' ) as f:\n",
    "    en_text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENglish:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "\n",
      "Frnedch:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "\n",
      "ENglish:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "\n",
      "Frnedch:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "\n",
      "ENglish:  california is usually quiet during march , and it is usually hot in june .\n",
      "\n",
      "Frnedch:  california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for en_sent, fr_sent in zip( en_text[:3], fr_text[:3]):\n",
    "    print( 'ENglish: ', en_sent )\n",
    "    print( 'Frnedch: ', fr_sent )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Tokenizing the Sentences\n",
    "\n",
    "Now to look at some of the attriutes of the DataSet  \n",
    "**Tokenization** - the process of breaking a sentence/phrase to individual tokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first sentence:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "\n",
      "\tWords:  ['new', 'jersey', 'is', 'sometimes', 'quiet', 'during', 'autumn', ',', 'and', 'it', 'is', 'snowy', 'in', 'april', '.\\n']\n"
     ]
    }
   ],
   "source": [
    "first_sent = en_text[0]\n",
    "print( 'first sentence: ', first_sent )\n",
    "first_words = first_sent.split(' ')\n",
    "print( '\\tWords: ', first_words )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Computing the average length of sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLIGH mean sentence length =  13.225678224285508\n",
      "FRENCH mean sentence length =  14.226737269693892\n"
     ]
    }
   ],
   "source": [
    "sent_length = [len(text.split(' ')) for text in en_text]\n",
    "mean_en_length = np.mean( sent_length )\n",
    "print( 'ENGLIGH mean sentence length = ', mean_en_length)\n",
    "\n",
    "sent_length = [len(text.split(' ')) for text in fr_text]\n",
    "mean_fr_length = np.mean( sent_length )\n",
    "print( 'FRENCH mean sentence length = ', mean_fr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLISH vocab size =  228\n",
      "FRENCH vocab size =  357\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "[all_words.extend( sent.split(' ')) for sent in en_text]\n",
    "en_vocab_size = len( set( all_words ) )\n",
    "print( 'ENGLISH vocab size = ', en_vocab_size )\n",
    "\n",
    "all_words = []\n",
    "[all_words.extend( sent.split(' ')) for sent in fr_text]\n",
    "fr_vocab_size = len( set( all_words ) )\n",
    "print( 'FRENCH vocab size = ', fr_vocab_size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Implementing the Encoder with `Keras`  \n",
    "\n",
    "Input Layer:  \n",
    "\n",
    "    en_inputs = Input( shape=(en_len, en_vocab))\n",
    "    \n",
    "GRU Layer:  \n",
    "\n",
    "    en_gru = GRU( hsize, return_state=True )\n",
    "    en_out, en_state = en_gru( en_Inputs )\n",
    "    \n",
    "`Keras` Model:  \n",
    "\n",
    "    encoder = Model( inputs=en_inputs, outputs=en_state )\n",
    "    print( encoder.summary() )\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        [(None, 15, 228)]         0         \n",
      "_________________________________________________________________\n",
      "gru_26 (GRU)                 [(None, 48), (None, 48)]  40032     \n",
      "=================================================================\n",
      "Total params: 40,032\n",
      "Trainable params: 40,032\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# defining the Encoder\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "en_len = 15\n",
    "en_vocab = 228\n",
    "hsize = 48\n",
    "\n",
    "# Define an input layer\n",
    "en_inputs = keras.layers.Input(shape=(en_len, en_vocab))\n",
    "# Define a GRU layer which returns the state\n",
    "en_gru = keras.layers.GRU(hsize, return_state = True)\n",
    "# Get the output and state from the GRU\n",
    "en_out, en_state = en_gru(en_inputs)\n",
    "# Define and print the model summary\n",
    "encoder = keras.models.Model(inputs=en_inputs, outputs=en_state)\n",
    "print(encoder.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Implementing the Decoder\n",
    "\n",
    "**Encoder-Decoder Model**  \n",
    "\n",
    "* Encoder consumes the English words one-by-one\n",
    "* Finally produces the context vector\n",
    "* Decoder takes the context vector as the initial state\n",
    "* Decoder produces French words one-by-one\n",
    "* Decoder is implemented using a `Keras` GPU layer. GRU requires two inputs:\n",
    "    1. a time series input\n",
    "    2. a hidden state\n",
    "\n",
    "How to produce the time series input for the GRU layer?  \n",
    "\n",
    "1. repeat the context vetor from the encoder N-many times\n",
    "    * ex: To produce a french sentence of 10 words, you repeat the context vector 10 times. \n",
    "    \n",
    "Understanding the `RepeatVector` layer:  \n",
    "\n",
    "* takes one argument which defines the sequence length of the required output\n",
    "* takes in an input of (batch_size, input_size)\n",
    "* output data will have the shape ( batch_size, sequence_length, input_size )\n",
    "\n",
    "**Defining a `RepeatVector` layer**  \n",
    "\n",
    "    from tensorflow.keras.layers import RepeatVector\n",
    "    rep = RepeatVector( 5 )\n",
    "    \n",
    "    r_inp = Input( shape( 3, ) )\n",
    "    r_out = rep( r_inp )\n",
    "    \n",
    "    repeat_model = Model( inputs= r_inp, outputs = r_out )\n",
    "    \n",
    "**Predicting with the Model**  \n",
    "\n",
    "    x = np.array( [ [0,1,2], [3,4,5] ] )\n",
    "    y = repeat_model.predict( x )\n",
    "    print( 'x.shape = ', x.shape, '\\ny.shape = ', y.shape )\n",
    "\n",
    "**Implementing the Decoder**  \n",
    "\n",
    "    de_inputs = RepeatVector( fr_len )( en_state )\n",
    "    decoder_gru = GRU( hsize, return_sequences=True )\n",
    "    gru_outputs = decoder_gru( de_inputs, initial_state=en_state )\n",
    "\n",
    "**Defining the Model**  \n",
    "\n",
    "    enc_dec = Model( inputs= en_inputs, outputs = gru_outputs )\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape =  (2, 2) \n",
      "y.shape =  (2, 6, 2)\n"
     ]
    }
   ],
   "source": [
    "# explore how the RepeatVector layer works\n",
    "from tensorflow.keras.layers import Input, RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "inp = Input(shape=(2,))\n",
    "# Define a RepeatVector that repeats the input 6 times\n",
    "rep = RepeatVector(6)(inp)\n",
    "# Define a model\n",
    "model = Model(inputs=inp, outputs=rep)\n",
    "# Define input x\n",
    "x = np.array([[0,1], [2,3]])\n",
    "# Get model prediction y\n",
    "y = model.predict( x )\n",
    "print('x.shape = ',x.shape,'\\ny.shape = ',y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_28\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 15, 228)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_26 (GRU)                    [(None, 48), (None,  40032       input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_7 (RepeatVector)  (None, 15, 48)       0           gru_26[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_27 (GRU)                    (None, 15, 48)       14112       repeat_vector_7[0][0]            \n",
      "                                                                 gru_26[0][1]                     \n",
      "==================================================================================================\n",
      "Total params: 54,144\n",
      "Trainable params: 54,144\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# implement the decoder and define an end-to-end model going from encoder inputs to the decoder GRU outputs. \n",
    "\n",
    "hsize = 48\n",
    "fr_len = 15\n",
    "# Define a RepeatVector layer\n",
    "de_inputs = RepeatVector(fr_len)(en_state)\n",
    "# Define a GRU model that returns all outputs\n",
    "decoder_gru = keras.layers.GRU(hsize, return_sequences=True)\n",
    "# Get the outputs of the decoder\n",
    "gru_outputs = decoder_gru(de_inputs, initial_state=en_state)\n",
    "# Define a model with the correct inputs and outputs\n",
    "enc_dec = Model(inputs=en_inputs, outputs=gru_outputs)\n",
    "enc_dec.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Dense and TimeDistributed Layers \n",
    "\n",
    "Introduction to the **Dense Layer** - a dense layer can be used to implement a fully-connected layer of a neural network.  \n",
    "\n",
    "* Dense Layer takes an input vector and converts to a probabilistic prediction\n",
    "    * y = Weightd.x + Bias  \n",
    "    \n",
    "Defining  Dense Laye with `Keras`:\n",
    "\n",
    "    dense = keras.layers.Dense( vicab_size, activation = 'softmax' )\n",
    "    inp = Input( shape=( vocab_size, )\n",
    "    pred = dense( inp )\n",
    "    model = Model( inputs=inp, outputs=pred )\n",
    "    \n",
    "Defining a Dense layer with custom initialization:\n",
    "\n",
    "    from tensorflow.keras.initializers import RandomNormal\n",
    "    init = RandomNormal( mean = 0.0, stddev = 0.05, seed = 6000 )\n",
    "    dense = Dense( vocab_size, activation='softmax', kernel_initializer=init, bias_initializer=init )\n",
    "    \n",
    "Inputs and outputs of a Dense Layer:  \n",
    "\n",
    "* Dense softmax layer\n",
    "    * takes a (batch_size, input_size) array\n",
    "    * produces a ( batch_size, num_classes ) array\n",
    "    * output for each sample is a probability distribution over the classes which sums to 1\n",
    "    * you can get the class of each sample using `np.argmax(y, axis=-1)`\n",
    "    \n",
    "Use a `TimeDistributed` layer as a wrapper for a `Dense` layer  \n",
    "\n",
    "    dense_time = TimeDistributedd( Dense( vocab_size, activation='softmax' ) )\n",
    "    inp = Input( shape = (  ) )\n",
    "    pred = dense_time( inp )\n",
    "    model = Model( inputs=inp, outputs=pred )\n",
    "    \n",
    "`TimeDistributed` Layer takes (batch_size, sequence_len, input_size) $\\longrightarrow$ ( batch_size, sequence_len, num_classes ) array  \n",
    "\n",
    "can get the class of each sample using `np.argmax( y, axis=-1 )`\n",
    "\n",
    "Iterating through time-distributed data:\n",
    "\n",
    "    for t in range( sequence_len ):\n",
    "        for prob, c in zip( y[:,t,:], classes[:,t]):\n",
    "            print( \"prob: ', prob, \", Class: ', c )\n",
    "            \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark has probabilities [0.3929537  0.37995604 0.22709025] and wins Gift voucher\n",
      "John has probabilities [0.33233336 0.34169823 0.32596847] and wins Car\n",
      "Kelly has probabilities [0.35587627 0.35802534 0.28609842] and wins Car\n"
     ]
    }
   ],
   "source": [
    "init = keras.initializers.RandomNormal( mean = 0.0, stddev = 0.05, seed = 6000 )\n",
    "# Define an input layer with batch size 3 and input size 3\n",
    "inp = Input(batch_shape = (3,3))\n",
    "# Get the output of the 3 node Dense layer\n",
    "pred = keras.layers.Dense(3, activation='softmax', kernel_initializer=init, bias_initializer=init)(inp)\n",
    "model = Model(inputs=inp, outputs=pred)\n",
    "\n",
    "names = [\"Mark\", \"John\", \"Kelly\"]\n",
    "prizes = [\"Gift voucher\", \"Car\", \"Nothing\"]\n",
    "x = np.array([[5, 0, 1], [0, 3, 1], [2, 2, 1]])\n",
    "# Compute the model prediction for x\n",
    "y = model.predict(x)\n",
    "# Get the most probable class for each sample\n",
    "classes = np.argmax(y, axis=-1)\n",
    "print(\"\\n\".join([\"{} has probabilities {} and wins {}\".format(n,p,prizes[c]) \\\n",
    "                 for n,p,c in zip(names, y, classes)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names=\n",
      " [['Mark', 'John', 'Kelly'], ['Jenny', 'Shan', 'Sarah']] \n",
      "x=\n",
      " [[[5 0 1]\n",
      "  [1 1 0]]\n",
      "\n",
      " [[0 3 1]\n",
      "  [0 4 0]]\n",
      "\n",
      " [[2 2 1]\n",
      "  [6 0 1]]] \n",
      "x.shape= (3, 2, 3)\n",
      "Game 1: Mark has probs [0.3929537  0.37995604 0.22709025] and wins Gift voucher\n",
      "\n",
      "Game 1: John has probs [0.33233336 0.34169823 0.32596847] and wins Car\n",
      "\n",
      "Game 1: Kelly has probs [0.35587627 0.35802534 0.28609842] and wins Car\n",
      "\n",
      "Game 2: Jenny has probs [0.34050465 0.3426381  0.31685725] and wins Car\n",
      "\n",
      "Game 2: Shan has probs [0.3069249  0.32335538 0.36971974] and wins Nothing\n",
      "\n",
      "Game 2: Sarah has probs [0.3994818  0.38477215 0.21574609] and wins Gift voucher\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [['Mark', 'John', 'Kelly'], ['Jenny', 'Shan', 'Sarah']]\n",
    "x = np.array([[[5, 0, 1],[1, 1, 0]],\n",
    "           [[0, 3, 1],[0, 4, 0]],\n",
    "           [[2, 2, 1],[6, 0, 1]]])\n",
    "# Print names and x\n",
    "print('names=\\n',names, '\\nx=\\n',x, '\\nx.shape=', x.shape)\n",
    "inp = Input(shape=(2, 3))\n",
    "# Create the TimeDistributed layer (the output of the Dense layer)\n",
    "dense_time = keras.layers.TimeDistributed(keras.layers.Dense(3, activation='softmax', kernel_initializer=init, bias_initializer=init))\n",
    "pred = dense_time(inp)\n",
    "model = Model(inputs=inp, outputs=pred)\n",
    "\n",
    "y = model.predict(x)\n",
    "# Get the most probable class for each sample\n",
    "classes = np.argmax(y, axis=-1)\n",
    "for t in range(2):\n",
    "  # Get the t-th time-dimension slice of y and classes\n",
    "  for n, p, c in zip(names[t], y[:, t, :], classes[:, t]):\n",
    "  \tprint(\"Game {}: {} has probs {} and wins {}\\n\".format(t+1,n,p,prizes[c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Implementing the Full Encoder/Decoder Model\n",
    "\n",
    "still need a top part of the decoder.  \n",
    "implement this with a `TimeDistributed` & `Dense` layer\n",
    "\n",
    "![](encoder_decoder.png)  \n",
    "\n",
    "Implementing the full model:  \n",
    "\n",
    "    # The softmax prediction layer\n",
    "    de_dense = keras.layers.Dense( fr_vocab_size, activation='softmax' )\n",
    "    de_dense_time = keras.layers.TimeDistributed( de_dense )\n",
    "    de_pred = de_seq_dense( de_out )\n",
    "    \n",
    "    # Defining the full model\n",
    "    nmt = keras.models.Model( inputs = en_inputs, outputs = de_pred )\n",
    "    \n",
    "    # Compiling the model\n",
    "    nmt.compile( optimizer='adam', loss='categorical_crossentropy`, metrics['acc'])\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (None, 15, 228)\n"
     ]
    }
   ],
   "source": [
    "fr_vocab_size = 228\n",
    "# Import Dense and TimeDistributed layers\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "# Define a softmax dense layer that has fr_vocab outputs\n",
    "de_dense = Dense(fr_vocab_size, activation='softmax')\n",
    "# Wrap the dense layer in a TimeDistributed layer\n",
    "de_dense_time = TimeDistributed(de_dense)\n",
    "# Get the final prediction of the model\n",
    "de_pred = de_dense_time(gru_outputs)\n",
    "print(\"Prediction shape: \", de_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_31\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 15, 228)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_26 (GRU)                    [(None, 48), (None,  40032       input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_7 (RepeatVector)  (None, 15, 48)       0           gru_26[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_27 (GRU)                    (None, 15, 48)       14112       repeat_vector_7[0][0]            \n",
      "                                                                 gru_26[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 15, 228)      11172       gru_27[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 65,316\n",
      "Trainable params: 65,316\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "# Define a model with encoder input and decoder output\n",
    "nmt = Model(inputs=en_inputs, outputs=de_pred)\n",
    "\n",
    "# Compile the model with an optimizer and a loss\n",
    "nmt.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# View the summary of the model \n",
    "nmt.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Training and Generating Translations\n",
    "\n",
    "### Preprocessing Data\n",
    "\n",
    "another look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sent:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "\n",
      "\f",
      "French sent:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "\n",
      "English sent:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "\n",
      "\f",
      "French sent:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "\n",
      "English sent:  california is usually quiet during march , and it is usually hot in june .\n",
      "\n",
      "\f",
      "French sent:  california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for en_sent, fr_sent in zip( en_text[:3], fr_text[:3]):\n",
    "    print( 'English sent: ', en_sent )\n",
    "    print( '\\fFrench sent: ', fr_sent )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**1st Step: Word Tokenization**  - the process of breaking a sentence/phrase to individual words/characters  \n",
    "\n",
    "Using the `Tokenizer()` object in `Keras`:  \n",
    "\n",
    "* learns the mapping from word to word ID using a given corpus\n",
    "* Can be used to convert a given string to a sequence of IDs  \n",
    "\n",
    "Instantiating a `Tokenizer()`:  \n",
    "\n",
    "    from tensorfloe.keras.preprocessing.text import Tokenizer\n",
    "    en_tok = Tokenizer()\n",
    "    \n",
    "Fitting the Tokenizer on data\n",
    "\n",
    "    en_tok = Tokenizer()\n",
    "    en_tok.fit_on_texts( en_text )\n",
    "    \n",
    "    # getting the word to ID mapping\n",
    "    id = en_tok.word_index[ \"january\" ]\n",
    "    \n",
    "    # getting the ID to word mapping\n",
    "    w = en_tok.index_word[ 51 ]\n",
    "    \n",
    "    # Transforming sentences to sequences:  \n",
    "    seq = en_tok.texts_to_sequences( [ 'she likes grapefruit, peaches, and lemons .' ] )  \n",
    "    \n",
    "Limiting the size of the vocabulary - you should not leave the tokenizer to do everything automatically. If you don't set up the tokenizer properly, it will learn many rare words in the dataset that are not powerful enough to improve the model. **out-of-vocabulary (OOV)** - words that are either rare or not present in the training set will be ignored by the tokenizer.\n",
    "\n",
    "    tok = Tokenizer( num_words = 50 )\n",
    "    \n",
    "    # defining OOV tokens\n",
    "    tok = Tokenizer( num_words=50, oov_token='UNK' )\n",
    "    \n",
    "<br>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "january  has id:  36\n",
      "apples  has id:  75\n",
      "summer  has id:  46\n"
     ]
    }
   ],
   "source": [
    "# tokenizing sentences with Keras\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define a Keras Tokenizer\n",
    "en_tok = Tokenizer()\n",
    "fr_tok = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on some text\n",
    "en_tok.fit_on_texts( en_text )\n",
    "fr_tok.fit_on_texts( fr_text )\n",
    "\n",
    "for w in [\"january\", \"apples\", \"summer\"]:\n",
    "  # Get the word ID of word w\n",
    "  id = en_tok.word_index[w]\n",
    "  # Print the word and the word ID\n",
    "  print(w, \" has id: \", id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word ID sequence:  [[27, 70, 28, 76, 7, 72]]\n",
      "Word ID sequence (with UNK):  [[28, 1, 29, 1, 8, 1]]\n",
      "The ID 1 represents the word:  UNK\n"
     ]
    }
   ],
   "source": [
    "# controlling the vocabulary with the Tokenizer\n",
    "# convert an arbitrary sentence to a sequence using a trained Tokenizer\n",
    "\n",
    "# Convert the sentence to a word ID sequence\n",
    "seq = en_tok.texts_to_sequences(['she likes grapefruit , peaches , and lemons .'])\n",
    "print('Word ID sequence: ', seq)\n",
    "\n",
    "# Define a tokenizer with vocabulary size 50 and oov_token 'UNK'\n",
    "en_tok_new = Tokenizer(num_words=50, oov_token='UNK')\n",
    "\n",
    "# Fit the tokenizer on en_text\n",
    "en_tok_new.fit_on_texts(en_text)\n",
    "\n",
    "# Convert the sentence to a word ID sequence\n",
    "seq_new = en_tok_new.texts_to_sequences(['she likes grapefruit , peaches , and lemons .'])\n",
    "print('Word ID sequence (with UNK): ', seq_new)\n",
    "print('The ID 1 represents the word: ', en_tok_new.index_word[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Processing the Text\n",
    "\n",
    "1. Adding special starting/ending tokens to target sentences\n",
    "2. Padding the sentences such that they all have the same length\n",
    "3. Reversing sentences - helps to make a stronger connection between the encoder & decoder\n",
    "\n",
    "an example of sentence padding:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 23, 1, 8, 67, 4, 39]  =>  [17 23  1  8 67  4 39  0  0  0  0  0]\n",
      "[22, 1, 10, 63, 4, 43, 6, 3, 1, 8, 53, 2, 48]  =>  [22  1 10 63  4 43  6  3  1  8 53  2]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "sentences = [\n",
    "    'new jersey is sometimes quiet during autumn .',\n",
    "    'california is never rainy during july , but it is sometimes beautiful in february .'\n",
    "]\n",
    "\n",
    "seqs = en_tok.texts_to_sequences( sentences )\n",
    "preproc_text = pad_sequences( seqs, padding='post', truncating= 'post', maxlen = 12 )\n",
    "for orig, padded in zip( seqs, preproc_text ):\n",
    "    print( orig, ' => ', padded )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an example of sentence reversal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:  california is never rainy during july , but it is sometimes beautiful in february .\n",
      "\tReversed:  july during rainy never is california\n"
     ]
    }
   ],
   "source": [
    "pad_seq = list( preproc_text[1] )\n",
    "pad_seq = pad_seq[::-1]\n",
    "rev_sent = [ en_tok.index_word[wid] for wid in pad_seq[-6:]]\n",
    "print( 'Sentences: ', sentences[1] )\n",
    "print( '\\tReversed: ',' '.join( rev_sent ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding tokens:  sos l'orange est son fruit préféré , mais la banane est votre favori .\n",
      " eos \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fr_text_new = []\n",
    "\n",
    "# Loop through all sentences in fr_text\n",
    "for sent in fr_text:  \n",
    "  # Add sos and eos tokens using string.join\n",
    "  sent_new = \" \".join(['sos', sent, 'eos'])\n",
    "  # Append the modified sentence to fr_text_new\n",
    "  fr_text_new.append(sent_new)\n",
    "\n",
    "    \n",
    "# Print sentence after adding tokens\n",
    "print(\"After adding tokens: \", sent_new, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0 27 70 28 76  7 72]]\n"
     ]
    }
   ],
   "source": [
    "# function to transform data conveniently to the format accepted \n",
    "#by the neural machine translation (NMT) model.\n",
    "en_len = 15\n",
    "en_vocab = 228\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def sents2seqs(input_type, sentences, onehot=False, pad_type='post'):\n",
    "    # Convert sentences to sequences      \n",
    "    encoded_text = en_tok.texts_to_sequences(sentences)\n",
    "    # Pad sentences to en_len\n",
    "    preproc_text = pad_sequences(encoded_text, padding=pad_type, truncating='post', maxlen=en_len)\n",
    "    if onehot:\n",
    "        # Convert the word IDs to onehot vectors\n",
    "        preproc_text = to_categorical(preproc_text, num_classes=en_vocab)\n",
    "    return preproc_text\n",
    "sentence = 'she likes grapefruit , peaches , and lemons .'  \n",
    "# Convert a sentence to sequence by pre-padding the sentence\n",
    "pad_seq = sents2seqs('source', [sentence], pad_type='pre')\n",
    "print( pad_seq )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tReversed:  july during rainy never is california\n"
     ]
    }
   ],
   "source": [
    "# reverse sentences for the encoder model\n",
    "# modify the sents2seqs function to reverse sentences\n",
    "sentences = [\"california is never rainy during july .\"]\n",
    "\n",
    "# Add new keyword parameter reverse which defaults to False\n",
    "def sents2seqs(input_type, sentences, onehot=False, pad_type='post', reverse=False):     \n",
    "    encoded_text = en_tok.texts_to_sequences(sentences)\n",
    "    preproc_text = pad_sequences(encoded_text, padding=pad_type, truncating='post', maxlen=en_len)\n",
    "    if reverse:\n",
    "      # Reverse the text using numpy axis reversing\n",
    "      preproc_text = preproc_text[:, ::-1]\n",
    "    if onehot:\n",
    "        preproc_text = to_categorical(preproc_text, num_classes=en_vocab)\n",
    "    return preproc_text\n",
    "\n",
    "\n",
    "# Call sents2seqs to get the padded and reversed sequence of IDs\n",
    "pad_seq = sents2seqs('source', sentences, reverse=True)\n",
    "rev_sent = [en_tok.index_word[wid] for wid in pad_seq[0][-6:]] \n",
    "print('\\tReversed: ',' '.join(rev_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Training the NMT Model\n",
    "\n",
    "Revisiting the model architecture:  \n",
    "\n",
    "* Encoder GRU\n",
    "    * consumes English words\n",
    "    * outputs a contect vector\n",
    "* Decoder GRU\n",
    "    * consumes the context vector\n",
    "    * outputs a sequence of GRU outputs\n",
    "* Decored prediction layer\n",
    "     * consumes the sequence of GRU outputs\n",
    "     * ouputs prediction probability for French words\n",
    "     \n",
    "Optimizing Model Parameters:  \n",
    "\n",
    "* often represented as `W` (weights) and `b` (bias) - these are initialized as random values\n",
    "* responsible for transforming a given input to a useful output\n",
    "* Changed over time to minimize a given loss using an optimizer\n",
    "    * **Loss** - computed as the difference between:\n",
    "        * the predictions (French words generated from the model)\n",
    "        * the actual outputs ( actual French words )\n",
    "* Inform the model during model compilation\n",
    "\n",
    "model compilation:\n",
    "\n",
    "    nmt.compile( optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'] )\n",
    "    \n",
    "Training the Model:  \n",
    "training the model involves iterating through the data in batches.  \n",
    "**batches** - a single iteration  \n",
    "**epochs** - a single traverse through all the data set  \n",
    "\n",
    "Training Iterations:  \n",
    "\n",
    "    for ei in range( n_epochs ): # single traverse through the dataset\n",
    "        for i in range( 0, data_size, batch_size ) # process a single batch\n",
    "        \n",
    "        # obtain a batch or training data\n",
    "        en_x = sents2seqs( 'source', en_text[ i:i+batch_size ], onehot=True, reverse=True )\n",
    "        de_y = sents2seqs( 'target', en_text[ i:i+batch_size ], onehot=True )\n",
    "        \n",
    "        # train on a single batch of data\n",
    "        nmt.train_on_batch( en_x, de_y )\n",
    "        \n",
    "        # evaluate the model\n",
    "        res = nmt.evaluate( en_x, de_y, batch_size=batch_size, verbose=0 )\n",
    "        print( \"Epoch {} => Train Loss:{}, Train Acc: {}\".format( ei+1, res[0], res[1]*100.0 )\n",
    "        \n",
    "Avoiding Overfitting  \n",
    "\n",
    "* Break the dataset into two parts:\n",
    "    - Training\n",
    "    - Validation\n",
    "* When the validation accuracy stops increasing, stop the training\n",
    "\n",
    "Splitting the Dataset:  \n",
    "\n",
    "    # define the train and validation datasets\n",
    "    train_size, valid_size = 800, 200\n",
    "    inds = np.arange( len( en_text ) )\n",
    "    np.random.shuffle( inds ) \n",
    "    \n",
    "    # get the train & validation indices\n",
    "    train_inds = inds[ :train_size ]\n",
    "    valid_inds = inds[ train_size : train_size+valid_size ] \n",
    "    \n",
    "    # splitting the dataset\n",
    "    tr_en = [ en_text[ ti ] for ti in train_inds ]\n",
    "    tr_fr = [ fr_text[ ti ] for ti in train_inds ]\n",
    "    v_en = [ en_text[ ti ] for ti in valid_inds ]\n",
    "    v_en = [ en_text[ ti ] for ti in valid_inds ]\n",
    "    \n",
    "Training the Model with Validation\n",
    "\n",
    "    for ei in range( n_epochs ): # single traverse through the dataset\n",
    "        for i in range( 0, data_size, batch_size ) # process a single batch\n",
    "        \n",
    "        # obtain a batch or training data\n",
    "        en_x = sents2seqs( 'source', tr_en[ i:i+batch_size ], onehot=True, reverse=True )\n",
    "        de_y = sents2seqs( 'target', te_fr[ i:i+batch_size ], onehot=True )\n",
    "        \n",
    "        # train on a single batch of data\n",
    "        nmt.train_on_batch( en_x, de_y )\n",
    "        \n",
    "    v_en_x = sents2seqs( 'source', v_en, onehot=True, padtype='pre' )\n",
    "    v_de_y = sents2seqs( 'target', v_fr, onehot=True )\n",
    "        \n",
    "    # evaluate the model\n",
    "    res = nmt.evaluate( v_en_x, dv_e_y, batch_size=batch_size, verbose=0 )\n",
    "    print( \"Epoch {} => Train Loss:{}, Train Acc: {}\".format( ei+1, res[0], res[1]*100.0 )\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (EN):\n",
      " ['china is sometimes cold during spring , and it is never chilly in may .\\n', 'california is busy during winter , and it is usually warm in january .\\n', 'california is never rainy during june , but it is cold in july .\\n'] \n",
      "Training (FR):\n",
      " ['la chine est parfois froid au printemps , et il est jamais froid en mai .\\n', \"californie est occupé pendant l' hiver , et il est habituellement chaud en janvier .\\n\", 'california est jamais pluvieux en juin , mais il fait froid en juillet .\\n']\n",
      "\n",
      "Valid (EN):\n",
      " ['california is usually mild during october , and it is usually relaxing in february .\\n', 'they dislike limes , oranges , and bananas.\\n', 'california is never quiet during summer , but it is never pleasant in july .\\n'] \n",
      "Valid (FR):\n",
      " ['californie est généralement doux en octobre , et il est relaxant habituellement en février .\\n', \"ils n'aiment pas , les oranges , citrons verts et les bananes .\\n\", \"california est jamais calme pendant l' été , mais il est jamais agréable en juillet .\\n\"]\n"
     ]
    }
   ],
   "source": [
    "# splitting the data into training and validation sets\n",
    "train_size, valid_size = 80000, 20000\n",
    "# Define a sequence of indices from 0 to len(en_text)\n",
    "inds = np.arange(len(en_text))\n",
    "np.random.shuffle(inds)\n",
    "train_inds = inds[:train_size]\n",
    "# Define valid_inds: last valid_size indices\n",
    "valid_inds = inds[train_size: train_size+valid_size]\n",
    "# Define tr_en (train EN sentences) and tr_fr (train FR sentences)\n",
    "tr_en = [en_text[ti] for ti in train_inds]\n",
    "tr_fr = [fr_text[ti] for ti in train_inds]\n",
    "# Define v_en (valid EN sentences) and v_fr (valid FR sentences)\n",
    "v_en = [en_text[vi] for vi in valid_inds]\n",
    "v_fr = [fr_text[vi] for vi in valid_inds]\n",
    "print('Training (EN):\\n', tr_en[:3], '\\nTraining (FR):\\n', tr_fr[:3])\n",
    "print('\\nValid (EN):\\n', v_en[:3], '\\nValid (FR):\\n', v_fr[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 => Loss:0.12522836029529572, Val Acc: 96.0669994354248\n",
      "2 => Loss:0.06793607771396637, Val Acc: 98.09200167655945\n",
      "3 => Loss:0.029000338166952133, Val Acc: 99.44133162498474\n"
     ]
    }
   ],
   "source": [
    "# Convert validation data to onehot\n",
    "v_en_x = sents2seqs('source', v_en, onehot=True, reverse=True)\n",
    "v_de_y = sents2seqs('target', v_fr, onehot=True)\n",
    "\n",
    "n_epochs, bsize = 3, 250\n",
    "for ei in range(n_epochs):\n",
    "  for i in range(0,train_size,bsize):\n",
    "    # Get a single batch of inputs and outputs\n",
    "    en_x = sents2seqs('source', tr_en[i:i+bsize], onehot=True, reverse=True)\n",
    "    de_y = sents2seqs('target', tr_fr[i:i+bsize], onehot=True)\n",
    "    # Train the model on a single batch of data\n",
    "    nmt.train_on_batch(en_x, de_y)    \n",
    "  # Evaluate the trained model on the validation data\n",
    "  res = nmt.evaluate(v_en_x, v_de_y, batch_size=valid_size, verbose=0)\n",
    "  print(\"{} => Loss:{}, Val Acc: {}\".format(ei+1,res[0], res[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_31\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 15, 228)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_26 (GRU)                    [(None, 48), (None,  40032       input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_7 (RepeatVector)  (None, 15, 48)       0           gru_26[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_27 (GRU)                    (None, 15, 48)       14112       repeat_vector_7[0][0]            \n",
      "                                                                 gru_26[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 15, 228)      11172       gru_27[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 65,316\n",
      "Trainable params: 65,316\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nmt.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Generating Translation with the NMT\n",
    "\n",
    "Motivation: we have a trained NMT model, but how can we use it to generate translations? \n",
    "\n",
    "try with an exampe sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34  2 51  8  1  3  6 47  4 62  8  1 21 20  5]]\n"
     ]
    }
   ],
   "source": [
    "en_st = ['the united states is sometimes chilly during december , but it is sometimes freezing in june .']\n",
    "en_seq = sents2seqs( 'source', en_st, onehot=True, reverse=True )\n",
    "print( np.argmax( en_seq, axis=-1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 15, 228)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# obtain the french translation\n",
    "fr_pred = nmt.predict( en_seq )\n",
    "print( fr_pred.shape )\n",
    "fr_seq = np.argmax( fr_pred, axis=-1)[0]\n",
    "print( fr_seq )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_sentence = ' '.join([fr_tok.index_word[i] for i in fr_seq if i != 0 ] )\n",
    "fr_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
