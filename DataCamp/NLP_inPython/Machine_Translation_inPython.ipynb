{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation in `Python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Source Language) $\\longrightarrow$ (Target Language)  \n",
    "\n",
    "One-hot encoded vectors:  \n",
    "\n",
    "* a sparse vector of ones and zeros\n",
    "    * 1: token is present\n",
    "    * 0: token is not present\n",
    "* vector length is determines by the size of the vocabulary\n",
    "    * vocabulary = set of tokens in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# mapping that contaains words and their corresponding indices\n",
    "word2index = { 'I':0, 'like':1, 'cats':2 }\n",
    "# converting words to IDs or indices\n",
    "words = [ 'I', 'like', 'cats' ]\n",
    "word_ids = [ word2index[w] for w in words ]\n",
    "print( word_ids )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', [1.0, 0.0, 0.0, 0.0, 0.0]), ('like', [0.0, 1.0, 0.0, 0.0, 0.0]), ('cats', [0.0, 0.0, 1.0, 0.0, 0.0])]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding with keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "onehot_1 = to_categorical( word_ids, num_classes=5 )\n",
    "print( [ (w,ohe.tolist()) for w,ohe in zip( words, onehot_1 )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# exploring the `to_categorical()` function\n",
    "def compute_onehot_length(words, word2index):\n",
    "  # Create word IDs for words\n",
    "  word_ids = [word2index[w] for w in words]\n",
    "  # Convert word IDs to onehot vectors\n",
    "  onehot = to_categorical(word_ids)\n",
    "  # Return the length of a single one-hot vector\n",
    "  return onehot.shape[1]\n",
    "\n",
    "word2index = {\"He\":0, \"drank\": 1, \"milk\": 2}\n",
    "# Compute and print onehot length of a list of words\n",
    "print(compute_onehot_length(['He','drank','milk'], word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length_1 => 9  and length_2 =>  6\n"
     ]
    }
   ],
   "source": [
    "# use the num_classes parameter to set the length of the vectors\n",
    "word2index = {'He': 6,'I': 0,'We': 3,'cats': 2,'dogs': 5,'hates': 7,'like': 4,'rabbits': 8}\n",
    "words_1 = [\"I\", \"like\", \"cats\", \"We\", \"like\", \"dogs\", \"He\", \"hates\", \"rabbits\"]\n",
    "# Call compute_onehot_length on words_1\n",
    "length_1 = compute_onehot_length(words_1, word2index)\n",
    "\n",
    "words_2 = [\"I\", \"like\", \"cats\", \"We\", \"like\", \"dogs\", \"We\", \"like\", \"cats\"]\n",
    "# Call compute_onehot_length on words_2\n",
    "length_2 = compute_onehot_length(words_2, word2index)\n",
    "\n",
    "# Print length_1 and length_2\n",
    "print(\"length_1 =>\", length_1, \" and length_2 => \", length_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Encoder Decoder Model\n",
    "\n",
    "A machine translation model works by, first, consuming words of the source language sequentially, and then, sequentially predicting the corresponding words in the target language  \n",
    "\n",
    "Input $\\longrightarrow$ **Encoder Model** $\\longrightarrow$ Context Vector $\\longrightarrow$  **Decoder Model** $\\longrightarrow$ Output\n",
    "\n",
    "**Writing the Encoder**  \n",
    "\n",
    "    def words2onehot( word_list, word2index ):\n",
    "        word_ids = [word2index[w] for w in word_list]\n",
    "        onehot = to_categorical( word_ids, 3 )\n",
    "        return onehot\n",
    "        \n",
    "    def encoder( onehot ):\n",
    "        word_ids = np.argmax( onehot, axis=1 ):\n",
    "        return word_ids \n",
    "        \n",
    "    onehot = word2onehot([\"I', 'like', 'cats']), words2index )\n",
    "    context = encoder( onehot )\n",
    "    print( context )\n",
    "    \n",
    "**Writing the Decoder**  \n",
    "\n",
    "    def decoder( context_vector ):\n",
    "        word_ids_rev = context_vector[::-1]\n",
    "        onehot_rev = to_categorical( word_ids_rev, 3 )\n",
    "        return onehot_rev\n",
    "        \n",
    "    def onehot2words( onehot, index2words):\n",
    "        ids = np.argmax( onehot, axis = 1 )\n",
    "        return [indext2word[id] for id in ids]\n",
    "        \n",
    "    onehot_rev = decoder( context )\n",
    "    reversed_words = onehot2words( onehot_rev, index2word )\n",
    "    print( reversed_words )\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', [1.0, 0.0, 0.0]), ('like', [0.0, 1.0, 0.0]), ('cats', [0.0, 0.0, 1.0])]\n"
     ]
    }
   ],
   "source": [
    "# The encoder\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "word2index = {'I': 0, 'cats': 2, 'like': 1}\n",
    "\n",
    "def words2onehot(word_list, word2index):\n",
    "  # Convert words to word IDs\n",
    "  word_ids = [word2index[w] for w in word_list]\n",
    "  # Convert word IDs to onehot vectors and return the onehot array\n",
    "  onehot = to_categorical(word_ids, num_classes=3)\n",
    "  return onehot\n",
    "\n",
    "words = [\"I\", \"like\", \"cats\"]\n",
    "# Convert words to onehot vectors using words2onehot\n",
    "onehot = words2onehot(words, word2index)\n",
    "# Print the result as (<word>, <onehot>) tuples\n",
    "print([(w,ohe.tolist()) for w,ohe in zip(words, onehot)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Encoder: Text reversing model\n",
    "def encoder(onehot):\n",
    "  # Get word IDs from onehot vectors and return the IDs\n",
    "  word_ids = np.argmax(onehot, axis=1)\n",
    "  return word_ids\n",
    "\n",
    "# Define \"We like dogs\" as words\n",
    "words = ['We','like','dogs']\n",
    "# Define the word2index dict\n",
    "word2index = {'We': 0, 'dogs': 2, 'like': 1}\n",
    "\n",
    "# Convert words to onehot vectors using words2onehot\n",
    "onehot = words2onehot(words, word2index)\n",
    "# Get the context vector by using the encoder function\n",
    "context = encoder(onehot)\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dogs', 'like', 'We']\n"
     ]
    }
   ],
   "source": [
    "index2word = {0: 'We', 1: 'like', 2: 'dogs'}\n",
    "# Implementing the Decoder\n",
    "# Define the onehot2words function that returns words for a set of onehot vectors\n",
    "def onehot2words(onehot, index2word):\n",
    "  ids = np.argmax(onehot, axis=1)\n",
    "  res = [index2word[id] for id in ids]\n",
    "  return res\n",
    "# Define the decoder function that returns reversed onehot vectors\n",
    "def decoder(context_vector):\n",
    "  word_ids_rev = context_vector[::-1]\n",
    "  onehot_rev = to_categorical(word_ids_rev, num_classes=3)\n",
    "  return onehot_rev\n",
    "# Convert context to reversed onehot vectors using decoder\n",
    "onehot_rev = decoder(context)\n",
    "# Get the reversed words using the onehot2words function\n",
    "reversed_words = onehot2words(onehot_rev, index2word)\n",
    "print(reversed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Understanding Sequential Models\n",
    "\n",
    "**Time Series inputs and Sequential Models**  \n",
    "\n",
    "* sentences as time series input\n",
    "    * current word is affected by the previous words\n",
    "* The encoder/decoder uses a machine leaarning model that: \n",
    "    * **sequential model** - can learn from times series inputs \n",
    "    \n",
    "**Gated Recurrent Unit (GRU)** - sequential GRU units take in input ad pass a hidden state to the next unit until the sequence is processes. the hidden states at each unit represent the 'memory' of what the model has seen.  \n",
    "\n",
    "**`Keras` (functional API) refresher**  \n",
    "\n",
    "* `Keras` has two important objects: `Layer` and `Model` objects\n",
    "* Input Layer\n",
    "    * `inp = keras.layers.Input( shape = (...))`\n",
    "* Hidden Layer\n",
    "    * `layer = keras.layers.GRU(...)`\n",
    "* Output\n",
    "    * `out = layer( inp )`\n",
    "* Model\n",
    "    * `mode = Model( inputs=inp, outputs=out )`\n",
    "    \n",
    "**Understanding the Shape of the Data**  \n",
    "* Sequence data is 3-dimensional\n",
    "    1. **batch dimension** - the number of sequences\n",
    "    2. **time dimension** - the length of the sequences\n",
    "    3. **Input dimention** - length of the onehot vector (vocab length)\n",
    "    \n",
    "** Implementing GRUs with `Keras`**  \n",
    "\n",
    "Defining `Keras` layers:  \n",
    "\n",
    "    inp = keras.layers.Input( batchdim, timedim, inputdim ) \n",
    "    #for a model that takes arbitrary number of samples, leave out batchdim\n",
    "    gru_out, gru_state = keras.layers.GRU( 10, return_state =True )(inp)\n",
    "    #alternatively:\n",
    "    gru_out = keras.layers.GRU( 10, return_sequences=True )(inp)\n",
    "    \n",
    "Defining a `Keras` model:  \n",
    "\n",
    "    model = keras.model.Model( input=inp, outputs-gru_out )\n",
    "\n",
    "Predicting with the `Keras` model:  \n",
    "\n",
    "    x = np.random.normal( size = ( batchdim, timedim, inputdim ) )\n",
    "    y = model.predict( x )\n",
    "    print( \"shape (y) =', y.shape, \"\\ny =\\n\", y )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (y) = (2, 10) \n",
      "y = \n",
      " [[ 0.2810379   0.03663985 -0.02324818 -0.0489272   0.04935639 -0.04420675\n",
      "   0.02311645  0.20025104  0.00613629  0.00435218]\n",
      " [-0.19966927 -0.2918625   0.21411544 -0.04881026 -0.19061796  0.10403307\n",
      "  -0.21819672 -0.131522    0.06279384  0.41709882]]\n"
     ]
    }
   ],
   "source": [
    "#implement a simple model that has an input layer and a GRU layer. \n",
    "#You will then use the model to produce output values for a random input array.\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "# Define an input layer\n",
    "inp = keras.layers.Input(batch_shape=(2,3,4))\n",
    "# Define a GRU layer that takes in the input\n",
    "gru_out = keras.layers.GRU(10)(inp)\n",
    "\n",
    "# Define a model that outputs the GRU output\n",
    "model = keras.models.Model(inputs=inp, outputs=gru_out)\n",
    "\n",
    "x = np.random.normal(size=(2,3,4))\n",
    "# Get the output of the model and print the result\n",
    "y = model.predict(x)\n",
    "print(\"shape (y) =\", y.shape, \"\\ny = \\n\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (y1) =  (2, 10)  shape (y2) =  (5, 10)\n"
     ]
    }
   ],
   "source": [
    "#see how you can use Keras models to accept arbitrary sized batches of inputs\n",
    "\n",
    "# Define an input layer\n",
    "inp = keras.layers.Input(shape=(3,4))\n",
    "# Define a GRU layer that takes in the input\n",
    "gru_out = keras.layers.GRU(10)(inp)\n",
    "# Define a model that outputs the GRU output\n",
    "model = keras.models.Model(inputs=inp, outputs=gru_out)\n",
    "\n",
    "x1 = np.random.normal(size=(2,3,4))\n",
    "x2 = np.random.normal(size=(5,3,4))\n",
    "\n",
    "# Get the output of the model and print the result\n",
    "y1 = model.predict(x1)\n",
    "y2 = model.predict(x2)\n",
    "print(\"shape (y1) = \", y1.shape, \" shape (y2) = \", y2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gru_out1.shape =  (3, 10)\n",
      "gru_out2.shape =  (3, 10)\n",
      "gru_state.shape =  (3, 10)\n",
      "gru_out3.shape =  (3, 25, 10)\n"
     ]
    }
   ],
   "source": [
    "# Define the Input layer\n",
    "inp = keras.layers.Input(batch_shape=(3,25,5))\n",
    "# Define a GRU layer that takes in inp as the input\n",
    "gru_out1 = keras.layers.GRU(10)(inp)\n",
    "print(\"gru_out1.shape = \", gru_out1.shape)\n",
    "\n",
    "# Define the second GRU and print the shape of the outputs\n",
    "gru_out2, gru_state = keras.layers.GRU(10, return_state=True)(inp)\n",
    "print(\"gru_out2.shape = \", gru_out2.shape)\n",
    "print(\"gru_state.shape = \", gru_state.shape)\n",
    "\n",
    "# Define the third GRU layer which will return all the outputs\n",
    "gru_out3 = keras.layers.GRU(10, return_sequences=True)(inp)\n",
    "print(\"gru_out3.shape = \", gru_out3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Implementing the Encoder/Decoder Model with `Keras`\n",
    "\n",
    "### Implementing the Encoder\n",
    "\n",
    "Understanding the Data:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( 'vocab_fr.txt' ) as f:\n",
    "    fr_text = f.readlines()\n",
    "    \n",
    "with open( 'vocab_en.txt' ) as f:\n",
    "    en_text = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENglish:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "\n",
      "Frnedch:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "\n",
      "ENglish:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "\n",
      "Frnedch:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "\n",
      "ENglish:  california is usually quiet during march , and it is usually hot in june .\n",
      "\n",
      "Frnedch:  california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for en_sent, fr_sent in zip( en_text[:3], fr_text[:3]):\n",
    "    print( 'ENglish: ', en_sent )\n",
    "    print( 'Frnedch: ', fr_sent )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Tokenizing the Sentences\n",
    "\n",
    "Now to look at some of the attriutes of the DataSet  \n",
    "**Tokenization** - the process of breaking a sentence/phrase to individual tokens  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first sentence:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "\n",
      "\tWords:  ['new', 'jersey', 'is', 'sometimes', 'quiet', 'during', 'autumn', ',', 'and', 'it', 'is', 'snowy', 'in', 'april', '.\\n']\n"
     ]
    }
   ],
   "source": [
    "first_sent = en_text[0]\n",
    "print( 'first sentence: ', first_sent )\n",
    "first_words = first_sent.split(' ')\n",
    "print( '\\tWords: ', first_words )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Computing the average length of sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLIGH mean sentence length =  13.225678224285508\n",
      "FRENCH mean sentence length =  14.226737269693892\n"
     ]
    }
   ],
   "source": [
    "sent_length = [len(text.split(' ')) for text in en_text]\n",
    "mean_en_length = np.mean( sent_length )\n",
    "print( 'ENGLIGH mean sentence length = ', mean_en_length)\n",
    "\n",
    "sent_length = [len(text.split(' ')) for text in fr_text]\n",
    "mean_fr_length = np.mean( sent_length )\n",
    "print( 'FRENCH mean sentence length = ', mean_fr_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLISH vocab size =  228\n",
      "FRENCH vocab size =  357\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "[all_words.extend( sent.split(' ')) for sent in en_text]\n",
    "en_vocab_size = len( set( all_words ) )\n",
    "print( 'ENGLISH vocab size = ', en_vocab_size )\n",
    "\n",
    "all_words = []\n",
    "[all_words.extend( sent.split(' ')) for sent in fr_text]\n",
    "fr_vocab_size = len( set( all_words ) )\n",
    "print( 'FRENCH vocab size = ', fr_vocab_size )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Implementing the Encoder with `Keras`  \n",
    "\n",
    "Input Layer:  \n",
    "\n",
    "    en_inputs = Input( shape=(en_len, en_vocab))\n",
    "    \n",
    "GRU Layer:  \n",
    "\n",
    "    en_gru = GRU( hsize, return_state=True )\n",
    "    en_out, en_state = en_gru( en_Inputs )\n",
    "    \n",
    "`Keras` Model:  \n",
    "\n",
    "    encoder = Model( inputs=en_inputs, outputs=en_state )\n",
    "    print( encoder.summary() )\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        [(None, 15, 228)]         0         \n",
      "_________________________________________________________________\n",
      "gru_26 (GRU)                 [(None, 48), (None, 48)]  40032     \n",
      "=================================================================\n",
      "Total params: 40,032\n",
      "Trainable params: 40,032\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# defining the Encoder\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "en_len = 15\n",
    "en_vocab = 228\n",
    "hsize = 48\n",
    "\n",
    "# Define an input layer\n",
    "en_inputs = keras.layers.Input(shape=(en_len, en_vocab))\n",
    "# Define a GRU layer which returns the state\n",
    "en_gru = keras.layers.GRU(hsize, return_state = True)\n",
    "# Get the output and state from the GRU\n",
    "en_out, en_state = en_gru(en_inputs)\n",
    "# Define and print the model summary\n",
    "encoder = keras.models.Model(inputs=en_inputs, outputs=en_state)\n",
    "print(encoder.summary() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Implementing the Decoder\n",
    "\n",
    "**Encoder-Decoder Model**  \n",
    "\n",
    "* Encoder consumes the English words one-by-one\n",
    "* Finally produces the context vector\n",
    "* Decoder takes the context vector as the initial state\n",
    "* Decoder produces French words one-by-one\n",
    "* Decoder is implemented using a `Keras` GPU layer. GRU requires two inputs:\n",
    "    1. a time series input\n",
    "    2. a hidden state\n",
    "\n",
    "How to produce the time series input for the GRU layer?  \n",
    "\n",
    "1. repeat the context vetor from the encoder N-many times\n",
    "    * ex: To produce a french sentence of 10 words, you repeat the context vector 10 times. \n",
    "    \n",
    "Understanding the `RepeatVector` layer:  \n",
    "\n",
    "* takes one argument which defines the sequence length of the required output\n",
    "* takes in an input of (batch_size, input_size)\n",
    "* output data will have the shape ( batch_size, sequence_length, input_size )\n",
    "\n",
    "**Defining a `RepeatVector` layer**  \n",
    "\n",
    "    from tensorflow.keras.layers import RepeatVector\n",
    "    rep = RepeatVector( 5 )\n",
    "    \n",
    "    r_inp = Input( shape( 3, ) )\n",
    "    r_out = rep( r_inp )\n",
    "    \n",
    "    repeat_model = Model( inputs= r_inp, outputs = r_out )\n",
    "    \n",
    "**Predicting with the Model**  \n",
    "\n",
    "    x = np.array( [ [0,1,2], [3,4,5] ] )\n",
    "    y = repeat_model.predict( x )\n",
    "    print( 'x.shape = ', x.shape, '\\ny.shape = ', y.shape )\n",
    "\n",
    "**Implementing the Decoder**  \n",
    "\n",
    "    de_inputs = RepeatVector( fr_len )( en_state )\n",
    "    decoder_gru = GRU( hsize, return_sequences=True )\n",
    "    gru_outputs = decoder_gru( de_inputs, initial_state=en_state )\n",
    "\n",
    "**Defining the Model**  \n",
    "\n",
    "    enc_dec = Model( inputs= en_inputs, outputs = gru_outputs )\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape =  (2, 2) \n",
      "y.shape =  (2, 6, 2)\n"
     ]
    }
   ],
   "source": [
    "# explore how the RepeatVector layer works\n",
    "from tensorflow.keras.layers import Input, RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "inp = Input(shape=(2,))\n",
    "# Define a RepeatVector that repeats the input 6 times\n",
    "rep = RepeatVector(6)(inp)\n",
    "# Define a model\n",
    "model = Model(inputs=inp, outputs=rep)\n",
    "# Define input x\n",
    "x = np.array([[0,1], [2,3]])\n",
    "# Get model prediction y\n",
    "y = model.predict( x )\n",
    "print('x.shape = ',x.shape,'\\ny.shape = ',y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_28\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 15, 228)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_26 (GRU)                    [(None, 48), (None,  40032       input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_7 (RepeatVector)  (None, 15, 48)       0           gru_26[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_27 (GRU)                    (None, 15, 48)       14112       repeat_vector_7[0][0]            \n",
      "                                                                 gru_26[0][1]                     \n",
      "==================================================================================================\n",
      "Total params: 54,144\n",
      "Trainable params: 54,144\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# implement the decoder and define an end-to-end model going from encoder inputs to the decoder GRU outputs. \n",
    "\n",
    "hsize = 48\n",
    "fr_len = 15\n",
    "# Define a RepeatVector layer\n",
    "de_inputs = RepeatVector(fr_len)(en_state)\n",
    "# Define a GRU model that returns all outputs\n",
    "decoder_gru = keras.layers.GRU(hsize, return_sequences=True)\n",
    "# Get the outputs of the decoder\n",
    "gru_outputs = decoder_gru(de_inputs, initial_state=en_state)\n",
    "# Define a model with the correct inputs and outputs\n",
    "enc_dec = Model(inputs=en_inputs, outputs=gru_outputs)\n",
    "enc_dec.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Dense and TimeDistributed Layers \n",
    "\n",
    "Introduction to the **Dense Layer** - a dense layer can be used to implement a fully-connected layer of a neural network.  \n",
    "\n",
    "* Dense Layer takes an input vector and converts to a probabilistic prediction\n",
    "    * y = Weightd.x + Bias  \n",
    "    \n",
    "Defining  Dense Laye with `Keras`:\n",
    "\n",
    "    dense = keras.layers.Dense( vicab_size, activation = 'softmax' )\n",
    "    inp = Input( shape=( vocab_size, )\n",
    "    pred = dense( inp )\n",
    "    model = Model( inputs=inp, outputs=pred )\n",
    "    \n",
    "Defining a Dense layer with custom initialization:\n",
    "\n",
    "    from tensorflow.keras.initializers import RandomNormal\n",
    "    init = RandomNormal( mean = 0.0, stddev = 0.05, seed = 6000 )\n",
    "    dense = Dense( vocab_size, activation='softmax', kernel_initializer=init, bias_initializer=init )\n",
    "    \n",
    "Inputs and outputs of a Dense Layer:  \n",
    "\n",
    "* Dense softmax layer\n",
    "    * takes a (batch_size, input_size) array\n",
    "    * produces a ( batch_size, num_classes ) array\n",
    "    * output for each sample is a probability distribution over the classes which sums to 1\n",
    "    * you can get the class of each sample using `np.argmax(y, axis=-1)`\n",
    "    \n",
    "Use a `TimeDistributed` layer as a wrapper for a `Dense` layer  \n",
    "\n",
    "    dense_time = TimeDistributedd( Dense( vocab_size, activation='softmax' ) )\n",
    "    inp = Input( shape = (  ) )\n",
    "    pred = dense_time( inp )\n",
    "    model = Model( inputs=inp, outputs=pred )\n",
    "    \n",
    "`TimeDistributed` Layer takes (batch_size, sequence_len, input_size) $\\longrightarrow$ ( batch_size, sequence_len, num_classes ) array  \n",
    "\n",
    "can get the class of each sample using `np.argmax( y, axis=-1 )`\n",
    "\n",
    "Iterating through time-distributed data:\n",
    "\n",
    "    for t in range( sequence_len ):\n",
    "        for prob, c in zip( y[:,t,:], classes[:,t]):\n",
    "            print( \"prob: ', prob, \", Class: ', c )\n",
    "            \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark has probabilities [0.3929537  0.37995604 0.22709025] and wins Gift voucher\n",
      "John has probabilities [0.33233336 0.34169823 0.32596847] and wins Car\n",
      "Kelly has probabilities [0.35587627 0.35802534 0.28609842] and wins Car\n"
     ]
    }
   ],
   "source": [
    "init = keras.initializers.RandomNormal( mean = 0.0, stddev = 0.05, seed = 6000 )\n",
    "# Define an input layer with batch size 3 and input size 3\n",
    "inp = Input(batch_shape = (3,3))\n",
    "# Get the output of the 3 node Dense layer\n",
    "pred = keras.layers.Dense(3, activation='softmax', kernel_initializer=init, bias_initializer=init)(inp)\n",
    "model = Model(inputs=inp, outputs=pred)\n",
    "\n",
    "names = [\"Mark\", \"John\", \"Kelly\"]\n",
    "prizes = [\"Gift voucher\", \"Car\", \"Nothing\"]\n",
    "x = np.array([[5, 0, 1], [0, 3, 1], [2, 2, 1]])\n",
    "# Compute the model prediction for x\n",
    "y = model.predict(x)\n",
    "# Get the most probable class for each sample\n",
    "classes = np.argmax(y, axis=-1)\n",
    "print(\"\\n\".join([\"{} has probabilities {} and wins {}\".format(n,p,prizes[c]) \\\n",
    "                 for n,p,c in zip(names, y, classes)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "names=\n",
      " [['Mark', 'John', 'Kelly'], ['Jenny', 'Shan', 'Sarah']] \n",
      "x=\n",
      " [[[5 0 1]\n",
      "  [1 1 0]]\n",
      "\n",
      " [[0 3 1]\n",
      "  [0 4 0]]\n",
      "\n",
      " [[2 2 1]\n",
      "  [6 0 1]]] \n",
      "x.shape= (3, 2, 3)\n",
      "Game 1: Mark has probs [0.3929537  0.37995604 0.22709025] and wins Gift voucher\n",
      "\n",
      "Game 1: John has probs [0.33233336 0.34169823 0.32596847] and wins Car\n",
      "\n",
      "Game 1: Kelly has probs [0.35587627 0.35802534 0.28609842] and wins Car\n",
      "\n",
      "Game 2: Jenny has probs [0.34050465 0.3426381  0.31685725] and wins Car\n",
      "\n",
      "Game 2: Shan has probs [0.3069249  0.32335538 0.36971974] and wins Nothing\n",
      "\n",
      "Game 2: Sarah has probs [0.3994818  0.38477215 0.21574609] and wins Gift voucher\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [['Mark', 'John', 'Kelly'], ['Jenny', 'Shan', 'Sarah']]\n",
    "x = np.array([[[5, 0, 1],[1, 1, 0]],\n",
    "           [[0, 3, 1],[0, 4, 0]],\n",
    "           [[2, 2, 1],[6, 0, 1]]])\n",
    "# Print names and x\n",
    "print('names=\\n',names, '\\nx=\\n',x, '\\nx.shape=', x.shape)\n",
    "inp = Input(shape=(2, 3))\n",
    "# Create the TimeDistributed layer (the output of the Dense layer)\n",
    "dense_time = keras.layers.TimeDistributed(keras.layers.Dense(3, activation='softmax', kernel_initializer=init, bias_initializer=init))\n",
    "pred = dense_time(inp)\n",
    "model = Model(inputs=inp, outputs=pred)\n",
    "\n",
    "y = model.predict(x)\n",
    "# Get the most probable class for each sample\n",
    "classes = np.argmax(y, axis=-1)\n",
    "for t in range(2):\n",
    "  # Get the t-th time-dimension slice of y and classes\n",
    "  for n, p, c in zip(names[t], y[:, t, :], classes[:, t]):\n",
    "  \tprint(\"Game {}: {} has probs {} and wins {}\\n\".format(t+1,n,p,prizes[c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Implementing the Full Encoder/Decoder Model\n",
    "\n",
    "still need a top part of the decoder.  \n",
    "implement this with a `TimeDistributed` & `Dense` layer\n",
    "\n",
    "![](encoder_decoder.png)  \n",
    "\n",
    "Implementing the full model:  \n",
    "\n",
    "    # The softmax prediction layer\n",
    "    de_dense = keras.layers.Dense( fr_vocab_size, activation='softmax' )\n",
    "    de_dense_time = keras.layers.TimeDistributed( de_dense )\n",
    "    de_pred = de_seq_dense( de_out )\n",
    "    \n",
    "    # Defining the full model\n",
    "    nmt = keras.models.Model( inputs = en_inputs, outputs = de_pred )\n",
    "    \n",
    "    # Compiling the model\n",
    "    nmt.compile( optimizer='adam', loss='categorical_crossentropy`, metrics['acc'])\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (None, 15, 228)\n"
     ]
    }
   ],
   "source": [
    "fr_vocab_size = 228\n",
    "# Import Dense and TimeDistributed layers\n",
    "from tensorflow.keras.layers import Dense, TimeDistributed\n",
    "# Define a softmax dense layer that has fr_vocab outputs\n",
    "de_dense = Dense(fr_vocab_size, activation='softmax')\n",
    "# Wrap the dense layer in a TimeDistributed layer\n",
    "de_dense_time = TimeDistributed(de_dense)\n",
    "# Get the final prediction of the model\n",
    "de_pred = de_dense_time(gru_outputs)\n",
    "print(\"Prediction shape: \", de_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_31\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 15, 228)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_26 (GRU)                    [(None, 48), (None,  40032       input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_7 (RepeatVector)  (None, 15, 48)       0           gru_26[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_27 (GRU)                    (None, 15, 48)       14112       repeat_vector_7[0][0]            \n",
      "                                                                 gru_26[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 15, 228)      11172       gru_27[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 65,316\n",
      "Trainable params: 65,316\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "# Define a model with encoder input and decoder output\n",
    "nmt = Model(inputs=en_inputs, outputs=de_pred)\n",
    "\n",
    "# Compile the model with an optimizer and a loss\n",
    "nmt.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# View the summary of the model \n",
    "nmt.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Training and Generating Translations\n",
    "\n",
    "### Preprocessing Data\n",
    "\n",
    "another look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sent:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "\n",
      "\f",
      "French sent:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "\n",
      "English sent:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "\n",
      "\f",
      "French sent:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n",
      "\n",
      "English sent:  california is usually quiet during march , and it is usually hot in june .\n",
      "\n",
      "\f",
      "French sent:  california est généralement calme en mars , et il est généralement chaud en juin .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for en_sent, fr_sent in zip( en_text[:3], fr_text[:3]):\n",
    "    print( 'English sent: ', en_sent )\n",
    "    print( '\\fFrench sent: ', fr_sent )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**1st Step: Word Tokenization**  - the process of breaking a sentence/phrase to individual words/characters  \n",
    "\n",
    "Using the `Tokenizer()` object in `Keras`:  \n",
    "\n",
    "* learns the mapping from word to word ID using a given corpus\n",
    "* Can be used to convert a given string to a sequence of IDs  \n",
    "\n",
    "Instantiating a `Tokenizer()`:  \n",
    "\n",
    "    from tensorfloe.keras.preprocessing.text import Tokenizer\n",
    "    en_tok = Tokenizer()\n",
    "    \n",
    "Fitting the Tokenizer on data\n",
    "\n",
    "    en_tok = Tokenizer()\n",
    "    en_tok.fit_on_texts( en_text )\n",
    "    \n",
    "    # getting the word to ID mapping\n",
    "    id = en_tok.word_index[ \"january\" ]\n",
    "    \n",
    "    # getting the ID to word mapping\n",
    "    w = en_tok.index_word[ 51 ]\n",
    "    \n",
    "    # Transforming sentences to sequences:  \n",
    "    seq = en_tok.texts_to_sequences( [ 'she likes grapefruit, peaches, and lemons .' ] )  \n",
    "    \n",
    "Limiting the size of the vocabulary - you should not leave the tokenizer to do everything automatically. If you don't set up the tokenizer properly, it will learn many rare words in the dataset that are not powerful enough to improve the model. **out-of-vocabulary (OOV)** - words that are either rare or not present in the training set will be ignored by the tokenizer.\n",
    "\n",
    "    tok = Tokenizer( num_words = 50 )\n",
    "    \n",
    "    # defining OOV tokens\n",
    "    tok = Tokenizer( num_words=50, oov_token='UNK' )\n",
    "    \n",
    "<br>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "january  has id:  36\n",
      "apples  has id:  75\n",
      "summer  has id:  46\n"
     ]
    }
   ],
   "source": [
    "# tokenizing sentences with Keras\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Define a Keras Tokenizer\n",
    "en_tok = Tokenizer()\n",
    "fr_tok = Tokenizer()\n",
    "\n",
    "# Fit the tokenizer on some text\n",
    "en_tok.fit_on_texts( en_text )\n",
    "fr_tok.fit_on_texts( fr_text )\n",
    "\n",
    "for w in [\"january\", \"apples\", \"summer\"]:\n",
    "  # Get the word ID of word w\n",
    "  id = en_tok.word_index[w]\n",
    "  # Print the word and the word ID\n",
    "  print(w, \" has id: \", id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word ID sequence:  [[27, 70, 28, 76, 7, 72]]\n",
      "Word ID sequence (with UNK):  [[28, 71, 29, 77, 8, 73]]\n",
      "The ID 1 represents the word:  UNK\n"
     ]
    }
   ],
   "source": [
    "# controlling the vocabulary with the Tokenizer\n",
    "# convert an arbitrary sentence to a sequence using a trained Tokenizer\n",
    "\n",
    "# Convert the sentence to a word ID sequence\n",
    "seq = en_tok.texts_to_sequences(['she likes grapefruit , peaches , and lemons .'])\n",
    "print('Word ID sequence: ', seq)\n",
    "\n",
    "# Define a tokenizer with vocabulary size 50 and oov_token 'UNK'\n",
    "en_tok_new = Tokenizer(num_words=100, oov_token='UNK')\n",
    "\n",
    "# Fit the tokenizer on en_text\n",
    "en_tok_new.fit_on_texts(en_text)\n",
    "\n",
    "# Convert the sentence to a word ID sequence\n",
    "seq_new = en_tok_new.texts_to_sequences(['she likes grapefruit , peaches , and lemons .'])\n",
    "print('Word ID sequence (with UNK): ', seq_new)\n",
    "print('The ID 1 represents the word: ', en_tok_new.index_word[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Processing the Text\n",
    "\n",
    "1. Adding special starting/ending tokens to target sentences\n",
    "2. Padding the sentences such that they all have the same length\n",
    "3. Reversing sentences - helps to make a stronger connection between the encoder & decoder\n",
    "\n",
    "an example of sentence padding:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 23, 1, 8, 67, 4, 39]  =>  [17 23  1  8 67  4 39  0  0  0  0  0]\n",
      "[22, 1, 10, 63, 4, 43, 6, 3, 1, 8, 53, 2, 48]  =>  [22  1 10 63  4 43  6  3  1  8 53  2]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "sentences = [\n",
    "    'new jersey is sometimes quiet during autumn .',\n",
    "    'california is never rainy during july , but it is sometimes beautiful in february .'\n",
    "]\n",
    "\n",
    "seqs = en_tok.texts_to_sequences( sentences )\n",
    "preproc_text = pad_sequences( seqs, padding='post', truncating= 'post', maxlen = 12 )\n",
    "for orig, padded in zip( seqs, preproc_text ):\n",
    "    print( orig, ' => ', padded )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an example of sentence reversal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:  california is never rainy during july , but it is sometimes beautiful in february .\n",
      "\tReversed:  july during rainy never is california\n"
     ]
    }
   ],
   "source": [
    "pad_seq = list( preproc_text[1] )\n",
    "pad_seq = pad_seq[::-1]\n",
    "rev_sent = [ en_tok.index_word[wid] for wid in pad_seq[-6:]]\n",
    "print( 'Sentences: ', sentences[1] )\n",
    "print( '\\tReversed: ',' '.join( rev_sent ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding tokens:  sos l'orange est son fruit préféré , mais la banane est votre favori .\n",
      " eos \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fr_text_new = []\n",
    "\n",
    "# Loop through all sentences in fr_text\n",
    "for sent in fr_text:  \n",
    "  # Add sos and eos tokens using string.join\n",
    "  sent_new = \" \".join(['sos', sent, 'eos'])\n",
    "  # Append the modified sentence to fr_text_new\n",
    "  fr_text_new.append(sent_new)\n",
    "\n",
    "    \n",
    "# Print sentence after adding tokens\n",
    "print(\"After adding tokens: \", sent_new, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0 27 70 28 76  7 72]]\n"
     ]
    }
   ],
   "source": [
    "# function to transform data conveniently to the format accepted \n",
    "#by the neural machine translation (NMT) model.\n",
    "en_len = 15\n",
    "en_vocab = 228\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def sents2seqs(input_type, sentences, onehot=False, pad_type='post'):\n",
    "    # Convert sentences to sequences      \n",
    "    encoded_text = en_tok.texts_to_sequences(sentences)\n",
    "    # Pad sentences to en_len\n",
    "    preproc_text = pad_sequences(encoded_text, padding=pad_type, truncating='post', maxlen=en_len)\n",
    "    if onehot:\n",
    "        # Convert the word IDs to onehot vectors\n",
    "        preproc_text = to_categorical(preproc_text, num_classes=en_vocab)\n",
    "    return preproc_text\n",
    "sentence = 'she likes grapefruit , peaches , and lemons .'  \n",
    "# Convert a sentence to sequence by pre-padding the sentence\n",
    "pad_seq = sents2seqs('source', [sentence], pad_type='pre')\n",
    "print( pad_seq )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tReversed:  july during rainy never is california\n"
     ]
    }
   ],
   "source": [
    "# reverse sentences for the encoder model\n",
    "# modify the sents2seqs function to reverse sentences\n",
    "sentences = [\"california is never rainy during july .\"]\n",
    "\n",
    "# Add new keyword parameter reverse which defaults to False\n",
    "def sents2seqs(input_type, sentences, onehot=False, pad_type='post', reverse=False):     \n",
    "    encoded_text = en_tok.texts_to_sequences(sentences)\n",
    "    preproc_text = pad_sequences(encoded_text, padding=pad_type, truncating='post', maxlen=en_len)\n",
    "    if reverse:\n",
    "      # Reverse the text using numpy axis reversing\n",
    "      preproc_text = preproc_text[:, ::-1]\n",
    "    if onehot:\n",
    "        preproc_text = to_categorical(preproc_text, num_classes=en_vocab)\n",
    "    return preproc_text\n",
    "\n",
    "\n",
    "# Call sents2seqs to get the padded and reversed sequence of IDs\n",
    "pad_seq = sents2seqs('source', sentences, reverse=True)\n",
    "rev_sent = [en_tok.index_word[wid] for wid in pad_seq[0][-6:]] \n",
    "print('\\tReversed: ',' '.join(rev_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Training the NMT Model\n",
    "\n",
    "Revisiting the model architecture:  \n",
    "\n",
    "* Encoder GRU\n",
    "    * consumes English words\n",
    "    * outputs a contect vector\n",
    "* Decoder GRU\n",
    "    * consumes the context vector\n",
    "    * outputs a sequence of GRU outputs\n",
    "* Decored prediction layer\n",
    "     * consumes the sequence of GRU outputs\n",
    "     * ouputs prediction probability for French words\n",
    "     \n",
    "Optimizing Model Parameters:  \n",
    "\n",
    "* often represented as `W` (weights) and `b` (bias) - these are initialized as random values\n",
    "* responsible for transforming a given input to a useful output\n",
    "* Changed over time to minimize a given loss using an optimizer\n",
    "    * **Loss** - computed as the difference between:\n",
    "        * the predictions (French words generated from the model)\n",
    "        * the actual outputs ( actual French words )\n",
    "* Inform the model during model compilation\n",
    "\n",
    "model compilation:\n",
    "\n",
    "    nmt.compile( optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'] )\n",
    "    \n",
    "Training the Model:  \n",
    "training the model involves iterating through the data in batches.  \n",
    "**batches** - a single iteration  \n",
    "**epochs** - a single traverse through all the data set  \n",
    "\n",
    "Training Iterations:  \n",
    "\n",
    "    for ei in range( n_epochs ): # single traverse through the dataset\n",
    "        for i in range( 0, data_size, batch_size ) # process a single batch\n",
    "        \n",
    "        # obtain a batch or training data\n",
    "        en_x = sents2seqs( 'source', en_text[ i:i+batch_size ], onehot=True, reverse=True )\n",
    "        de_y = sents2seqs( 'target', en_text[ i:i+batch_size ], onehot=True )\n",
    "        \n",
    "        # train on a single batch of data\n",
    "        nmt.train_on_batch( en_x, de_y )\n",
    "        \n",
    "        # evaluate the model\n",
    "        res = nmt.evaluate( en_x, de_y, batch_size=batch_size, verbose=0 )\n",
    "        print( \"Epoch {} => Train Loss:{}, Train Acc: {}\".format( ei+1, res[0], res[1]*100.0 )\n",
    "        \n",
    "Avoiding Overfitting  \n",
    "\n",
    "* Break the dataset into two parts:\n",
    "    - Training\n",
    "    - Validation\n",
    "* When the validation accuracy stops increasing, stop the training\n",
    "\n",
    "Splitting the Dataset:  \n",
    "\n",
    "    # define the train and validation datasets\n",
    "    train_size, valid_size = 800, 200\n",
    "    inds = np.arange( len( en_text ) )\n",
    "    np.random.shuffle( inds ) \n",
    "    \n",
    "    # get the train & validation indices\n",
    "    train_inds = inds[ :train_size ]\n",
    "    valid_inds = inds[ train_size : train_size+valid_size ] \n",
    "    \n",
    "    # splitting the dataset\n",
    "    tr_en = [ en_text[ ti ] for ti in train_inds ]\n",
    "    tr_fr = [ fr_text[ ti ] for ti in train_inds ]\n",
    "    v_en = [ en_text[ ti ] for ti in valid_inds ]\n",
    "    v_en = [ en_text[ ti ] for ti in valid_inds ]\n",
    "    \n",
    "Training the Model with Validation\n",
    "\n",
    "    for ei in range( n_epochs ): # single traverse through the dataset\n",
    "        for i in range( 0, data_size, batch_size ) # process a single batch\n",
    "        \n",
    "        # obtain a batch or training data\n",
    "        en_x = sents2seqs( 'source', tr_en[ i:i+batch_size ], onehot=True, reverse=True )\n",
    "        de_y = sents2seqs( 'target', te_fr[ i:i+batch_size ], onehot=True )\n",
    "        \n",
    "        # train on a single batch of data\n",
    "        nmt.train_on_batch( en_x, de_y )\n",
    "        \n",
    "    v_en_x = sents2seqs( 'source', v_en, onehot=True, padtype='pre' )\n",
    "    v_de_y = sents2seqs( 'target', v_fr, onehot=True )\n",
    "        \n",
    "    # evaluate the model\n",
    "    res = nmt.evaluate( v_en_x, dv_e_y, batch_size=batch_size, verbose=0 )\n",
    "    print( \"Epoch {} => Train Loss:{}, Train Acc: {}\".format( ei+1, res[0], res[1]*100.0 )\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (EN):\n",
      " ['china is sometimes cold during spring , and it is never chilly in may .\\n', 'california is busy during winter , and it is usually warm in january .\\n', 'california is never rainy during june , but it is cold in july .\\n'] \n",
      "Training (FR):\n",
      " ['la chine est parfois froid au printemps , et il est jamais froid en mai .\\n', \"californie est occupé pendant l' hiver , et il est habituellement chaud en janvier .\\n\", 'california est jamais pluvieux en juin , mais il fait froid en juillet .\\n']\n",
      "\n",
      "Valid (EN):\n",
      " ['california is usually mild during october , and it is usually relaxing in february .\\n', 'they dislike limes , oranges , and bananas.\\n', 'california is never quiet during summer , but it is never pleasant in july .\\n'] \n",
      "Valid (FR):\n",
      " ['californie est généralement doux en octobre , et il est relaxant habituellement en février .\\n', \"ils n'aiment pas , les oranges , citrons verts et les bananes .\\n\", \"california est jamais calme pendant l' été , mais il est jamais agréable en juillet .\\n\"]\n"
     ]
    }
   ],
   "source": [
    "# splitting the data into training and validation sets\n",
    "train_size, valid_size = 80000, 20000\n",
    "# Define a sequence of indices from 0 to len(en_text)\n",
    "inds = np.arange(len(en_text))\n",
    "np.random.shuffle(inds)\n",
    "train_inds = inds[:train_size]\n",
    "# Define valid_inds: last valid_size indices\n",
    "valid_inds = inds[train_size: train_size+valid_size]\n",
    "# Define tr_en (train EN sentences) and tr_fr (train FR sentences)\n",
    "tr_en = [en_text[ti] for ti in train_inds]\n",
    "tr_fr = [fr_text[ti] for ti in train_inds]\n",
    "# Define v_en (valid EN sentences) and v_fr (valid FR sentences)\n",
    "v_en = [en_text[vi] for vi in valid_inds]\n",
    "v_fr = [fr_text[vi] for vi in valid_inds]\n",
    "print('Training (EN):\\n', tr_en[:3], '\\nTraining (FR):\\n', tr_fr[:3])\n",
    "print('\\nValid (EN):\\n', v_en[:3], '\\nValid (FR):\\n', v_fr[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 => Loss:0.12522836029529572, Val Acc: 96.0669994354248\n",
      "2 => Loss:0.06793607771396637, Val Acc: 98.09200167655945\n",
      "3 => Loss:0.029000338166952133, Val Acc: 99.44133162498474\n"
     ]
    }
   ],
   "source": [
    "# Convert validation data to onehot\n",
    "v_en_x = sents2seqs('source', v_en, onehot=True, reverse=True)\n",
    "v_de_y = sents2seqs('target', v_fr, onehot=True)\n",
    "\n",
    "n_epochs, bsize = 3, 250\n",
    "for ei in range(n_epochs):\n",
    "  for i in range(0,train_size,bsize):\n",
    "    # Get a single batch of inputs and outputs\n",
    "    en_x = sents2seqs('source', tr_en[i:i+bsize], onehot=True, reverse=True)\n",
    "    de_y = sents2seqs('target', tr_fr[i:i+bsize], onehot=True)\n",
    "    # Train the model on a single batch of data\n",
    "    nmt.train_on_batch(en_x, de_y)    \n",
    "  # Evaluate the trained model on the validation data\n",
    "  res = nmt.evaluate(v_en_x, v_de_y, batch_size=valid_size, verbose=0)\n",
    "  print(\"{} => Loss:{}, Val Acc: {}\".format(ei+1,res[0], res[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_31\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 15, 228)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_26 (GRU)                    [(None, 48), (None,  40032       input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_7 (RepeatVector)  (None, 15, 48)       0           gru_26[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "gru_27 (GRU)                    (None, 15, 48)       14112       repeat_vector_7[0][0]            \n",
      "                                                                 gru_26[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 15, 228)      11172       gru_27[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 65,316\n",
      "Trainable params: 65,316\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nmt.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Generating Translation with the NMT\n",
    "\n",
    "Motivation: we have a trained NMT model, but how can we use it to generate translations? \n",
    "\n",
    "try with an exampe sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[34  2 51  8  1  3  6 47  4 62  8  1 21 20  5]]\n"
     ]
    }
   ],
   "source": [
    "en_st = ['the united states is sometimes chilly during december , but it is sometimes freezing in june .']\n",
    "en_seq = sents2seqs( 'source', en_st, onehot=True, reverse=True )\n",
    "print( np.argmax( en_seq, axis=-1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 15, 228)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# obtain the french translation\n",
    "fr_pred = nmt.predict( en_seq )\n",
    "print( fr_pred.shape )\n",
    "fr_seq = np.argmax( fr_pred, axis=-1)[0]\n",
    "print( fr_seq )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_sentence = ' '.join([fr_tok.index_word[i] for i in fr_seq if i != 0 ] )\n",
    "fr_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Teacher Forcing and Word Embeddings\n",
    "\n",
    "### Introduction to Teacher Forcing\n",
    "\n",
    "**Teacher Forcing** - the teacher guides the translation at each step to help the model learn faster  \n",
    "\n",
    "Implementing the model with Teacher Forcing:\n",
    "\n",
    "    # Encoder\n",
    "    en_inputs  = layers.Input( shape=(en_len, en_vocab))\n",
    "    en_gru - layers.GRU( hsize, return_state=True )\n",
    "    en_out, en_state = en_gru( en_inputs )\n",
    "    \n",
    "    # Decoder\n",
    "    de_inputs = layers.Input( shape= (fr_len-1, fr_vocab) )\n",
    "    de_gru = layers.GRU( hsize, return_sequences=True )\n",
    "    de_out = de_gru( de_inputs, initial_state=en_state )\n",
    "    \n",
    "    # Decoder Prediction\n",
    "    de_dense = layers.TimeDistributed( layers.Dense( fr_vocab, activation='softmax' ) )\n",
    "    de_pred = de_dense( de_out )\n",
    "    \n",
    "    # Compiling the Model\n",
    "    nmt_tf = Model( inputs=[en_inputs, de_inputs], outputs = de_pred )\n",
    "    nmt_tf.compile( optimizer='adam', loss='categorical_crossentropy', metrics=['acc'] )\n",
    "    \n",
    "Preprocessing the Data:  \n",
    "\n",
    "    # Encoder\n",
    "    # Inputs - all english words (onehot encoded)\n",
    "    en_x = sent2seqs( 'source', en_text, onehot=True, reverse=True )\n",
    "    \n",
    "    # Decoder\n",
    "    de_xy = sents2seqs( 'target', fr_text, onehot=True )\n",
    "    # Inputs - all french words except the last (onehot encoded)\n",
    "    de_x = de_xy[:,:-1,:]\n",
    "    # Outputs/Targets - all french words except the first word (onehot encodedd)\n",
    "    de_y = de_xy[:,1:,:]\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2onehot(tokenizer, word, vocab_size):\n",
    "    de_seq = tokenizer.texts_to_sequences([[word]])\n",
    "    de_onehot = to_categorical(de_seq, num_classes=vocab_size)\n",
    "    de_onehot = np.expand_dims(de_onehot, axis=1)    \n",
    "    return de_onehot\n",
    "\n",
    "def probs2word(probs, tok):\n",
    "    wid = np.argmax(probs[0,:], axis=-1)\n",
    "    w = tok.index_word[wid]\n",
    "    return w\n",
    "\n",
    "def sents2seqs(input_type, sentences, onehot=False, pad_type='post', reverse=False):\n",
    "    assert input_type in [\"source\", \"target\"]\n",
    "    if input_type == 'source':\n",
    "      tokenizer = en_tok\n",
    "      pad_length = en_len\n",
    "      vocab_size = en_vocab\n",
    "    elif input_type == 'target':\n",
    "      tokenizer = fr_tok\n",
    "      pad_length = fr_len\n",
    "      vocab_size = fr_vocab\n",
    "    \n",
    "    encoded_text = tokenizer.texts_to_sequences(sentences)\n",
    "    preproc_text = pad_sequences(encoded_text, padding=pad_type, maxlen=pad_length)\n",
    "    if reverse:\n",
    "      preproc_text = preproc_text[:,::-1]\n",
    "      \n",
    "    if onehot:\n",
    "        assert vocab_size is not None, \"Cannot do to_categorical without num_classes for safety\"\n",
    "        preproc_text = to_categorical(preproc_text, num_classes=vocab_size)\n",
    "    return preproc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the Teacher Forcing model layers\n",
    "\n",
    "en_len = 20\n",
    "fr_len = 25\n",
    "en_vocab = 200\n",
    "fr_vocab = 350 #357\n",
    "hsize = 64\n",
    "\n",
    "\n",
    "# Import the layers submodule from keras\n",
    "import tensorflow.keras.layers as layers\n",
    "\n",
    "en_inputs = layers.Input(shape=(en_len, en_vocab))\n",
    "en_gru = layers.GRU(hsize, return_state=True)\n",
    "# Get the encoder output and state\n",
    "en_out, en_state = en_gru(en_inputs)\n",
    "\n",
    "# Define the decoder input layer\n",
    "de_inputs = layers.Input(shape=(fr_len-1, fr_vocab))\n",
    "de_gru = layers.GRU(hsize, return_sequences=True)\n",
    "de_out = de_gru(de_inputs, initial_state=en_state)\n",
    "# Define a TimeDistributed Dense softmax layer with fr_vocab nodes\n",
    "de_dense = layers.TimeDistributed(layers.Dense(fr_vocab, activation='softmax'))\n",
    "de_pred = de_dense(de_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           [(None, 20, 200)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 24, 350)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru_14 (GRU)                    [(None, 64), (None,  51072       input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gru_15 (GRU)                    (None, 24, 64)       79872       input_16[0][0]                   \n",
      "                                                                 gru_14[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, 24, 350)      22750       gru_15[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 153,694\n",
      "Trainable params: 153,694\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# define the Teacher Forcing model\n",
    "\n",
    "# Import the Keras Model object\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define a model\n",
    "nmt_tf = Model(inputs=[en_inputs, de_inputs], outputs=de_pred)\n",
    "# Compile the model with optimizer and loss\n",
    "nmt_tf.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"acc\"])\n",
    "# Print the summary of the model\n",
    "nmt_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the Data\n",
    "\n",
    "#Define a tokenizer with vocabulary size 50 and oov_token 'UNK'\n",
    "#en_tok_new = Tokenizer(num_words=en_vocab, oov_token='UNK')\n",
    "#fr_tok_new = Tokenizer(num_words=fr_vocab, oov_token='UNK')\n",
    "\n",
    "\n",
    "bsize = 100\n",
    "for i in range(0, len(en_text), bsize):\n",
    "  # Get the encoder inputs using the sents2seqs() function\n",
    "  en_x = sents2seqs('source', en_text[i:i+bsize], onehot=True, reverse=True)\n",
    "  # Get the decoder inputs/outputs using the sents2seqs() function\n",
    "  de_xy = sents2seqs('target', fr_text[i:i+bsize], onehot=True)\n",
    "  # Separate the decoder inputs from de_xy\n",
    "  de_x = de_xy[:,:-1,:]\n",
    "  # Separate the decoder outputs from de_xy\n",
    "  de_y = de_xy[:,1:,:]\n",
    "  \n",
    "  #print(\"Data from \", i, \" to \", i+bsize)\n",
    "  #print(\"\\tnp.argmax() => en_x[0]: \", np.argmax(en_x[0], axis=-1))\n",
    "  #print(\"\\tnp.argmax() => de_x[0]: \", np.argmax(de_x[0], axis=-1))\n",
    "  #print(\"\\tnp.argmax() => de_y[0]: \", np.argmax(de_y[0], axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Training the Model with Teacher Forcing\n",
    "\n",
    "Training requires:\n",
    "\n",
    "* A loss function ('categotical_crossentropy'). to compute the loss, the following are required:\n",
    "    * probabilistic predictions generated using inputs ([batch_size, seq_len. vocab_size])\n",
    "    * actual onehot encoded french words ([batch_size, seq_len. vocab_size])\n",
    "    * cross entropy is found as the difference between the targets and predicted words\n",
    "* An optimizer ('adam') - loss is passed an optimizer like 'adam' which will change the model parameters to minimize the loss each time the train on batch is called. gradually improving the model parameters.\n",
    "\n",
    "Training the Model:  \n",
    "\n",
    "    n_epochs, bsize = 3, 250\n",
    "    \n",
    "    for ei in range( n_epochs ):\n",
    "        for i in range( 0, data_size, bsize ):\n",
    "            # Encoder inputs, decoder inputs and outputs\n",
    "            en_x = sents2seqs( 'source`, en_text[ i:i+bsize ], onehot=True, reverse=True )\n",
    "            de_xy = sents2seqs( 'target', fr_text[ i:i+bsize ], onehot=True )\n",
    "            # Separating decoder inputs and outputs\n",
    "            de_x = de_xy[:.:-1,:]\n",
    "            de_y = de_xy[:,1:,:]\n",
    "            # Training and evaluating on a single batch\n",
    "            nmt_tf.train_on_batch( [en_x, de_x], de_y )\n",
    "            res = nmt_tf.evaluate( [en_x, de_x], de_y, batch_size=bsize, verbose=0 )\n",
    "            print( \"{} => Train Loss:{}, Train Acc:{}\".format( ei+1, res[0], res[1]*100.0))\n",
    "            \n",
    "Creating Training and Validation Data:\n",
    "\n",
    "    # define the train and validation datasets\n",
    "    train_size, valid_size = 800, 200\n",
    "    inds = np.arange( len( en_text ) )\n",
    "    np.random.shuffle( inds ) \n",
    "\n",
    "    # get the train & validation indices\n",
    "    train_inds = inds[ :train_size ]\n",
    "    valid_inds = inds[ train_size : train_size+valid_size ] \n",
    "\n",
    "    # splitting the dataset\n",
    "    tr_en = [ en_text[ ti ] for ti in train_inds ]\n",
    "    tr_fr = [ fr_text[ ti ] for ti in train_inds ]\n",
    "    v_en = [ en_text[ ti ] for ti in valid_inds ]\n",
    "    v_en = [ en_text[ ti ] for ti in valid_inds ]\n",
    "    \n",
    "Training with Validation\n",
    "\n",
    "    n_epochs, bsize = 3, 250\n",
    "    \n",
    "    for ei in range( n_epochs ):\n",
    "        for i in range( 0, data_size, bsize ):\n",
    "            # Encoder inputs, decoder inputs and outputs\n",
    "            en_x = sents2seqs( 'source`, en_text[ i:i+bsize ], onehot=True, reverse=True )\n",
    "            de_xy = sents2seqs( 'target', fr_text[ i:i+bsize ], onehot=True )\n",
    "            # Separating decoder inputs and outputs\n",
    "            de_x = de_xy[:.:-1,:]\n",
    "            de_y = de_xy[:,1:,:]\n",
    "            # Training and evaluating on a single batch\n",
    "            nmt_tf.train_on_batch( [en_x, de_x], de_y )\n",
    "        v_en_x = sents2seqs( 'source', v_en, onehot=True, reverse=True )\n",
    "        v_de_xy = sents2seqs( 'target', v_fr, onehot=True )\n",
    "        v_de_x, v_de_y = v_de_xy[:,:-1,:], v_de_xy[:,1:,:]\n",
    "        res = nmt_tf.evaluate( [v_en_x, v_de_x], v_de_y, batch_size=bsize, verbose=0 )\n",
    "        print( \"{} => Loss:{}, Validation Acc:{}\".format( ei+1, res[0], res[1]*100.0))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (EN):\n",
      " ['he likes a big blue truck .\\n', 'new jersey is usually chilly during february , but it is quiet in fall .\\n', 'france is sometimes rainy during autumn , and it is usually snowy in march .\\n'] \n",
      "Training (FR):\n",
      " ['il aime un gros camion bleu .\\n', \"new jersey est généralement froid en février , mais il est calme à l' automne .\\n\", 'la france est parfois pluvieux en automne , et il est généralement enneigée en mars .\\n']\n",
      "\n",
      "Valid (EN):\n",
      " ['the united states is never nice during april , but it is wet in october .\\n', 'you dislike grapefruit , lemons , and pears .\\n', 'india is nice during may , but it is freezing in february .\\n'] \n",
      "Valid (FR):\n",
      " ['les états-unis est jamais agréable en avril , mais il est humide en octobre .\\n', \"vous n'aimez pas pamplemousses , les citrons et les poires .\\n\", \"l' inde est agréable au mois de mai , mais il gèle en février .\\n\"]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# splitting training and validation sets\n",
    "\n",
    "train_size, valid_size = 800, 200\n",
    "# Define a sequence of indices from 0 to size of en_text\n",
    "inds = np.arange(len(en_text))\n",
    "np.random.shuffle(inds)\n",
    "# Define train_inds as first train_size indices\n",
    "train_inds = inds[:train_size]\n",
    "valid_inds = inds[train_size:train_size+valid_size]\n",
    "# Define tr_en (train EN sentences) and tr_fr (train FR sentences)\n",
    "tr_en = [en_text[ti] for ti in train_inds]\n",
    "tr_fr = [fr_text[ti] for ti in train_inds]\n",
    "# Define v_en (valid EN sentences) and v_fr (valid FR sentences)\n",
    "v_en = [en_text[vi] for vi in valid_inds]\n",
    "v_fr = [fr_text[vi] for vi in valid_inds]\n",
    "print('Training (EN):\\n', tr_en[:3], '\\nTraining (FR):\\n', tr_fr[:3])\n",
    "print('\\nValid (EN):\\n', v_en[:3], '\\nValid (FR):\\n', v_fr[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 => Loss:5.795658111572266, Val Acc: 48.95833432674408\n",
      "2 => Loss:5.742404937744141, Val Acc: 54.83333468437195\n",
      "3 => Loss:5.6771697998046875, Val Acc: 54.6875\n"
     ]
    }
   ],
   "source": [
    "# training the model with validation\n",
    "n_epochs, bsize = 3, 250\n",
    "for ei in range(n_epochs):\n",
    "    for i in range(0,train_size,bsize):\n",
    "        en_x = sents2seqs('source', tr_en[i:i+bsize], onehot=True, reverse=True)\n",
    "        de_xy = sents2seqs('target', tr_fr[i:i+bsize], onehot=True)\n",
    "        # Create a single batch of decoder inputs and outputs\n",
    "        de_x, de_y = de_xy[:,:-1,:], de_xy[:,1:,:]\n",
    "        # Train the model on a single batch of data\n",
    "        nmt_tf.train_on_batch([en_x,de_x], de_y)      \n",
    "    v_en_x = sents2seqs('source', v_en, onehot=True, reverse=True)\n",
    "    # Create a single batch of validation decoder inputs and outputs\n",
    "    v_de_xy = sents2seqs('target', v_fr, onehot=True)\n",
    "    v_de_x, v_de_y = v_de_xy[:,:-1,:], v_de_xy[:,1:,:]\n",
    "    # Evaluate the trained model on the validation data\n",
    "    res = nmt_tf.evaluate([v_en_x,v_de_x], v_de_y, batch_size=valid_size, verbose=0)\n",
    "    print(\"{} => Loss:{}, Val Acc: {}\".format(ei+1,res[0], res[1]*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Generating Translations from the Model\n",
    "\n",
    "Decoder of the Inference Model:  \n",
    "\n",
    "* Takes in:\n",
    "    - a onehot encoded word\n",
    "    - a state input\n",
    "* Produces:\n",
    "    - a new state\n",
    "    - a prediction\n",
    "Recursively feed the predicted word and the state back to the model as input  \n",
    "the `sos` marking the beginning og the translation as the first word to the decoder  \n",
    "the `eos` marks the end of translation  \n",
    "as a safety measure use a maximum length the model can predict for\n",
    "\n",
    "Defining the Generator enccoder:  \n",
    "\n",
    "    # import keras layers and model\n",
    "    import tensorflow.keras.layers as layers\n",
    "    from tensorflow.keras.models import Model\n",
    "    \n",
    "    # Defining model layers\n",
    "    en_inputs = layers.Input( shape=( en_len, en_vocab ) )\n",
    "    en_gru = layers.GRU( hsize, return_state=True )\n",
    "    en_out, en_state = en_gru( en_inputs )\n",
    "    \n",
    "    # Defining Model object\n",
    "    encoder = Model( inputs= en_inputs, outputs= en_state )\n",
    "    \n",
    "Defining the Generator Decoder:  \n",
    "\n",
    "    # defining decoder Input layers\n",
    "    de_inputs = layers.Input( shape=(1,fr_vocab) )\n",
    "    de_state_in = layers.Input( shape=(hsize,) )\n",
    "    \n",
    "    # defining the decoders interim layers\n",
    "    de_gru = layers.GRU( hsize, return_state=True )\n",
    "    de_out, de_state_out = de_gru( de_inputs, initial_state=de_state_in )\n",
    "    de_dense = layers.Dense( fr_vocab, activation='softmax' )\n",
    "    de_pred = de_dense( de_out )\n",
    "    \n",
    "    # defining decoder model\n",
    "    decoder = Model( inputs=[ de_inputs, de_state_in ], outputs=[de_pred, de_state_out ] )\n",
    "    \n",
    "Important: need to copy the weights from Encoder GRU, Decoder GRU and Decoder Dense layers from the trained model to the prediction model  \n",
    "\n",
    "Generating Translation:  \n",
    "\n",
    "    en_sent['the united states is sometimes chilly during december , but is sometimes freezing in june .']\n",
    "    \n",
    "    # convert to a sequence\n",
    "    en_seq = sents2seqs( 'source', en_st, onehot=True, reverse=True )\n",
    "    \n",
    "    # get the context vector\n",
    "    de_s_t = encoder.predict( en_st )\n",
    "    \n",
    "    # converting 'sos' to a sequence\n",
    "    de_seq = word2onehot( fr_tok, 'sos', fr_vocab )\n",
    "    \n",
    "    #generating translation\n",
    "    fr_sent = ''\n",
    "    for _ in range( fr_len ):\n",
    "        de_prob, de_s_t = decoder.predict( [de_seq,de_s_t] )\n",
    "        de_w = probs2word( de_prob, fr_tok )\n",
    "        de_seq = word2onehot( fr_tok, de_w, fr_vocab )\n",
    "        if de_w == 'eos': break\n",
    "        fr_sent += de_w + ' '\n",
    "        \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras.models import Model\n",
    "# Define an input layer that accepts a single onehot encoded word\n",
    "de_inputs = layers.Input(shape=(1, fr_vocab))\n",
    "# Define an input to accept the t-1 state\n",
    "de_state_in = layers.Input(shape=(hsize,))\n",
    "de_gru = layers.GRU(hsize, return_state=True)\n",
    "# Get the output and state from the GRU layer\n",
    "de_out, de_state_out = de_gru(de_inputs, initial_state=de_state_in)\n",
    "de_dense = layers.Dense(fr_vocab, activation='softmax')\n",
    "de_pred = de_dense(de_out)\n",
    "\n",
    "# Define a model\n",
    "decoder = Model(inputs=[de_inputs, de_state_in], outputs=[de_pred, de_state_out])\n",
    "print(decoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of linking the trained model weights with the inference model\n",
    "\n",
    "# Load the weights to the encoder GRU from the trained model\n",
    "en_gru_w = tr_en_gru.get_weights()\n",
    "# Set the weights of the encoder GRU of the inference model\n",
    "en_gru.set_weights(en_gru_w)\n",
    "# Load and set the weights to the decoder GRU\n",
    "de_gru.set_weights(tr_de_gru.get_weights())\n",
    "# Load and set the weights to the decoder Dense\n",
    "de_dense.set_weights(tr_de_dense.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sent = ['the united states is sometimes chilly during december , but it is sometimes freezing in june .']\n",
    "print('English: {}'.format(en_sent))\n",
    "en_seq = sents2seqs('source', en_sent, onehot=True, reverse=True)\n",
    "# Predict the initial decoder state with the encoder\n",
    "de_s_t = encoder.predict(en_seq)\n",
    "de_seq = word2onehot(fr_tok, 'sos', fr_vocab)\n",
    "fr_sent = ''\n",
    "for i in range(fr_len):    \n",
    "  # Predict from the decoder and recursively assign the new state to de_s_t\n",
    "  de_prob, de_s_t = decoder.predict([de_seq,de_s_t])\n",
    "  # Get the word from the probability output using probs2word\n",
    "  de_w = probs2word(de_prob, fr_tok)\n",
    "  # Convert the word to a onehot sequence using word2onehot\n",
    "  de_seq = word2onehot(fr_tok, de_w, fr_vocab)\n",
    "  if de_w == 'eos': break\n",
    "  fr_sent += de_w + ' '\n",
    "print(\"French (Ours): {}\".format(fr_sent))\n",
    "print(\"French (Google Translate): les etats-unis sont parfois froids en décembre, mais parfois gelés en juin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Using Word Embeddings for Machine Translation\n",
    "\n",
    "finding the cosine similarity between word vectors:  \n",
    "\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "Implementing embeddings for the encoder:  \n",
    "\n",
    "    en_inputs = Input( shape=( en_len, ) )\n",
    "    en_emb = Embedding( en_vocab, embedding_size, input_length=en_len )( en_inputs )\n",
    "    en_out, en_state = GRU( hsize, return_state=True )( en_emb )\n",
    "    \n",
    "Implementing the decoder with Embedding:  \n",
    "\n",
    "    de_inputs = Input( shape=( fr_len-1, ) )\n",
    "    de_emb = Embedding( fr_vocab, embedding_size, input_length=fr_len-1 )( de_inputs )\n",
    "    de_out, _ = GRU( hsize, return_state=True, return_state=True )( de_emb, inititl_state=en_state )\n",
    "    \n",
    "Training the model:  \n",
    "\n",
    "    n_epochs, bsize = 3, 250\n",
    "\n",
    "    for ei in range( n_epochs ):\n",
    "        for i in range( 0, train_size, bsize ):\n",
    "            # Encoder inputs, decoder inputs and outputs\n",
    "            en_x = sents2seqs( 'source`, tr_en[ i:i+bsize ], onehot=True, reverse=True )\n",
    "            de_xy = sents2seqs( 'target', tr_fr[ i:i+bsize ], onehot=True )\n",
    "            # Separating decoder inputs and outputs\n",
    "            de_x = de_xy[:.:-1,:]\n",
    "            de_xy_oh = sents2seqs( 'target', tr_fr[i:i+bsize], onehot=True)\n",
    "            de_y = de_xy_oh[:,1:,:]\n",
    "            # Training and evaluating on a single batch\n",
    "            nmt_emb.train_on_batch( [en_x, de_x], de_y )\n",
    "            res = nmt_emb.evaluate( [en_x, de_x], de_y, batch_size=bsize, verbose=0 )\n",
    "            print( \"{} => Train Loss:{}, Train Acc:{}\".format( ei+1, res[0], res[1]*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_20 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           [(None, 24)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 20, 96)       33600       input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 24, 96)       33600       input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gru_18 (GRU)                    [(None, 64), (None,  31104       embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "gru_19 (GRU)                    [(None, 24, 64), (No 31104       embedding_4[0][0]                \n",
      "                                                                 gru_18[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 24, 350)      22750       gru_19[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 152,158\n",
      "Trainable params: 152,158\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# defining an embedding model\n",
    "\n",
    "# Define an input layer which accepts a sequence of word IDs\n",
    "en_inputs = layers.Input(shape=(en_len,))\n",
    "# Define an Embedding layer which accepts en_inputs\n",
    "en_emb = layers.Embedding(fr_vocab, 96, input_length=en_len)(en_inputs)\n",
    "en_out, en_state = layers.GRU(hsize, return_state=True)(en_emb)\n",
    "\n",
    "de_inputs = layers.Input(shape=(fr_len-1,))\n",
    "# Define an Embedding layer which accepts de_inputs\n",
    "de_emb = layers.Embedding(fr_vocab, 96, input_length=fr_len-1)(de_inputs)\n",
    "de_out, _ = layers.GRU(hsize, return_sequences=True, return_state=True)(de_emb, initial_state=en_state)\n",
    "de_pred = layers.TimeDistributed(layers.Dense(fr_vocab, activation='softmax'))(de_out)\n",
    "\n",
    "# Define the Model which accepts encoder/decoder inputs and outputs predictions \n",
    "nmt_emb = Model([en_inputs, de_inputs], de_pred)\n",
    "nmt_emb.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "nmt_emb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 => Loss:5.59381628036499, Train Acc: 54.366666078567505\n",
      "1 => Loss:5.555060386657715, Train Acc: 53.64999771118164\n",
      "1 => Loss:5.50683069229126, Train Acc: 53.64999771118164\n",
      "1 => Loss:5.423998832702637, Train Acc: 56.25\n",
      "2 => Loss:5.372900485992432, Train Acc: 53.94999980926514\n",
      "2 => Loss:5.296853065490723, Train Acc: 53.38333249092102\n",
      "2 => Loss:5.19899320602417, Train Acc: 52.88333296775818\n",
      "2 => Loss:5.029825210571289, Train Acc: 55.916666984558105\n",
      "3 => Loss:4.914877414703369, Train Acc: 53.64999771118164\n",
      "3 => Loss:4.7456254959106445, Train Acc: 52.99999713897705\n",
      "3 => Loss:4.529751300811768, Train Acc: 52.86666750907898\n",
      "3 => Loss:4.191209316253662, Train Acc: 55.75000047683716\n",
      "4 => Loss:3.971393585205078, Train Acc: 52.666664123535156\n",
      "4 => Loss:3.713937520980835, Train Acc: 52.016669511795044\n",
      "4 => Loss:3.467141628265381, Train Acc: 51.866668462753296\n",
      "4 => Loss:3.1483168601989746, Train Acc: 55.33333420753479\n",
      "5 => Loss:3.072063684463501, Train Acc: 52.666664123535156\n",
      "5 => Loss:2.9872772693634033, Train Acc: 52.016669511795044\n",
      "5 => Loss:2.9129676818847656, Train Acc: 51.866668462753296\n",
      "5 => Loss:2.7121899127960205, Train Acc: 55.33333420753479\n",
      "6 => Loss:2.7909319400787354, Train Acc: 52.666664123535156\n",
      "6 => Loss:2.8011152744293213, Train Acc: 52.016669511795044\n",
      "6 => Loss:2.795919895172119, Train Acc: 51.866668462753296\n",
      "6 => Loss:2.6265575885772705, Train Acc: 55.33333420753479\n",
      "7 => Loss:2.748014211654663, Train Acc: 52.666664123535156\n",
      "7 => Loss:2.7709710597991943, Train Acc: 52.016669511795044\n",
      "7 => Loss:2.771747350692749, Train Acc: 51.866668462753296\n",
      "7 => Loss:2.5974974632263184, Train Acc: 55.33333420753479\n",
      "8 => Loss:2.7157835960388184, Train Acc: 52.666664123535156\n",
      "8 => Loss:2.732551336288452, Train Acc: 52.016669511795044\n",
      "8 => Loss:2.7299089431762695, Train Acc: 51.866668462753296\n",
      "8 => Loss:2.556868314743042, Train Acc: 55.33333420753479\n",
      "9 => Loss:2.669156312942505, Train Acc: 52.666664123535156\n",
      "9 => Loss:2.6856205463409424, Train Acc: 52.016669511795044\n",
      "9 => Loss:2.6851484775543213, Train Acc: 51.866668462753296\n",
      "9 => Loss:2.5221853256225586, Train Acc: 55.33333420753479\n",
      "10 => Loss:2.6300225257873535, Train Acc: 52.666664123535156\n",
      "10 => Loss:2.650010824203491, Train Acc: 52.016669511795044\n",
      "10 => Loss:2.653909683227539, Train Acc: 51.866668462753296\n",
      "10 => Loss:2.5023484230041504, Train Acc: 55.33333420753479\n"
     ]
    }
   ],
   "source": [
    "# training the word embedding model\n",
    "\n",
    "for ei in range(10):\n",
    "  for i in range(0, train_size, bsize):    \n",
    "    en_x = sents2seqs('source', tr_en[i:i+bsize], onehot=False, reverse=True)\n",
    "    # Get a single batch of French sentences with no onehot encoding\n",
    "    de_xy = sents2seqs('target', tr_fr[i:i+bsize], onehot=False)\n",
    "    # Get all words except the last word in that batch\n",
    "    de_x = de_xy[:,:-1]\n",
    "    de_xy_oh = sents2seqs('target', tr_fr[i:i+bsize], onehot=True)\n",
    "    # Get all words except the first from de_xy_oh\n",
    "    de_y = de_xy_oh[:,1:,:]\n",
    "    # Training the model on a single batch of data\n",
    "    nmt_emb.train_on_batch([en_x,de_x], de_y)    \n",
    "    res = nmt_emb.evaluate([en_x, de_x], de_y, batch_size=bsize, verbose=0)\n",
    "    print(\"{} => Loss:{}, Train Acc: {}\".format(ei+1,res[0], res[1]*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Summary\n",
    "\n",
    "This tutorial worked through several different Machine Translation approaches ordered by increased complexity:  \n",
    "\n",
    "1. Model 1: NMT  \n",
    "    - encoder consumes english onehot encoded english words and returns a context vector\n",
    "    - decoder consumes the contect vector and outputs a translation\n",
    "2. Model 2: NMT + Teacher Forcing\n",
    "    - encoder consumes english onehot encoded english words and returns a context vector\n",
    "    - decoder consumes a given onehot encoded word of the translation and predicts the next word\n",
    "3. Model 3: NMT + TF + Embedding\n",
    "    - encoder uses word embeddings that capture the semantice relationships between words\n",
    "    - decoder consumes the embedding and returns a translation of the next word\n",
    "    \n",
    "Other developments:  \n",
    "\n",
    "* BLUE score\n",
    "* Word piece model\n",
    "* Transformer models - uses attention not sequential models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
