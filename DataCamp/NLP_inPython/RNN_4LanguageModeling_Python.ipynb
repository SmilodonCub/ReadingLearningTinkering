{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks for Language Modeling in `Python`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks and `Keras`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to the Course\n",
    "\n",
    "We will look at four applications of machine learning for text data  \n",
    "\n",
    "* Sentiment Analysis\n",
    "* Multi-class classification\n",
    "* Text generation\n",
    "* Machine neural translation\n",
    "\n",
    "**Recurrent Neural Networks** - reduce the number of parameters by avoiding one-hot encoding  \n",
    "**Sequence to sequence models** - man to one (e.g. for classification); many to many (e.g. text generation)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Language Models\n",
    "\n",
    "Language Models determine sentence probability\n",
    "\n",
    "* Probability of 'I loved this movie'\n",
    "* Unigram $P(\\mbox{sentence}) = P(\\mbox{I})P(\\mbox{loved})P(\\mbox{this})P(\\mbox{movie})$\n",
    "* Bigram $P(\\mbox{sentence})=P(\\mbox{I})P(\\mbox{loved|I})P(\\mbox{this|loved})P(\\mbox{movie|this})$\n",
    "* Trigram $P(\\mbox{sentence})=P(\\mbox{I})P(\\mbox{loved|I})P(\\mbox{this|I loved})P(\\mbox{movie|loved this})$\n",
    "* Skipgram $P(\\mbox{sentence})=P(\\mbox{context or I |I})P(\\mbox{context of loved|loved})P(\\mbox{context of this|this})P(\\mbox{context of movie|movie})$\n",
    "* Neural Network models\n",
    "    - $P(\\mbox{sentence})$ is given by a softmax function on the outer layer of the network\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build vocabulary dictionaries\n",
    "\n",
    "#Get unique words\n",
    "unique_words = list( set( text.split(' ') ) )\n",
    "\n",
    "#Create a dictionary: word is key, index is the value\n",
    "word_to_index = { k:v for (v,k) in enumerate( unique_words ) }\n",
    "\n",
    "#Create a dictionary: index is key, word is value\n",
    "index_to_word = { k:v for (k,v) in enumerate( unique_words ) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing input\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "#Loop over the text: length 'sentence_size' per time with step equal to 'step'\n",
    "for i in range( 0, len( text ) - sentence_size, step ):\n",
    "    X.append( text[ i:i + sentence_size])\n",
    "    y,append( text[ i + sentence_size ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming new texts\n",
    "new_text_split = []\n",
    "#Loop and get the indexes from the dictionary\n",
    "for sentence in new_text:\n",
    "    sent_split = []\n",
    "    for wd in sentence.split(' '):\n",
    "        ix = wd_to_index[ wd ]\n",
    "        sent_split.append( ix )\n",
    "    new_text_split.append( sent_split )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '(3', 1: 'Ah,', 2: \"Amy's\", 3: 'And', 4: 'Explorer', 5: 'Firefox.', 6: 'For', 7: 'Galileo,', 8: 'Goblin', 9: 'Green', 10: 'Hubble', 11: 'I', 12: \"I'm\", 13: 'Internet', 14: 'Ladybugs', 15: 'Oh', 16: 'Paul', 17: 'Penny', 18: 'Penny!', 19: 'Pope', 20: 'Scissors', 21: 'She', 22: 'Spider-Man,', 23: 'Spock', 24: 'Spock,', 25: 'Thankfully', 26: 'The', 27: 'Two', 28: 'V', 29: 'Well,', 30: 'What', 31: 'Wheaton!', 32: 'Wil', 33: \"You're\", 34: 'a', 35: 'afraid', 36: 'all', 37: 'always', 38: 'am', 39: 'and', 40: 'appeals', 41: 'are', 42: 'art', 43: 'as', 44: 'at', 45: 'aware', 46: 'based', 47: 'be', 48: 'became', 49: 'because', 50: 'been', 51: 'birthday', 52: 'bitch.', 53: 'black', 54: 'blood', 55: 'bottle.', 56: 'bottom', 57: 'brain', 58: 'breaker.', 59: 'bus', 60: 'but', 61: 'calls', 62: 'can', 63: 'care', 64: 'catatonic.', 65: 'center', 66: 'chance', 67: 'circuit', 68: 'computer', 69: 'could', 70: 'covers', 71: 'crushes', 72: 'cry', 73: 'cuts', 74: 'days', 75: 'decapitates', 76: 'deity.', 77: 'discovering', 78: 'disproves', 79: 'do', 80: 'does', 81: \"don't\", 82: 'eat', 83: 'eats', 84: 'every', 85: 'example,', 86: 'flashlight', 87: 'for', 88: 'free', 89: 'genitals,', 90: 'genitals.', 91: 'get', 92: 'ghost', 93: 'girlfriend', 94: 'gravity,', 95: 'had', 96: 'hand.', 97: 'has,', 98: 'have', 99: 'have?', 100: 'having', 101: 'heartless', 102: 'here', 103: 'hole', 104: 'humans', 105: 'if', 106: 'impairment;', 107: 'in', 108: 'insane,', 109: 'insects', 110: 'involves', 111: 'is', 112: \"isn't\", 113: 'it', 114: 'it.', 115: 'just', 116: 'kept', 117: 'knocks)', 118: 'later,', 119: 'little', 120: 'living', 121: 'lizard', 122: 'lizard,', 123: 'loud', 124: 'makes', 125: 'man', 126: 'masturbating', 127: 'me', 128: 'memory', 129: 'messy,', 130: 'money.', 131: 'moon-pie', 132: 'mother', 133: 'moved', 134: 'much', 135: 'must', 136: 'my', 137: 'next', 138: 'not', 139: 'nummy-nummy', 140: 'of', 141: 'on', 142: 'one.', 143: 'other', 144: 'others', 145: 'paper', 146: 'paper,', 147: 'people', 148: 'please', 149: 'poisons', 150: 'present', 151: 'prize', 152: 'relationship', 153: 'render', 154: 'reproduce', 155: 'right', 156: 'rock', 157: 'rock,', 158: 'rushed', 159: 'sad.', 160: 'say', 161: 'scissors', 162: 'scissors,', 163: 'scissors.', 164: 'searching', 165: 'sexual', 166: 'she', 167: 'smashes', 168: 'so', 169: 'sooner', 170: 'stopping', 171: 'stupid,', 172: 'taken', 173: 'telescope', 174: 'tested.', 175: 'that', 176: 'the', 177: 'things', 178: 'think', 179: 'thou', 180: 'three', 181: 'to', 182: 'today', 183: 'town.', 184: 'tried', 185: 'unnecessary', 186: 'unsanitary', 187: 'up.', 188: 'used', 189: 'usually', 190: 'vaporizes', 191: 'vodka', 192: 'way', 193: 'we', 194: 'well,', 195: 'which', 196: 'white', 197: 'will', 198: 'with', 199: 'women,', 200: 'would', 201: 'years,', 202: 'you', 203: 'your'}\n",
      "{'(3': 0, 'Ah,': 1, \"Amy's\": 2, 'And': 3, 'Explorer': 4, 'Firefox.': 5, 'For': 6, 'Galileo,': 7, 'Goblin': 8, 'Green': 9, 'Hubble': 10, 'I': 11, \"I'm\": 12, 'Internet': 13, 'Ladybugs': 14, 'Oh': 15, 'Paul': 16, 'Penny': 17, 'Penny!': 18, 'Pope': 19, 'Scissors': 20, 'She': 21, 'Spider-Man,': 22, 'Spock': 23, 'Spock,': 24, 'Thankfully': 25, 'The': 26, 'Two': 27, 'V': 28, 'Well,': 29, 'What': 30, 'Wheaton!': 31, 'Wil': 32, \"You're\": 33, 'a': 34, 'afraid': 35, 'all': 36, 'always': 37, 'am': 38, 'and': 39, 'appeals': 40, 'are': 41, 'art': 42, 'as': 43, 'at': 44, 'aware': 45, 'based': 46, 'be': 47, 'became': 48, 'because': 49, 'been': 50, 'birthday': 51, 'bitch.': 52, 'black': 53, 'blood': 54, 'bottle.': 55, 'bottom': 56, 'brain': 57, 'breaker.': 58, 'bus': 59, 'but': 60, 'calls': 61, 'can': 62, 'care': 63, 'catatonic.': 64, 'center': 65, 'chance': 66, 'circuit': 67, 'computer': 68, 'could': 69, 'covers': 70, 'crushes': 71, 'cry': 72, 'cuts': 73, 'days': 74, 'decapitates': 75, 'deity.': 76, 'discovering': 77, 'disproves': 78, 'do': 79, 'does': 80, \"don't\": 81, 'eat': 82, 'eats': 83, 'every': 84, 'example,': 85, 'flashlight': 86, 'for': 87, 'free': 88, 'genitals,': 89, 'genitals.': 90, 'get': 91, 'ghost': 92, 'girlfriend': 93, 'gravity,': 94, 'had': 95, 'hand.': 96, 'has,': 97, 'have': 98, 'have?': 99, 'having': 100, 'heartless': 101, 'here': 102, 'hole': 103, 'humans': 104, 'if': 105, 'impairment;': 106, 'in': 107, 'insane,': 108, 'insects': 109, 'involves': 110, 'is': 111, \"isn't\": 112, 'it': 113, 'it.': 114, 'just': 115, 'kept': 116, 'knocks)': 117, 'later,': 118, 'little': 119, 'living': 120, 'lizard': 121, 'lizard,': 122, 'loud': 123, 'makes': 124, 'man': 125, 'masturbating': 126, 'me': 127, 'memory': 128, 'messy,': 129, 'money.': 130, 'moon-pie': 131, 'mother': 132, 'moved': 133, 'much': 134, 'must': 135, 'my': 136, 'next': 137, 'not': 138, 'nummy-nummy': 139, 'of': 140, 'on': 141, 'one.': 142, 'other': 143, 'others': 144, 'paper': 145, 'paper,': 146, 'people': 147, 'please': 148, 'poisons': 149, 'present': 150, 'prize': 151, 'relationship': 152, 'render': 153, 'reproduce': 154, 'right': 155, 'rock': 156, 'rock,': 157, 'rushed': 158, 'sad.': 159, 'say': 160, 'scissors': 161, 'scissors,': 162, 'scissors.': 163, 'searching': 164, 'sexual': 165, 'she': 166, 'smashes': 167, 'so': 168, 'sooner': 169, 'stopping': 170, 'stupid,': 171, 'taken': 172, 'telescope': 173, 'tested.': 174, 'that': 175, 'the': 176, 'things': 177, 'think': 178, 'thou': 179, 'three': 180, 'to': 181, 'today': 182, 'town.': 183, 'tried': 184, 'unnecessary': 185, 'unsanitary': 186, 'up.': 187, 'used': 188, 'usually': 189, 'vaporizes': 190, 'vodka': 191, 'way': 192, 'we': 193, 'well,': 194, 'which': 195, 'white': 196, 'will': 197, 'with': 198, 'women,': 199, 'would': 200, 'years,': 201, 'you': 202, 'your': 203}\n"
     ]
    }
   ],
   "source": [
    "#extracted the vocabulary unique_words of the raw texts and created dictionaries to \n",
    "#go from words to numerical indexes and vice versa.\n",
    "\n",
    "sheldon_quotes = [\"You're afraid of insects and women, Ladybugs must render you catatonic.\",\n",
    " 'Scissors cuts paper, paper covers rock, rock crushes lizard, lizard poisons Spock, Spock smashes scissors, scissors decapitates lizard, lizard eats paper, paper disproves Spock, Spock vaporizes rock, and as it always has, rock crushes scissors.',\n",
    " 'For example, I cry because others are stupid, and that makes me sad.',\n",
    " \"I'm not insane, my mother had me tested.\",\n",
    " 'Two days later, Penny moved in and so much blood rushed to your genitals, your brain became a ghost town.',\n",
    " \"Amy's birthday present will be my genitals.\",\n",
    " '(3 knocks) Penny! (3 knocks) Penny! (3 knocks) Penny!',\n",
    " 'Thankfully all the things my girlfriend used to do can be taken care of with my right hand.',\n",
    " 'I would have been here sooner but the bus kept stopping for other people to get on it.',\n",
    " 'Oh gravity, thou art a heartless bitch.',\n",
    " 'I am aware of the way humans usually reproduce which is messy, unsanitary and based on living next to you for three years, involves loud and unnecessary appeals to a deity.',\n",
    " 'Well, today we tried masturbating for money.',\n",
    " 'I think that you have as much of a chance of having a sexual relationship with Penny as the Hubble telescope does of discovering at the center of every black hole is a little man with a flashlight searching for a circuit breaker.',\n",
    " \"Well, well, well, if it isn't Wil Wheaton! The Green Goblin to my Spider-Man, the Pope Paul V to my Galileo, the Internet Explorer to my Firefox.\",\n",
    " \"What computer do you have? And please don't say a white one.\",\n",
    " \"She calls me moon-pie because I'm nummy-nummy and she could just eat me up.\",\n",
    " 'Ah, memory impairment; the free prize at the bottom of every vodka bottle.']\n",
    "\n",
    "# Transform the list of sentences into a list of words\n",
    "all_words = ' '.join(sheldon_quotes).split(' ')\n",
    "\n",
    "# Get number of unique words\n",
    "unique_words = list(set(all_words))\n",
    "\n",
    "# Dictionary of indexes as keys and words as values\n",
    "index_to_word = {i:wd for i, wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(index_to_word)\n",
    "\n",
    "# Dictionary of words as keys and indexes as values\n",
    "word_to_index = {wd:i for i, wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to keep the sentences and the next character\n",
    "sentences = []   # ~ Training data\n",
    "next_chars = []  # ~ Training labels\n",
    "\n",
    "# Define hyperparameters\n",
    "step = 2          # ~ Step to take when reading the texts in characters\n",
    "chars_window = 10 # ~ Number of characters to use to predict the next one  \n",
    "\n",
    "# Loop over the text: length `chars_window` per time with step equal to `step`\n",
    "for i in range(0, len(sheldon_quotes) - chars_window, step):\n",
    "    sentences.append(sheldon_quotes[i:i + chars_window])\n",
    "    next_chars.append(sheldon_quotes[i + chars_window])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 125, 0, 0, 0, 43, 113, 0, 181, 0, 0, 113, 0, 39, 0, 113, 0, 0, 0, 0, 0, 141, 113, 39, 0, 181, 0, 0]\n",
      "(3 man (3 (3 (3 as it (3 to (3 (3 it (3 and (3 it (3 (3 (3 (3 (3 on it and (3 to (3 (3\n"
     ]
    }
   ],
   "source": [
    "new_text = ['A man either lives life as it happens to him meets it head-on and licks it or he turns his back on it and starts to wither away',\n",
    " 'To the brave crew and passengers of the Kobayshi Maru sucks to be you',\n",
    " 'Beware of more powerful weapons They often inflict as much damage to your soul as they do to you enemies',\n",
    " 'They are merely scars not mortal wounds and you must use them to propel you forward',\n",
    " 'You cannot explain away a wantonly immoral act because you think that it is connected to some higher purpose']\n",
    "\n",
    "# Loop through the sentences and get indexes\n",
    "new_text_split = []\n",
    "for sentence in new_text:\n",
    "    sent_split = []\n",
    "    for wd in sentence.split(' '):\n",
    "        index = word_to_index.get(wd, 0)\n",
    "        sent_split.append(index)\n",
    "    new_text_split.append(sent_split)\n",
    "\n",
    "# Print the first sentence's indexes\n",
    "print(new_text_split[0])\n",
    "\n",
    "# Print the sentence converted using the dictionary\n",
    "print(' '.join([index_to_word[index] for index in new_text_split[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Introduction to RNN inside `Keras`\n",
    "\n",
    "* `keras.models`\n",
    "    - `keras.models.Sequntial` - each layer is input to the following\n",
    "    - `keras.models.Model` - allows for more flexible model architecture\n",
    "* `keras.layers`\n",
    "    - `LSTM`\n",
    "    - `GRU`\n",
    "    - `Dense`\n",
    "    - `Dropout`\n",
    "    - `Embedding`\n",
    "    - `Bidirectional`\n",
    "* `keras.preprocessing`\n",
    "    - `keras.preprocessing.sequence.pad_sequences( text, maxlen=3 )` - make fixed length vectors\n",
    "* `keras.datasets` - IMDB, Reuters & more\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Building the Model\n",
    "\n",
    "    #Instantiate the model class\n",
    "    model = Sequential()\n",
    "\n",
    "    #Add the layers\n",
    "    model.add( Dense( 64, activation='relu', input_dim=100 ) )\n",
    "    model.add( Dense( 1, activation='sigmoid' ) )\n",
    "\n",
    "    #Compile the model\n",
    "    model.compile( optimizer='adam', loss='mean_squared_error', metrics=['accuracy'] )\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "\n",
    "    model.fit( X_train, y_train, epochs = 10, batch_size = 32 )\n",
    "    \n",
    "where:  \n",
    "\n",
    "1. **epochs** - determine how many weight updates will be done on the model\n",
    "2. **batch_size** - size of the data on each step\n",
    "\n",
    "\n",
    "Evaluate the Model\n",
    "\n",
    "    model.evaluate( X_test, y_test )\n",
    "    model.predict( new_data )\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "LSTM (LSTM)                  (None, 128)               71168     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 71,297\n",
      "Trainable params: 71,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential Model\n",
    "\n",
    "# Instantiate the class\n",
    "model = Sequential()\n",
    "\n",
    "# One LSTM layer (defining the input shape because it is the \n",
    "# initial layer)\n",
    "model.add(LSTM(128, input_shape=(None, 10), name=\"LSTM\"))\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "model.add(Dense(1, activation=\"sigmoid\", name=\"output\"))\n",
    "\n",
    "# The summary shows the layers and the number of parameters \n",
    "# that will be trained\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model model\n",
    "\n",
    "# Define the input layer\n",
    "main_input = Input(shape=(None, 10), name=\"input\")\n",
    "\n",
    "# One LSTM layer (input shape is already defined)\n",
    "lstm_layer = LSTM(128, name=\"LSTM\")(main_input)\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "main_output = Dense(1, activation=\"sigmoid\", name=\"output\")(lstm_layer)\n",
    "\n",
    "# Instantiate the class at the end\n",
    "model = Model(inputs=main_input, outputs=main_output, name=\"modelclass_model\")\n",
    "\n",
    "# Same amount of parameters to train as before (71,297)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it’s unobserved it will, however, if it’s observed after it’s left the plane but before it hits its target, it will not have gone through both slits.',\n",
       "       'Hello, female children. Allow me to inspire you with a story about a great female scientist. Polish-born, French-educated Madame Curie. Co-discoverer of radioactivity, she was a hero of science, until her hair fell out, her vomit and stool became filled with blood, and she was poisoned to death by her own discovery. With a little hard work, I see no reason why that can’t happen to any of you. Are we done? Can we go?'],\n",
       "      dtype='<U419')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "texts = np.array(['So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it’s unobserved it will, however, if it’s observed after it’s left the plane but before it hits its target, it will not have gone through both slits.',\n",
    "       'Hello, female children. Allow me to inspire you with a story about a great female scientist. Polish-born, French-educated Madame Curie. Co-discoverer of radioactivity, she was a hero of science, until her hair fell out, her vomit and stool became filled with blood, and she was poisoned to death by her own discovery. With a little hard work, I see no reason why that can’t happen to any of you. Are we done? Can we go?'],\n",
    "      dtype='<U419')\n",
    "\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the sample texts: (54, 78)\n",
      "Now the texts have fixed length: 60. Let's see the first one: \n",
      "[ 0  0  0  0  0  0 24  4  1 25 13 26  5  1 14  3 27  6 28  2  7 29 30 13\n",
      " 15  2  8 16 17  5 18  6  4  9 31  2  8 32  4  9 15 33  9 34 35 14 36 37\n",
      "  2 38 39 40  2  8 16 41 42  5 18  6]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text\n",
    "\n",
    "# Import relevant classes/functions\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Build the dictionary of indexes\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Change texts into sequence of indexes\n",
    "texts_numeric = tokenizer.texts_to_sequences(texts)\n",
    "print(\"Number of words in the sample texts: ({0}, {1})\".format(len(texts_numeric[0]), len(texts_numeric[1])))\n",
    "\n",
    "# Pad the sequences\n",
    "texts_pad = pad_sequences(texts_numeric, 60)\n",
    "print(\"Now the texts have fixed length: 60. Let's see the first one: \\n{0}\".format(texts_pad[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## RNN Architecture\n",
    "\n",
    "### Vanishing and Exploding Gradients\n",
    "\n",
    "**exploding gradient problem** - derivatives of a function can increase exponentially. this can be addressed with simple techniques such as gradient clipping.  \n",
    "\n",
    "**vanishing gradient problem** - when gradients vanish or go to zero.\n",
    "\n",
    "### GRU & LSTM Cells\n",
    "\n",
    "**GRU cells** - add a memory cell gate to determine if the value should update or maintain the previous weight. this prevents the RNN cell weight from going to zero (vanishing)  \n",
    "\n",
    "**LSTM (Long Short-Term Memory) cells** - adds 3 gates to the RNN cell. forget gate decides if the previous state should be forgotten. Update gate determines if the candidate state should be considered. Output gate determines whether the new hidden state should be considered  \n",
    "\n",
    "No more vanishing gradients  \n",
    "\n",
    "* the `simpleRNN` cell can have gradient problems\n",
    "    - the weight matrix power t multiples the other terms\n",
    "* `GRU` and `LSTM` cells don't have vanishing gradient problems\n",
    "    - because of their gates\n",
    "    - don't have the wieght matrix terms multiplying the rest\n",
    "    - exploding gradient problems are easier to solve\n",
    "    \n",
    "Usage in `keras`:\n",
    "\n",
    "    # Import the layers\n",
    "    from keras.layers import GRU, LSTM\n",
    "    \n",
    "    # Add the layers to a model\n",
    "    model.add( GRU( unit=128, return_sequences=True, name='GRU layer' ) )\n",
    "    model.add( LSTM( units=64, return_sequences=FALSE, name='LSTM layer' ) )\n",
    "    \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Embedding Layer\n",
    "\n",
    "Embeddings reduce the dimensionality of the data in comparison with one-hot encoding. Embeddings are a dense representations of the words. However, training can take a lot longer as there are a lot of parameters.  \n",
    "\n",
    "in `keras`:\n",
    "\n",
    "    from keras.layers import Embedding\n",
    "    model = Sequential()\n",
    "    \n",
    "    #use the embedding as the first layer\n",
    "    model.add( Embedding( input_dim=100000, #size of vocab\n",
    "                          output_dim=300, #dims of embedding space\n",
    "                          trainable=True, #update weights during training\n",
    "                          embeddings_initializer=None, #use pretrained weights\n",
    "                          input_length=120 ) ) #length of sequences to be modelled\n",
    "                          \n",
    "transfer learning for language modesl (GloVE, word2vec, BERT)  \n",
    "in `keras`:  \n",
    "\n",
    "    from keras.initializers import Constant\n",
    "    model.add( Embedding( input_dim = vocabulary size,\n",
    "                          output_dim = embedding_dim,\n",
    "                          embeddings_initializer=Constant(pre_trained_vectors))\n",
    "                          \n",
    "Using GloVE pre-trained vectors:  \n",
    "(https://nlp.stanford.edu/projects/glove/)  \n",
    "\n",
    "    #Get the CloVE vectors\n",
    "    def get_glove_vectors( filename=\"glove.6B.300d.txt\" )\n",
    "        # Get all word vectors from pre-trained model\n",
    "        glove_vector_dict = {}\n",
    "        with  open( filename ) as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = values[1:]\n",
    "                glove_vector_dict[word] = np.asarray( coefs, dtype='float32' )\n",
    "        return glove_vector_dict\n",
    "        \n",
    "        \n",
    "Using the GloVE on a specific task:  \n",
    "\n",
    "    #Filter GloVE vectors to be specific for the task vocab\n",
    "    def filter_glove( vocabulary_dict, glove_dict, wordvec_dim=300 ):\n",
    "        # Create a matrix to store the vectors\n",
    "        embedding_matrix = np.zeros( ( len( vocabulary_dict ) + 1, wordvec_dim ) )\n",
    "        for word,i in vocabulary_dict.items():\n",
    "            embedding_vector = glove_dict.get( word )\n",
    "            if embedding_vector is not None:\n",
    "                # words not found in the glove_dict will be all zeros\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        return embedding_mtrix\n",
    "        \n",
    "<br>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Classification Revisited\n",
    "\n",
    "Improving a model's performance:  \n",
    "\n",
    "* Add the embedding layer\n",
    "* Increase the number of layers\n",
    "* Tune the paramters\n",
    "* Increase vocabulary size\n",
    "* Accept longer sentences with more memory cells\n",
    "\n",
    "Avoiding overfitting:  \n",
    "\n",
    "* Test different batch sizes\n",
    "* Add `Dropout` layers\n",
    "* Add `dropout` and `recurrent_dropout` parameters to RNN layers  \n",
    "\n",
    "Adding a Convolution Layer:  \n",
    "\n",
    "* Convolution layer will do the feature selection on the embedding layer\n",
    "* Achieves tate-of-the-art results in may NLP problems\n",
    "\n",
    "An example model:  \n",
    "\n",
    "    model = Sequential()\n",
    "    model.add( Embedding( vocabulary_size, word_dim, trainable=True,\n",
    "                          embeddings_initializer=Constant(glove_matrix),\n",
    "                          input_length=max_text_len, name='Embedding' ) )\n",
    "    model.add( Dense( wordvec_dim, activation='relu', name='Dense11 ) )\n",
    "    model.add( Dropout( rate=0.25 ) ) #add noise by removing inputs\n",
    "    model.add( LSTM( 64, return_sequences=True, dropout=0.15, name='LSTM' ) )\n",
    "    model.add( GRU( 64, return_sequences=False, dropout=0.15, name='GRU' ) )\n",
    "    model.add( Dense( 64, name='Dense2' ) )\n",
    "    model.add( Dropout( rate=0.25 ) )\n",
    "    model.add( Dense( 32, name='Dense3' ) )\n",
    "    model.add( Dense( 1, activation='sigmoid', name='Output' ) )\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Classification\n",
    "\n",
    "### Data pre-processing\n",
    "\n",
    "differences from binary to multi-class classification problems:  \n",
    "\n",
    "* shape of the output variable $y$\n",
    "    - one hot encoding will make each class equidistant\n",
    "    - solfmax function will return the probability of each class\n",
    "* Number of units on the output layer\n",
    "* Activation function on the output layer\n",
    "    - solfmax used instead of a sigmoid (logistic) function\n",
    "* Loss function\n",
    "    - use the `categorical_crossentropy` function\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    3\n",
      "1    1\n",
      "2    0\n",
      "3    3\n",
      "4    2\n",
      "dtype: int8\n",
      "Index(['data_science', 'economy', 'finance', 'sports'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Preparing text categories for keras (numerical representation)\n",
    "\n",
    "import pandas as pd \n",
    "y = [ 'sports', 'economy', 'data_science', 'sports', 'finance' ]\n",
    "y_series = pd.Series( y, dtype='category' )\n",
    "print( y_series.cat.codes )\n",
    "print( y_series.cat.categories )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Preparing the data as 1-hot encoded\n",
    "\n",
    "from keras.utils.np_utils import to_categorical \n",
    "y_prep = to_categorical( y_series.cat.codes )\n",
    "print( y_prep )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Transfer Learning for Language Models\n",
    "\n",
    "**Transfer learning** - start with better than random initial weights obtained from a previously trainied model that was trained on very big datasets  \n",
    "\n",
    "Some available architectures:  \n",
    "\n",
    "* Word2Vec (Google)\n",
    "    - Continuous Bag of Words\n",
    "    - Skip-gram\n",
    "* FastText (Facebook)\n",
    "    - uses words and n-grams of chars\n",
    "* ELMo\n",
    "    - uses words, embeddings per context\n",
    "    - uses Deep bidirectional language models\n",
    "    \n",
    "Word2Vec and FastText are available on the package `gensim` and ELMo on `tensorflow_hub`\n",
    "\n",
    "example using Word2Vec\n",
    "\n",
    "    from gensim.models import word2vec\n",
    "    # Train the model\n",
    "    w2v_model = word2vec.Word2Vec( tokenized_corpus, size = embedding_dim,\n",
    "                                   window=neighbot_words_num, iter = 100 )\n",
    "    # Get the top 3 similar words to 'captain'\n",
    "    w2v_model.wv.most_similar( ['captain'], topn=3 )\n",
    "    \n",
    "example using FastText\n",
    "\n",
    "    from gensim.models import fasttext\n",
    "    # instantiate the model\n",
    "    ft_model = fasttext.FastText( size = embedding_dim, window=neighbor_words_num )\n",
    "    # build vocabulary\n",
    "    ft_model.build_vocab( sentences=tokenized_corpus,\n",
    "                          total_examples=len(tokenized_corpus),\n",
    "                          epochs=100 )\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class Classification Models\n",
    "\n",
    "Building a multi-class classification model:  \n",
    "\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add( Embedding( 10000, 128 ) )\n",
    "    model.add( LSTM( 128, dropout = 0.2 ) )\n",
    "    # output layer has 'num_classes' units and uses 'softmax'\n",
    "    model.add( Dense( num_classes, activation='softmax' ) )\n",
    "    # compile the model\n",
    "    model.commpile( loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'] )\n",
    "    ...\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news_train = fetch_20newsgroups( subset = 'train' )\n",
    "news_test = fetch_20newsgroups( subset = 'test' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "Usage\n",
      "~~~~~\n",
      "\n",
      "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "fetching / caching functions that downloads the data archive from\n",
      "the original `20 newsgroups website`_, extracts the archive contents\n",
      "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      ":func:`sklearn.datasets.load_files` on either the training or\n",
      "testing set folder, or both of them::\n",
      "\n",
      "  >>> from sklearn.datasets import fetch_20newsgroups\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "  >>> from pprint import pprint\n",
      "  >>> pprint(list(newsgroups_train.target_names))\n",
      "  ['alt.atheism',\n",
      "   'comp.graphics',\n",
      "   'comp.os.ms-windows.misc',\n",
      "   'comp.sys.ibm.pc.hardware',\n",
      "   'comp.sys.mac.hardware',\n",
      "   'comp.windows.x',\n",
      "   'misc.forsale',\n",
      "   'rec.autos',\n",
      "   'rec.motorcycles',\n",
      "   'rec.sport.baseball',\n",
      "   'rec.sport.hockey',\n",
      "   'sci.crypt',\n",
      "   'sci.electronics',\n",
      "   'sci.med',\n",
      "   'sci.space',\n",
      "   'soc.religion.christian',\n",
      "   'talk.politics.guns',\n",
      "   'talk.politics.mideast',\n",
      "   'talk.politics.misc',\n",
      "   'talk.religion.misc']\n",
      "\n",
      "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "attribute is the integer index of the category::\n",
      "\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "It is possible to load only a sub-selection of the categories by passing the\n",
      "list of the categories to load to the\n",
      ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "  >>> cats = ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "  >>> list(newsgroups_train.target_names)\n",
      "  ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      "Converting text to vectors\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "In order to feed predictive or clustering models with the text data,\n",
      "one first need to turn the text into vectors of numerical values suitable\n",
      "for statistical analysis. This can be achieved with the utilities of the\n",
      "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "example that extract `TF-IDF`_ vectors of unigram tokens\n",
      "from a subset of 20news::\n",
      "\n",
      "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "  ...               'comp.graphics', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectorizer = TfidfVectorizer()\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> vectors.shape\n",
      "  (2034, 34118)\n",
      "\n",
      "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "components by sample in a more than 30000-dimensional space\n",
      "(less than .5% non-zero features)::\n",
      "\n",
      "  >>> vectors.nnz / float(vectors.shape[0])       # doctest: +ELLIPSIS\n",
      "  159.01327...\n",
      "\n",
      ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \n",
      "returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
      ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
      "\n",
      "\n",
      "Filtering text for more realistic training\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "It is easy for a classifier to overfit on particular things that appear in the\n",
      "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "high F-scores, but their results would not generalize to other documents that\n",
      "aren't from this window of time.\n",
      "\n",
      "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "which is fast to train and achieves a decent F-score::\n",
      "\n",
      "  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "  >>> from sklearn import metrics\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  # doctest: +ELLIPSIS\n",
      "  0.88213...\n",
      "\n",
      "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "the training and test data, instead of segmenting by time, and in that case\n",
      "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "yet of what's going on inside this classifier?)\n",
      "\n",
      "Let's take a look at what the most informative features are:\n",
      "\n",
      "  >>> import numpy as np\n",
      "  >>> def show_top10(classifier, vectorizer, categories):\n",
      "  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n",
      "  ...     for i, category in enumerate(categories):\n",
      "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "  ...\n",
      "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "  alt.atheism: edu it and in you that is of to the\n",
      "  comp.graphics: edu in graphics it is for and of to the\n",
      "  sci.space: edu it that is in and space to of the\n",
      "  talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "You can now see many things that these features have overfit to:\n",
      "\n",
      "- Almost every group is distinguished by whether headers such as\n",
      "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "- Another significant feature involves whether the sender is affiliated with\n",
      "  a university, as indicated either by their headers or their signature.\n",
      "- The word \"article\" is a significant feature, based on how often people quote\n",
      "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "  wrote:\"\n",
      "- Other features match the names and e-mail addresses of particular people who\n",
      "  were posting at the time.\n",
      "\n",
      "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "barely have to identify topics from text at all, and they all perform at the\n",
      "same high level.\n",
      "\n",
      "For this reason, the functions that load 20 Newsgroups data provide a\n",
      "parameter called **remove**, telling it what kinds of information to strip out\n",
      "of each file. **remove** should be a tuple containing any subset of\n",
      "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "blocks, and quotation blocks respectively.\n",
      "\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  # doctest: +ELLIPSIS\n",
      "  0.77310...\n",
      "\n",
      "This classifier lost over a lot of its F-score, just because we removed\n",
      "metadata that has little to do with topic classification.\n",
      "It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  # doctest: +ELLIPSIS\n",
      "  0.76995...\n",
      "\n",
      "Some other classifiers cope better with this harder version of the task. Try\n",
      "running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n",
      "the ``--filter`` option to compare the results.\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n",
      "  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "  lower because it is more realistic.\n",
      "\n",
      ".. topic:: Examples\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print( news_train.DESCR )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# create and fit the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts( news_train.data )\n",
    "\n",
    "# transform the text to vector representations\n",
    "X_train = tokenizer.texts_to_sequences( news_train.data )\n",
    "X_train = pad_sequences( X_train, maxlen=400 )\n",
    "Y_train = to_categorical( news_train.target )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.fit( X_train, Y_train, batch_size=64, epochs=100 )\n",
    "\n",
    "# evaluate on test data\n",
    "model.evaluate( X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Assessing the Model's Performance\n",
    "\n",
    "Accuracy is not very informative. A Confusion Matrix provides richer information  \n",
    "\n",
    "**Precision** - is the model accurately predicting each class? $\\frac{\\mbox{Correct}_{\\mbox{class}}}{\\mbox{Predicted}_{\\mbox{class}}}$\n",
    "\n",
    "**Recall** - are the classes being correctly classified? $\\frac{\\mbox{Correct}_{\\mbox{class}}}{\\mbox{N}_{\\mbox{class}}}$\n",
    "\n",
    "**F1-score** - a weighted harmonic average between precision and recall.  \n",
    "$$\\mbox{F1 score} = 2\\cdot \\frac{\\mbox{Precision}_{\\mbox{class}} \\cdot \\mbox{Recall}_{\\mbox{class}}}{\\mbox{Precision}_{\\mbox{class}} + \\mbox{Recall}_{\\mbox{class}}}$$\n",
    "\n",
    "\n",
    "using `sklearn`:\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    # build the confusion matrix\n",
    "    confusion_matrix( y_true, y_pred )\n",
    "    \n",
    "other performance metrics:\n",
    "\n",
    "    from sklearn.metrics import precision_score\n",
    "    from sklearn.metrics import recall_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    # add average=None to precision, recall and f1 score functions as follows:\n",
    "    print( precision_score( y_true, y_pred, average=None )\n",
    "    \n",
    "<br>    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
