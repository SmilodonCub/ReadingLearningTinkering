{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks for Language Modeling in `Python`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks and `Keras`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to the Course\n",
    "\n",
    "We will look at four applications of machine learning for text data  \n",
    "\n",
    "* Sentiment Analysis\n",
    "* Multi-class classification\n",
    "* Text generation\n",
    "* Machine neural translation\n",
    "\n",
    "**Recurrent Neural Networks** - reduce the number of parameters by avoiding one-hot encoding  \n",
    "**Sequence to sequence models** - man to one (e.g. for classification); many to many (e.g. text generation)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Language Models\n",
    "\n",
    "Language Models determine sentence probability\n",
    "\n",
    "* Probability of 'I loved this movie'\n",
    "* Unigram $P(\\mbox{sentence}) = P(\\mbox{I})P(\\mbox{loved})P(\\mbox{this})P(\\mbox{movie})$\n",
    "* Bigram $P(\\mbox{sentence})=P(\\mbox{I})P(\\mbox{loved|I})P(\\mbox{this|loved})P(\\mbox{movie|this})$\n",
    "* Trigram $P(\\mbox{sentence})=P(\\mbox{I})P(\\mbox{loved|I})P(\\mbox{this|I loved})P(\\mbox{movie|loved this})$\n",
    "* Skipgram $P(\\mbox{sentence})=P(\\mbox{context or I |I})P(\\mbox{context of loved|loved})P(\\mbox{context of this|this})P(\\mbox{context of movie|movie})$\n",
    "* Neural Network models\n",
    "    - $P(\\mbox{sentence})$ is given by a softmax function on the outer layer of the network\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build vocabulary dictionaries\n",
    "\n",
    "#Get unique words\n",
    "unique_words = list( set( text.split(' ') ) )\n",
    "\n",
    "#Create a dictionary: word is key, index is the value\n",
    "word_to_index = { k:v for (v,k) in enumerate( unique_words ) }\n",
    "\n",
    "#Create a dictionary: index is key, word is value\n",
    "index_to_word = { k:v for (k,v) in enumerate( unique_words ) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing input\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "#Loop over the text: length 'sentence_size' per time with step equal to 'step'\n",
    "for i in range( 0, len( text ) - sentence_size, step ):\n",
    "    X.append( text[ i:i + sentence_size])\n",
    "    y,append( text[ i + sentence_size ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming new texts\n",
    "new_text_split = []\n",
    "#Loop and get the indexes from the dictionary\n",
    "for sentence in new_text:\n",
    "    sent_split = []\n",
    "    for wd in sentence.split(' '):\n",
    "        ix = wd_to_index[ wd ]\n",
    "        sent_split.append( ix )\n",
    "    new_text_split.append( sent_split )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '(3', 1: 'Ah,', 2: \"Amy's\", 3: 'And', 4: 'Explorer', 5: 'Firefox.', 6: 'For', 7: 'Galileo,', 8: 'Goblin', 9: 'Green', 10: 'Hubble', 11: 'I', 12: \"I'm\", 13: 'Internet', 14: 'Ladybugs', 15: 'Oh', 16: 'Paul', 17: 'Penny', 18: 'Penny!', 19: 'Pope', 20: 'Scissors', 21: 'She', 22: 'Spider-Man,', 23: 'Spock', 24: 'Spock,', 25: 'Thankfully', 26: 'The', 27: 'Two', 28: 'V', 29: 'Well,', 30: 'What', 31: 'Wheaton!', 32: 'Wil', 33: \"You're\", 34: 'a', 35: 'afraid', 36: 'all', 37: 'always', 38: 'am', 39: 'and', 40: 'appeals', 41: 'are', 42: 'art', 43: 'as', 44: 'at', 45: 'aware', 46: 'based', 47: 'be', 48: 'became', 49: 'because', 50: 'been', 51: 'birthday', 52: 'bitch.', 53: 'black', 54: 'blood', 55: 'bottle.', 56: 'bottom', 57: 'brain', 58: 'breaker.', 59: 'bus', 60: 'but', 61: 'calls', 62: 'can', 63: 'care', 64: 'catatonic.', 65: 'center', 66: 'chance', 67: 'circuit', 68: 'computer', 69: 'could', 70: 'covers', 71: 'crushes', 72: 'cry', 73: 'cuts', 74: 'days', 75: 'decapitates', 76: 'deity.', 77: 'discovering', 78: 'disproves', 79: 'do', 80: 'does', 81: \"don't\", 82: 'eat', 83: 'eats', 84: 'every', 85: 'example,', 86: 'flashlight', 87: 'for', 88: 'free', 89: 'genitals,', 90: 'genitals.', 91: 'get', 92: 'ghost', 93: 'girlfriend', 94: 'gravity,', 95: 'had', 96: 'hand.', 97: 'has,', 98: 'have', 99: 'have?', 100: 'having', 101: 'heartless', 102: 'here', 103: 'hole', 104: 'humans', 105: 'if', 106: 'impairment;', 107: 'in', 108: 'insane,', 109: 'insects', 110: 'involves', 111: 'is', 112: \"isn't\", 113: 'it', 114: 'it.', 115: 'just', 116: 'kept', 117: 'knocks)', 118: 'later,', 119: 'little', 120: 'living', 121: 'lizard', 122: 'lizard,', 123: 'loud', 124: 'makes', 125: 'man', 126: 'masturbating', 127: 'me', 128: 'memory', 129: 'messy,', 130: 'money.', 131: 'moon-pie', 132: 'mother', 133: 'moved', 134: 'much', 135: 'must', 136: 'my', 137: 'next', 138: 'not', 139: 'nummy-nummy', 140: 'of', 141: 'on', 142: 'one.', 143: 'other', 144: 'others', 145: 'paper', 146: 'paper,', 147: 'people', 148: 'please', 149: 'poisons', 150: 'present', 151: 'prize', 152: 'relationship', 153: 'render', 154: 'reproduce', 155: 'right', 156: 'rock', 157: 'rock,', 158: 'rushed', 159: 'sad.', 160: 'say', 161: 'scissors', 162: 'scissors,', 163: 'scissors.', 164: 'searching', 165: 'sexual', 166: 'she', 167: 'smashes', 168: 'so', 169: 'sooner', 170: 'stopping', 171: 'stupid,', 172: 'taken', 173: 'telescope', 174: 'tested.', 175: 'that', 176: 'the', 177: 'things', 178: 'think', 179: 'thou', 180: 'three', 181: 'to', 182: 'today', 183: 'town.', 184: 'tried', 185: 'unnecessary', 186: 'unsanitary', 187: 'up.', 188: 'used', 189: 'usually', 190: 'vaporizes', 191: 'vodka', 192: 'way', 193: 'we', 194: 'well,', 195: 'which', 196: 'white', 197: 'will', 198: 'with', 199: 'women,', 200: 'would', 201: 'years,', 202: 'you', 203: 'your'}\n",
      "{'(3': 0, 'Ah,': 1, \"Amy's\": 2, 'And': 3, 'Explorer': 4, 'Firefox.': 5, 'For': 6, 'Galileo,': 7, 'Goblin': 8, 'Green': 9, 'Hubble': 10, 'I': 11, \"I'm\": 12, 'Internet': 13, 'Ladybugs': 14, 'Oh': 15, 'Paul': 16, 'Penny': 17, 'Penny!': 18, 'Pope': 19, 'Scissors': 20, 'She': 21, 'Spider-Man,': 22, 'Spock': 23, 'Spock,': 24, 'Thankfully': 25, 'The': 26, 'Two': 27, 'V': 28, 'Well,': 29, 'What': 30, 'Wheaton!': 31, 'Wil': 32, \"You're\": 33, 'a': 34, 'afraid': 35, 'all': 36, 'always': 37, 'am': 38, 'and': 39, 'appeals': 40, 'are': 41, 'art': 42, 'as': 43, 'at': 44, 'aware': 45, 'based': 46, 'be': 47, 'became': 48, 'because': 49, 'been': 50, 'birthday': 51, 'bitch.': 52, 'black': 53, 'blood': 54, 'bottle.': 55, 'bottom': 56, 'brain': 57, 'breaker.': 58, 'bus': 59, 'but': 60, 'calls': 61, 'can': 62, 'care': 63, 'catatonic.': 64, 'center': 65, 'chance': 66, 'circuit': 67, 'computer': 68, 'could': 69, 'covers': 70, 'crushes': 71, 'cry': 72, 'cuts': 73, 'days': 74, 'decapitates': 75, 'deity.': 76, 'discovering': 77, 'disproves': 78, 'do': 79, 'does': 80, \"don't\": 81, 'eat': 82, 'eats': 83, 'every': 84, 'example,': 85, 'flashlight': 86, 'for': 87, 'free': 88, 'genitals,': 89, 'genitals.': 90, 'get': 91, 'ghost': 92, 'girlfriend': 93, 'gravity,': 94, 'had': 95, 'hand.': 96, 'has,': 97, 'have': 98, 'have?': 99, 'having': 100, 'heartless': 101, 'here': 102, 'hole': 103, 'humans': 104, 'if': 105, 'impairment;': 106, 'in': 107, 'insane,': 108, 'insects': 109, 'involves': 110, 'is': 111, \"isn't\": 112, 'it': 113, 'it.': 114, 'just': 115, 'kept': 116, 'knocks)': 117, 'later,': 118, 'little': 119, 'living': 120, 'lizard': 121, 'lizard,': 122, 'loud': 123, 'makes': 124, 'man': 125, 'masturbating': 126, 'me': 127, 'memory': 128, 'messy,': 129, 'money.': 130, 'moon-pie': 131, 'mother': 132, 'moved': 133, 'much': 134, 'must': 135, 'my': 136, 'next': 137, 'not': 138, 'nummy-nummy': 139, 'of': 140, 'on': 141, 'one.': 142, 'other': 143, 'others': 144, 'paper': 145, 'paper,': 146, 'people': 147, 'please': 148, 'poisons': 149, 'present': 150, 'prize': 151, 'relationship': 152, 'render': 153, 'reproduce': 154, 'right': 155, 'rock': 156, 'rock,': 157, 'rushed': 158, 'sad.': 159, 'say': 160, 'scissors': 161, 'scissors,': 162, 'scissors.': 163, 'searching': 164, 'sexual': 165, 'she': 166, 'smashes': 167, 'so': 168, 'sooner': 169, 'stopping': 170, 'stupid,': 171, 'taken': 172, 'telescope': 173, 'tested.': 174, 'that': 175, 'the': 176, 'things': 177, 'think': 178, 'thou': 179, 'three': 180, 'to': 181, 'today': 182, 'town.': 183, 'tried': 184, 'unnecessary': 185, 'unsanitary': 186, 'up.': 187, 'used': 188, 'usually': 189, 'vaporizes': 190, 'vodka': 191, 'way': 192, 'we': 193, 'well,': 194, 'which': 195, 'white': 196, 'will': 197, 'with': 198, 'women,': 199, 'would': 200, 'years,': 201, 'you': 202, 'your': 203}\n"
     ]
    }
   ],
   "source": [
    "#extracted the vocabulary unique_words of the raw texts and created dictionaries to \n",
    "#go from words to numerical indexes and vice versa.\n",
    "\n",
    "sheldon_quotes = [\"You're afraid of insects and women, Ladybugs must render you catatonic.\",\n",
    " 'Scissors cuts paper, paper covers rock, rock crushes lizard, lizard poisons Spock, Spock smashes scissors, scissors decapitates lizard, lizard eats paper, paper disproves Spock, Spock vaporizes rock, and as it always has, rock crushes scissors.',\n",
    " 'For example, I cry because others are stupid, and that makes me sad.',\n",
    " \"I'm not insane, my mother had me tested.\",\n",
    " 'Two days later, Penny moved in and so much blood rushed to your genitals, your brain became a ghost town.',\n",
    " \"Amy's birthday present will be my genitals.\",\n",
    " '(3 knocks) Penny! (3 knocks) Penny! (3 knocks) Penny!',\n",
    " 'Thankfully all the things my girlfriend used to do can be taken care of with my right hand.',\n",
    " 'I would have been here sooner but the bus kept stopping for other people to get on it.',\n",
    " 'Oh gravity, thou art a heartless bitch.',\n",
    " 'I am aware of the way humans usually reproduce which is messy, unsanitary and based on living next to you for three years, involves loud and unnecessary appeals to a deity.',\n",
    " 'Well, today we tried masturbating for money.',\n",
    " 'I think that you have as much of a chance of having a sexual relationship with Penny as the Hubble telescope does of discovering at the center of every black hole is a little man with a flashlight searching for a circuit breaker.',\n",
    " \"Well, well, well, if it isn't Wil Wheaton! The Green Goblin to my Spider-Man, the Pope Paul V to my Galileo, the Internet Explorer to my Firefox.\",\n",
    " \"What computer do you have? And please don't say a white one.\",\n",
    " \"She calls me moon-pie because I'm nummy-nummy and she could just eat me up.\",\n",
    " 'Ah, memory impairment; the free prize at the bottom of every vodka bottle.']\n",
    "\n",
    "# Transform the list of sentences into a list of words\n",
    "all_words = ' '.join(sheldon_quotes).split(' ')\n",
    "\n",
    "# Get number of unique words\n",
    "unique_words = list(set(all_words))\n",
    "\n",
    "# Dictionary of indexes as keys and words as values\n",
    "index_to_word = {i:wd for i, wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(index_to_word)\n",
    "\n",
    "# Dictionary of words as keys and indexes as values\n",
    "word_to_index = {wd:i for i, wd in enumerate(sorted(unique_words))}\n",
    "\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists to keep the sentences and the next character\n",
    "sentences = []   # ~ Training data\n",
    "next_chars = []  # ~ Training labels\n",
    "\n",
    "# Define hyperparameters\n",
    "step = 2          # ~ Step to take when reading the texts in characters\n",
    "chars_window = 10 # ~ Number of characters to use to predict the next one  \n",
    "\n",
    "# Loop over the text: length `chars_window` per time with step equal to `step`\n",
    "for i in range(0, len(sheldon_quotes) - chars_window, step):\n",
    "    sentences.append(sheldon_quotes[i:i + chars_window])\n",
    "    next_chars.append(sheldon_quotes[i + chars_window])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 125, 0, 0, 0, 43, 113, 0, 181, 0, 0, 113, 0, 39, 0, 113, 0, 0, 0, 0, 0, 141, 113, 39, 0, 181, 0, 0]\n",
      "(3 man (3 (3 (3 as it (3 to (3 (3 it (3 and (3 it (3 (3 (3 (3 (3 on it and (3 to (3 (3\n"
     ]
    }
   ],
   "source": [
    "new_text = ['A man either lives life as it happens to him meets it head-on and licks it or he turns his back on it and starts to wither away',\n",
    " 'To the brave crew and passengers of the Kobayshi Maru sucks to be you',\n",
    " 'Beware of more powerful weapons They often inflict as much damage to your soul as they do to you enemies',\n",
    " 'They are merely scars not mortal wounds and you must use them to propel you forward',\n",
    " 'You cannot explain away a wantonly immoral act because you think that it is connected to some higher purpose']\n",
    "\n",
    "# Loop through the sentences and get indexes\n",
    "new_text_split = []\n",
    "for sentence in new_text:\n",
    "    sent_split = []\n",
    "    for wd in sentence.split(' '):\n",
    "        index = word_to_index.get(wd, 0)\n",
    "        sent_split.append(index)\n",
    "    new_text_split.append(sent_split)\n",
    "\n",
    "# Print the first sentence's indexes\n",
    "print(new_text_split[0])\n",
    "\n",
    "# Print the sentence converted using the dictionary\n",
    "print(' '.join([index_to_word[index] for index in new_text_split[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Introduction to RNN inside `Keras`\n",
    "\n",
    "* `keras.models`\n",
    "    - `keras.models.Sequntial` - each layer is input to the following\n",
    "    - `keras.models.Model` - allows for more flexible model architecture\n",
    "* `keras.layers`\n",
    "    - `LSTM`\n",
    "    - `GRU`\n",
    "    - `Dense`\n",
    "    - `Dropout`\n",
    "    - `Embedding`\n",
    "    - `Bidirectional`\n",
    "* `keras.preprocessing`\n",
    "    - `keras.preprocessing.sequence.pad_sequences( text, maxlen=3 )` - make fixed length vectors\n",
    "* `keras.datasets` - IMDB, Reuters & more\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Building the Model\n",
    "\n",
    "    #Instantiate the model class\n",
    "    model = Sequential()\n",
    "\n",
    "    #Add the layers\n",
    "    model.add( Dense( 64, activation='relu', input_dim=100 ) )\n",
    "    model.add( Dense( 1, activation='sigmoid' ) )\n",
    "\n",
    "    #Compile the model\n",
    "    model.compile( optimizer='adam', loss='mean_squared_error', metrics=['accuracy'] )\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "\n",
    "    model.fit( X_train, y_train, epochs = 10, batch_size = 32 )\n",
    "    \n",
    "where:  \n",
    "\n",
    "1. **epochs** - determine how many weight updates will be done on the model\n",
    "2. **batch_size** - size of the data on each step\n",
    "\n",
    "\n",
    "Evaluate the Model\n",
    "\n",
    "    model.evaluate( X_test, y_test )\n",
    "    model.predict( new_data )\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "LSTM (LSTM)                  (None, 128)               71168     \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 71,297\n",
      "Trainable params: 71,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential Model\n",
    "\n",
    "# Instantiate the class\n",
    "model = Sequential()\n",
    "\n",
    "# One LSTM layer (defining the input shape because it is the \n",
    "# initial layer)\n",
    "model.add(LSTM(128, input_shape=(None, 10), name=\"LSTM\"))\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "model.add(Dense(1, activation=\"sigmoid\", name=\"output\"))\n",
    "\n",
    "# The summary shows the layers and the number of parameters \n",
    "# that will be trained\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model model\n",
    "\n",
    "# Define the input layer\n",
    "main_input = Input(shape=(None, 10), name=\"input\")\n",
    "\n",
    "# One LSTM layer (input shape is already defined)\n",
    "lstm_layer = LSTM(128, name=\"LSTM\")(main_input)\n",
    "\n",
    "# Add a dense layer with one unit\n",
    "main_output = Dense(1, activation=\"sigmoid\", name=\"output\")(lstm_layer)\n",
    "\n",
    "# Instantiate the class at the end\n",
    "model = Model(inputs=main_input, outputs=main_output, name=\"modelclass_model\")\n",
    "\n",
    "# Same amount of parameters to train as before (71,297)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it’s unobserved it will, however, if it’s observed after it’s left the plane but before it hits its target, it will not have gone through both slits.',\n",
       "       'Hello, female children. Allow me to inspire you with a story about a great female scientist. Polish-born, French-educated Madame Curie. Co-discoverer of radioactivity, she was a hero of science, until her hair fell out, her vomit and stool became filled with blood, and she was poisoned to death by her own discovery. With a little hard work, I see no reason why that can’t happen to any of you. Are we done? Can we go?'],\n",
       "      dtype='<U419')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "texts = np.array(['So if a photon is directed through a plane with two slits in it and either slit is observed it will not go through both slits. If it’s unobserved it will, however, if it’s observed after it’s left the plane but before it hits its target, it will not have gone through both slits.',\n",
    "       'Hello, female children. Allow me to inspire you with a story about a great female scientist. Polish-born, French-educated Madame Curie. Co-discoverer of radioactivity, she was a hero of science, until her hair fell out, her vomit and stool became filled with blood, and she was poisoned to death by her own discovery. With a little hard work, I see no reason why that can’t happen to any of you. Are we done? Can we go?'],\n",
    "      dtype='<U419')\n",
    "\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the sample texts: (54, 78)\n",
      "Now the texts have fixed length: 60. Let's see the first one: \n",
      "[ 0  0  0  0  0  0 24  4  1 25 13 26  5  1 14  3 27  6 28  2  7 29 30 13\n",
      " 15  2  8 16 17  5 18  6  4  9 31  2  8 32  4  9 15 33  9 34 35 14 36 37\n",
      "  2 38 39 40  2  8 16 41 42  5 18  6]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text\n",
    "\n",
    "# Import relevant classes/functions\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Build the dictionary of indexes\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Change texts into sequence of indexes\n",
    "texts_numeric = tokenizer.texts_to_sequences(texts)\n",
    "print(\"Number of words in the sample texts: ({0}, {1})\".format(len(texts_numeric[0]), len(texts_numeric[1])))\n",
    "\n",
    "# Pad the sequences\n",
    "texts_pad = pad_sequences(texts_numeric, 60)\n",
    "print(\"Now the texts have fixed length: 60. Let's see the first one: \\n{0}\".format(texts_pad[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
