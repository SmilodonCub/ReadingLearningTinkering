{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Natural Language Generation in `Python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Introduction to Sequential Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tfc\n",
    "from keras.layers import SimpleRNN, Dense, Activation, TimeDistributed, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras import backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Handling Sequential Data\n",
    "\n",
    "**sequential data** - any kind of data where the order matters (e.g. text data, time series data, DNA sequences, etc.)  \n",
    "\n",
    "**word delimiters** - specify the start and end of a name using special start and end tokens  \n",
    "**start** - `\\t`  \n",
    "**end** - `\\n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258000\n",
      "['john', 'william', 'james', 'charles', 'george', 'frank', 'joseph', 'thomas', 'henry', 'robert']\n"
     ]
    }
   ],
   "source": [
    "url = 'https://assets.datacamp.com/production/repositories/5286/datasets/45e193467da41ae7631b0d4d626c63d832a34cab/names.txt'\n",
    "names = requests.get( url )\n",
    "names = names.text.split()\n",
    "names = [ name.lower() for name in names ]\n",
    "print( len( names ) )\n",
    "print( names[0:10] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>john</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>william</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>james</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>charles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>george</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name\n",
       "0     john\n",
       "1  william\n",
       "2    james\n",
       "3  charles\n",
       "4   george"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_df = pd.DataFrame( names )\n",
    "names_df.columns = ['name']\n",
    "names_df.head( 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\tjohn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\twilliam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\tjames</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\tcharles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\tgeorge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name\n",
       "0     \\tjohn\n",
       "1  \\twilliam\n",
       "2    \\tjames\n",
       "3  \\tcharles\n",
       "4   \\tgeorge"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start token in front of the name\n",
    "names_df[ 'name' ] = names_df[ 'name' ].apply( lambda x: '\\t' + x )\n",
    "names_df.head( 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\tjohn</td>\n",
       "      <td>john\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\twilliam</td>\n",
       "      <td>william\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\tjames</td>\n",
       "      <td>james\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\tcharles</td>\n",
       "      <td>charles\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\tgeorge</td>\n",
       "      <td>george\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name     target\n",
       "0     \\tjohn     john\\n\n",
       "1  \\twilliam  william\\n\n",
       "2    \\tjames    james\\n\n",
       "3  \\tcharles  charles\\n\n",
       "4   \\tgeorge   george\\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# end token at the end of the name\n",
    "names_df[ 'target' ] = names_df[ 'name' ].apply( lambda x: x[1:len(x)] + '\\n' )\n",
    "names_df.head( 5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Vocabulary** - set of all unique characters used in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary( names ):\n",
    "    \"\"\"\n",
    "    Define vocabulary as a set and include start and end tokens\n",
    "    \"\"\"\n",
    "    vocabulary = set( [ '\\t', '\\n' ] )\n",
    "    #iterate over all names and all characters of each na,e\n",
    "    for name in names:\n",
    "        for c in name:\n",
    "            if c not in vocabulary:\n",
    "                vocabulary.add( c )\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r', 'p', '\\n', 'm', 'u', 'h', 'k', 'e', 'b', 'z', 'g', 'f', '\\t', 'x', 'o', 'y', 'n', 'a', 'j', 'i', 't', 'c', 's', 'q', 'v', 'w', 'd', 'l'}\n",
      "{'\\t': 0, '\\n': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n"
     ]
    }
   ],
   "source": [
    "# Sort the vocabulary and assign numbers in order\n",
    "\n",
    "chars = get_vocabulary( names )\n",
    "print( chars )\n",
    "ctoi = { char: idx for idx, char in enumerate( sorted( chars ) ) }\n",
    "print( ctoi )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\t', 1: '\\n', 2: 'a', 3: 'b', 4: 'c', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'h', 10: 'i', 11: 'j', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'q', 19: 'r', 20: 's', 21: 't', 22: 'u', 23: 'v', 24: 'w', 25: 'x', 26: 'y', 27: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# sort the inverse: and integer to character mapping\n",
    "\n",
    "itoc = { idx : char for idx, char in enumerate( sorted( chars ) ) }\n",
    "print( itoc )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Introduction to Recurrent Neural Network\n",
    "\n",
    "**feedforward neural networks** - accept a fixed size input and return a fixed size output using a fixed nnumber of hidden layers in between.  \n",
    "**recurrent neural networks** - in feedforward NN architecture, the inputs are independent; this is not suitable for sequential data where inputs are reliant on context. Recurrant NNs: the history and the current input are used together to create the output.  \n",
    "\n",
    "RNN for a baby name generator: generate next character given the current. keep track of history so far. continue until the end of the sequence  \n",
    "\n",
    "**Encoding the characters** - a one-hot encoding the length of the number of characters (vocabulary size)  \n",
    "**Number of Time steps** - the length of the longest name. predict each sequence as a name of length `max_len`  \n",
    "**input vector** - initialize as a 3D vector with dims ( num_names, max_lenth +1, length_vocabulary )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# get the length of the longest name\n",
    "\n",
    "def get_max_len( names ):\n",
    "    length_list = [ len( name ) for name in names_df[ 'name' ] ]\n",
    "    max_len = np.max( length_list )\n",
    "    return max_len\n",
    "\n",
    "max_len = get_max_len( names_df )\n",
    "print( max_len )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(258000, 13, 28)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# create a 3D input vector\n",
    "input_data = np.zeros( (len( names_df.name ), max_len+1, len( chars ) ), dtype='float32' )\n",
    "print( input_data.shape )\n",
    "print( input_data[ 0, :, :] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# use the character to integer mappings to fill the input vector\n",
    "\n",
    "for n_idx, name in enumerate( names_df.name ):\n",
    "    for c_idx, char in enumerate( name ):\n",
    "        input_data[ n_idx, c_idx, ctoi[ char ] ] = 1\n",
    "        \n",
    "print( input_data[ 0, :, :] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# initialize and define the target vector\n",
    "\n",
    "target_data = np.zeros( (len( names_df.name ), max_len+1, len( chars ) ), dtype='float32' )\n",
    "\n",
    "for n_idx, name in enumerate( names_df.target ):\n",
    "    for c_idx, char in enumerate( name ):\n",
    "        target_data[ n_idx, c_idx, ctoi[ char ] ] = 1\n",
    "        \n",
    "print( target_data[ 0, :, :] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn (SimpleRNN)       (None, 13, 50)            3950      \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 13, 28)            1428      \n",
      "=================================================================\n",
      "Total params: 5,378\n",
      "Trainable params: 5,378\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build and compile an RNN in Keras\n",
    "model = Sequential()\n",
    "model.add( SimpleRNN( 50, input_shape=( max_len + 1, len( chars ) ),\n",
    "                    return_sequences = True ) )\n",
    "model.add( TimeDistributed( Dense( len( chars), activation = 'softmax' ) ) )\n",
    "model.compile( loss = 'categorical_crossentropy', optimizer = 'adam' )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Inference Using Recurrent Neural Network\n",
    "\n",
    "**Understanding Training**  \n",
    "\n",
    "* NN: a black box\n",
    "* Input target pairs (x,y): ideal output y for input x\n",
    "* the model takes input x $\\rightarrow$ some internal processes $\\rightarrow$ an output z\n",
    "* GOAL: reduce the differences between actual output z and the ideal output y\n",
    "* Training - adjust the internal model parameters to achieve the goal\n",
    "* After training the actual output should be more similar to the ideal output\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "2016/2016 [==============================] - 12s 3ms/step - loss: 1.2404\n",
      "Epoch 2/15\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 1.0316\n",
      "Epoch 3/15\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.9900\n",
      "Epoch 4/15\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.9648\n",
      "Epoch 5/15\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.9478\n",
      "Epoch 6/15\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.9345\n",
      "Epoch 7/15\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.9226\n",
      "Epoch 8/15\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.9146\n",
      "Epoch 9/15\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.9087\n",
      "Epoch 10/15\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.9018\n",
      "Epoch 11/15\n",
      "2016/2016 [==============================] - 7s 3ms/step - loss: 0.8986\n",
      "Epoch 12/15\n",
      "2016/2016 [==============================] - 7s 3ms/step - loss: 0.8947\n",
      "Epoch 13/15\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.8926\n",
      "Epoch 14/15\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.8877\n",
      "Epoch 15/15\n",
      "2016/2016 [==============================] - 6s 3ms/step - loss: 0.8853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f70ec277fd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Traing the RNN\n",
    "\n",
    "model.fit( input_data, target_data, batch_size = 128, epochs = 15 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "where:  \n",
    "\n",
    "* **batch size** - number of sample after which the paramters are adjusted\n",
    "* **epoch** - number of times to iterate over the full dataset\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.32839573e-09 1.01555976e-04 1.98415011e-01 5.17036766e-04\n",
      "  1.87392649e-03 2.21476104e-04 1.45850286e-01 3.36497105e-05\n",
      "  1.88017308e-04 5.56050614e-03 2.24777922e-01 6.55317708e-05\n",
      "  2.03310192e-04 1.04195829e-02 8.83700792e-04 2.87403614e-04\n",
      "  1.65073812e-01 2.42132621e-04 8.02669310e-05 5.23641296e-02\n",
      "  8.23235547e-04 2.45434046e-03 1.61420986e-01 3.11078969e-03\n",
      "  2.56880256e-03 2.16547269e-05 2.18079090e-02 6.33075717e-04]]\n"
     ]
    }
   ],
   "source": [
    "# Predict the first character\n",
    "\n",
    "# initialize the first character of the sequence\n",
    "output_seq = np.zeros( ( 1, max_len+1, len( chars ) ) )\n",
    "output_seq[ 0, 0, ctoi['\\t'] ] = 1\n",
    "\n",
    "# probability distribution for the next character\n",
    "probs = model.predict_proba( output_seq, verbose = 0 )[ :,1,: ]\n",
    "print( probs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first char:  a \n",
      "second char:  l\n"
     ]
    }
   ],
   "source": [
    "# sample the vocabulary to randomly generate a first character\n",
    "first_char = np.random.choice( sorted( list( chars ) ), replace = False, p = probs.reshape( 28 ) )\n",
    "\n",
    "# insert the first character into a sequence\n",
    "output_seq[ 0, 1, ctoi[ first_char ] ] = 1\n",
    "\n",
    "# sample from probability distribution\n",
    "probs = model.predict_proba( output_seq, verbose=0 )[:,1,:]\n",
    "second_char = np.random.choice( sorted( list( chars ) ), replace=False, p = probs.reshape( 28 ) )\n",
    "print( 'first char: ', first_char, '\\nsecond char: ', second_char )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to generate names\n",
    "\n",
    "def generate_names( n ):\n",
    "    for i in range( 0, n ):\n",
    "        stop = False\n",
    "        counter = 1\n",
    "        name = ''\n",
    "        # initialize the fisrt char of the output sequence\n",
    "        output_seq = np.zeros( ( 1, max_len+1, 28 ) )\n",
    "        output_seq[ 0, 0, ctoi[ '\\t' ] ] = 1\n",
    "        # continue until a newline is generated or max number of characters is reached\n",
    "        while stop == False and counter < 10:\n",
    "            # get the prob distribution for the next character\n",
    "            probs = model.predict_proba( output_seq, verbose=0 )[ :,counter-1,: ]\n",
    "            # sample vocabulary to get the most probable next charachter\n",
    "            c = np.random.choice( sorted( list( chars ) ), replace = False, p=probs.reshape( 28 ) )\n",
    "            if c == '\\n':\n",
    "                stop = True\n",
    "            else:\n",
    "                name = name + c\n",
    "                output_seq[ 0, counter, ctoi[c] ] = 1\n",
    "                counter += 1\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randitt\n",
      "judta\n",
      "sod\n",
      "trent\n",
      "annkalvi\n",
      "harronne\n",
      "judre\n",
      "reokieva\n",
      "shawes\n",
      "allyno\n"
     ]
    }
   ],
   "source": [
    "lens = np.random.randint( low = 5, high = 12, size = 10 )\n",
    "\n",
    "for alen in lens:\n",
    "    a_name = generate_names( alen )\n",
    "    print( a_name )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Write Like Shakespeare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Limitations of Recurrent Neural Networks\n",
    "\n",
    "RNNs are not the best for handling long sequences. We will need another approach.  \n",
    "\n",
    "**Simple neural networks** can be thought of as nodes arranged into layers where nodes in different layers are connected by weights. A node/neuron takes in the weights from the previous layer and performs a linear transformation to combine them. Then, a nonlinear 'activation' transformation is applied to create the ouput. In theory, the combination of linear followed by nonlinear transformations makes the network very powerful and it could be able to approximate just about any functions.  \n",
    "\n",
    "**Gradients and Training**  \n",
    "Error: squared difference of the actual output and the predicted output\n",
    "$$E = \\sum e_i = \\sum(y_i-\\hat{y}_i)^2$$\n",
    "Gradient: rate of change of error with respect to the weights\n",
    "$$g_i = \\frac{\\Delta E}{\\Delta w_i} = \\frac{\\partial E}{\\partial w_i}$$\n",
    "training is nothing but adjusting the weights by the gradient fraction to reducing the error\n",
    "$$w_i = w_i-\\eta * \\frac{\\partial E}{\\partial w_i}$$\n",
    "Learning Rate ($\\eta$): factor by which to adjust the weights  \n",
    "\n",
    "Gradients in the output layer can be found by differentiation and other layers by an application of the Chain Rule. Gradients are the product of many gradient values from subsequent time-steps. The gradient that is calculated at the output layer is backpropagated to previous layers where the gradients typically become smaller and smaller than at earlier timepoints in training.  \n",
    "**Vanishing Gradients** - if gradients are very close or equal to zero, then the model will stop learning  \n",
    "**Exploding Gradients** - of gradients become too large and increase, the value will continue to increase with backpropagation.  \n",
    "**Solutions**: use a fixed number of time-steps to avoid vanishing gradients and/or clip gradients to avoid explosion. However, these will result in suboptimal traing and reduce performance.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a sequential model\n",
    "simple_model = Sequential()\n",
    "\n",
    "# Create a dense layer of 12 units\n",
    "simple_model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "# Create a dense layer of 8 units\n",
    "simple_model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "# Create a dense layer of 1 unit\n",
    "simple_model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# Compile the model and get gradients\n",
    "simple_model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "simple_model.summary()\n",
    "\n",
    "inputs = tf.ones((8,8))\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    preds = simple_model( inputs )\n",
    "gradients = tape.gradient(preds, simple_model.trainable_weights)\n",
    "print( gradients )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Introduction to Long Short Term Memeory (LSTM)\n",
    "\n",
    "**Long-term dependencies**  \n",
    "\n",
    "* short-term: The birds are flying in the ___\n",
    "* long-term: I was born in Germany. (many sentences) I can speak ___\n",
    "\n",
    "RNNs are good for short-term memory, but struggle with long term due to vanishing and exploding gradients  \n",
    "LSTM uses an additionaly state to capture longer-term memory  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99993\n",
      "that, poor\n"
     ]
    }
   ],
   "source": [
    "url = 'https://assets.datacamp.com/production/repositories/5286/datasets/2b130693c9bd45c528b60fa9efbf5148a3ff14e5/shakespear.txt'\n",
    "\n",
    "text = requests.get( url )\n",
    "text = text.text.lower()\n",
    "print( len( text ) )\n",
    "print( text[0:10] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = sorted( set( text ) )\n",
    "vocabulary = ['\\n',' ','!',\"'\",',','-','.',':',';','?','a','b','c','d','e','f','g','h','i','j','k','l','m','n',\n",
    "              'o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "char_to_idx = dict( (char,idx) for idx, char in enumerate( vocabulary ) )\n",
    "idx_to_char = dict( (idx,char) for idx, char in enumerate( vocabulary ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of Sequences: 99953\n"
     ]
    }
   ],
   "source": [
    "input_data = []\n",
    "target_data = []\n",
    "maxlen = 40\n",
    "for i in range( 0, len( text ) - maxlen ):\n",
    "    input_data.append( text[i:i+maxlen])\n",
    "    target_data.append(text[i+maxlen])\n",
    "    \n",
    "# Print number of sequences in input data\n",
    "print('No of Sequences:', len(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"that, poor contempt, or claim'd thou sle\", \"hat, poor contempt, or claim'd thou slep\", \"at, poor contempt, or claim'd thou slept\"]\n",
      "['p', 't', ' ']\n"
     ]
    }
   ],
   "source": [
    "print( input_data[0:3])\n",
    "print( target_data[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create input and target vectors\n",
    "x = np.zeros((len(input_data), maxlen, len( vocabulary)), dtype='float32')\n",
    "y = np.zeros((len(target_data), len( vocabulary)), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over the sequences\n",
    "for s_idx, sequence in enumerate( input_data ):\n",
    "    for idx, char in enumerate( sequence ):\n",
    "        x[ s_idx, idx, char_to_idx[ char ] ] = 1\n",
    "    y[ s_idx, char_to_idx[ target_data[s_idx] ] ] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               84480     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 36)                4644      \n",
      "=================================================================\n",
      "Total params: 89,124\n",
      "Trainable params: 89,124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the LSTM network in Keras\n",
    "lstmmod = Sequential()\n",
    "lstmmod.add( LSTM( 128, input_shape=(maxlen, len(vocabulary)) ) )\n",
    "lstmmod.add( Dense( len(vocabulary), activation='softmax' ) )\n",
    "lstmmod.compile( loss='categorical_crossentropy', optimizer='adam' )\n",
    "lstmmod.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Inference Using LSTM\n",
    "\n",
    "How LSTM can be trained and used for prediction  \n",
    "use a validation split to keep samples aside\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1250/1250 [==============================] - 38s 21ms/step - loss: 2.8422 - val_loss: 2.2906\n",
      "Epoch 2/20\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 2.2472 - val_loss: 2.1373\n",
      "Epoch 3/20\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 2.0921 - val_loss: 2.0301\n",
      "Epoch 4/20\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 1.9938 - val_loss: 1.9647\n",
      "Epoch 5/20\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 1.9103 - val_loss: 1.9078\n",
      "Epoch 6/20\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 1.8439 - val_loss: 1.8682\n",
      "Epoch 7/20\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 1.7939 - val_loss: 1.8352\n",
      "Epoch 8/20\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 1.7407 - val_loss: 1.8046\n",
      "Epoch 9/20\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 1.6900 - val_loss: 1.7778\n",
      "Epoch 10/20\n",
      "1250/1250 [==============================] - 24s 20ms/step - loss: 1.6558 - val_loss: 1.7658\n",
      "Epoch 11/20\n",
      "1250/1250 [==============================] - 28s 22ms/step - loss: 1.6173 - val_loss: 1.7516\n",
      "Epoch 12/20\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 1.5822 - val_loss: 1.7442\n",
      "Epoch 13/20\n",
      "1250/1250 [==============================] - 28s 22ms/step - loss: 1.5494 - val_loss: 1.7337\n",
      "Epoch 14/20\n",
      "1250/1250 [==============================] - 28s 22ms/step - loss: 1.5109 - val_loss: 1.7263\n",
      "Epoch 15/20\n",
      "1250/1250 [==============================] - 28s 22ms/step - loss: 1.4860 - val_loss: 1.7271\n",
      "Epoch 16/20\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 1.4636 - val_loss: 1.7309\n",
      "Epoch 17/20\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 1.4387 - val_loss: 1.7272\n",
      "Epoch 18/20\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.4124 - val_loss: 1.7374\n",
      "Epoch 19/20\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 1.3802 - val_loss: 1.7476\n",
      "Epoch 20/20\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 1.3610 - val_loss: 1.7531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f714c6b2610>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the LSTM\n",
    "lstmmod.fit( x, y, batch_size = 64, epochs = 20, validation_split = 0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"that, poor contempt, or claim'd thou sle\"\n",
    "\n",
    "#one hot encode the sentence\n",
    "X_test = np.zeros( ( 1, maxlen, len( vocabulary ) ) )\n",
    "for t, char in enumerate( sentence ):\n",
    "    X_test[ 0, t, char_to_idx[ char ] ] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "# predeict the next character\n",
    "\n",
    "preds = lstmmod.predict( X_test, verbose=0 )\n",
    "prob_next_char = preds[0]\n",
    "next_index = np.argmax( prob_next_char )\n",
    "next_char = idx_to_char[ next_index ]\n",
    "print( next_char )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text( sentence, n ):\n",
    "    generated = sentence\n",
    "    for i in range( n ):\n",
    "        x_pred = np.zeros((1,maxlen,len(vocabulary)))\n",
    "        for t,char in enumerate( sentence ):\n",
    "            x_pred[0,t,char_to_idx[char]]=1.\n",
    "        preds = lstmmod.predict( x_pred,verbose=0)[0]\n",
    "        next_index = np.argmax( preds )\n",
    "        next_char = idx_to_char[ next_index ]\n",
    "        sentence = sentence[1:]+next_char\n",
    "        generated += next_char\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that, poor contempt, or claim'd thou slead tone and the starn\n",
      "as he was stay'd and the starn\n",
      "and so stor out of no soul be with the partions,\n",
      "that he doust of discourse and in the partions the starn\n",
      "as he was stay'd and the starn\n",
      "and so stor out of no soul be with the partions,\n",
      "that he doust of discourse and in the partions the starn\n",
      "as he was stay'd and the starn\n",
      "and so stor out of no soul be with the partions,\n",
      "that he doust of discour\n"
     ]
    }
   ],
   "source": [
    "res = generate_text( sentence, 400 )\n",
    "print( res )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upon our wrongs despised, i will note\n",
      " 'n tasye  ' oirrr eoias noeeeesaa:,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,oaoaaauu eou eii eoee eee eae\n",
      "iatoaa iore\n",
      "ne ieee eai  ireeaaa\n",
      "ne aoeei caei e neioeisoa\n",
      "le ieee eaee eiiau iyeaa\n",
      "ree iat\n",
      " toe le eeiat ieeeereisoaa,oa ne oee eoe\n",
      "ieeee eii eoee eoe\n"
     ]
    }
   ],
   "source": [
    "sent2 = 'upon our wrongs despised, i will not'\n",
    "res = generate_text( sent2, 250 )\n",
    "print( res )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
