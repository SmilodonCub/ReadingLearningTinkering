{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Advanced NLP with `spaCy`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding words, phrases, names and concepts\n",
    "\n",
    "### Intro to `spaCy`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the English language class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# create an nlp object\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "the nlp object contains a processing pipeline, language-specific rules for tokenization\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# a Doc object is created by processing a string of text with the nlp object\n",
    "doc = nlp( \"Hello world!\" )\n",
    "\n",
    "#iterate over tokens in a doc:\n",
    "for token in doc:\n",
    "    print( token.text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n",
      "world\n"
     ]
    }
   ],
   "source": [
    "# index into the Doc to get a single token\n",
    "token =  doc[1]\n",
    "print( token )\n",
    "\n",
    "# get the token text by way of the .text attribute\n",
    "print( token.text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world!\n"
     ]
    }
   ],
   "source": [
    "# Span object: consistes of multiple tokens .. a slice of the Doc object\n",
    "span = doc[1:4]\n",
    "print( span.text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:   [0, 1, 2, 3, 4]\n",
      "Text:   ['It', 'costs', '$', '5', '.']\n",
      "is_alpha   [True, True, False, False, False]\n",
      "is_punct   [False, False, False, False, True]\n",
      "like_num   [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "# lexical attributes\n",
    "doc = nlp( \"It costs $5.\" )\n",
    "\n",
    "print( 'Index:  ', [ token.i for token in doc ] )\n",
    "print( 'Text:  ', [ token.text for token in doc ] )\n",
    "print( 'is_alpha  ', [ token.is_alpha for token in doc ] )\n",
    "print( 'is_punct  ', [ token.is_punct for token in doc ] )\n",
    "print( 'like_num  ', [ token.like_num for token in doc ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liebe Grüße!\n"
     ]
    }
   ],
   "source": [
    "# Import the German language class\n",
    "from spacy.lang.de import German\n",
    "\n",
    "# Create the nlp object\n",
    "nlp = German()\n",
    "\n",
    "# Process a text (this is German for: \"Kind regards!\")\n",
    "doc = nlp(\"Liebe Grüße!\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "# Process the text\n",
    "doc = nlp(\"In 1990, more than 60% of people in East Asia were in extreme poverty. Now less than 4% are.\")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i+1]\n",
    "        # Check if the next token's text equals '%'\n",
    "        if next_token.text == '%':\n",
    "            print('Percentage found:', token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Statistical Models\n",
    "\n",
    "enable `spaCy` to predict linguistic attributes in context  \n",
    "\n",
    "* POS tags\n",
    "* suntactic dependencies\n",
    "* named entities\n",
    "\n",
    "train on labeled example texts and can be updated with more examples to fine-tune predictions  \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# load the small english model\n",
    "nlp = spacy.load( 'en_core_web_sm' )\n",
    "#process the text\n",
    "doc = nlp( 'She ate the pizza' )\n",
    "#iterate over the tokens\n",
    "for token in doc:\n",
    "    print( token.text, token.pos_, token.dep_, token.head.text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "doc = nlp( u\"Apple is looking at buying U.K. startup for $1 billion\" )\n",
    "for ent in doc.ents: \n",
    "    print( ent.text, ent.label_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries, cities, states\n",
      "noun, proper singular\n",
      "direct object\n"
     ]
    }
   ],
   "source": [
    "# for some help\n",
    "print( spacy.explain( 'GPE' ) )\n",
    "print( spacy.explain( 'NNP' ) )\n",
    "print( spacy.explain( 'dobj' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It          PRON      dep       \n",
      "’s          INTJ      intj      \n",
      "official    ADJ       amod      \n",
      ":           PUNCT     punct     \n",
      "Apple       PROPN     nsubj     \n",
      "is          AUX       ROOT      \n",
      "the         DET       det       \n",
      "first       ADJ       amod      \n",
      "U.S.        PROPN     nmod      \n",
      "public      ADJ       amod      \n",
      "company     NOUN      attr      \n",
      "to          PART      aux       \n",
      "reach       VERB      relcl     \n",
      "a           DET       det       \n",
      "$           SYM       quantmod  \n",
      "1           NUM       compound  \n",
      "trillion    NUM       nummod    \n",
      "market      NOUN      compound  \n",
      "value       NOUN      dobj      \n"
     ]
    }
   ],
   "source": [
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp( text)\n",
    "\n",
    "for token in doc:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # This is for formatting only\n",
    "    print('{:<12}{:<10}{:<10}'.format(token_text, token_pos, token_dep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "first ORDINAL\n",
      "U.S. GPE\n",
      "$1 trillion MONEY\n"
     ]
    }
   ],
   "source": [
    "text = \"It’s official: Apple is the first U.S. public company to reach a $1 trillion market value\"\n",
    "\n",
    "# Process the text\n",
    "doc = nlp( text )\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Rule-based Matching\n",
    "\n",
    "**match patterns** - list of dictionaries, one per token\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9528407286733565721, 1, 3)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "nlp = spacy.load( 'en_core_web_sm' )\n",
    "matcher = Matcher( nlp.vocab)\n",
    "#add pattern to the matcher\n",
    "pattern = [ { 'ORTH':'iPhone' }, { 'ORTH':'X' } ]\n",
    "matcher.add( 'IPHONE_PATTERN', [ pattern ] )\n",
    "#return matches on a doc\n",
    "doc = nlp( 'New iPhone X release date leaked' )\n",
    "matches = matcher( doc )\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    # iterate over and matches and create a span object\n",
    "    matched_span = doc[ start:end ]\n",
    "    print( matched_span.text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(17311505950452258848, 0, 5)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lexical matches\n",
    "pattern = [\n",
    "    { 'IS_DIGIT': True },\n",
    "    { 'LOWER':'fifa' },\n",
    "    { 'LOWER':'world' },\n",
    "    { 'LOWER':'cup' },\n",
    "    { 'IS_PUNCT': True }\n",
    "]\n",
    "\n",
    "doc = nlp( '2018 FIFA World Cup: France won!')\n",
    "matcher.add( 'FIFA_PATTERN', [pattern] )\n",
    "matches = matcher( doc )\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(18437031736592595799, 1, 3), (18437031736592595799, 6, 8)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = [\n",
    "    { 'LEMMA': 'love', 'POS': 'VERB' },\n",
    "    { 'POS': 'NOUN' }\n",
    "]\n",
    "\n",
    "doc = nlp( 'I loved dogs but now I love cats more' )\n",
    "matcher.add( 'LOVE', [pattern] )\n",
    "matches = matcher( doc )\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Using operators and quantifiers\n",
    "\n",
    "| Operator |    Description   |\n",
    "|:-----------:|:----------------------------:|\n",
    "| {'OP': '!'} |    Negation: match 0 times   |\n",
    "| {'OP': '?'} | Optional: match 0 or 1 times |\n",
    "| {'OP': '+'} | Match 1 or more times        |\n",
    "| {'OP': '*'} | Match 0 or more times        |\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: iOS 7\n",
      "Match found: iOS 11\n",
      "Match found: iOS 10\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"After making the iOS update you won't notice a radical system-wide redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of iOS 11's furniture remains the same as in iOS 10. But you will discover some tweaks once you delve a little deeper.\")\n",
    "\n",
    "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{'TEXT': 'iOS'}, {'IS_DIGIT': True}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('IOS_VERSION_PATTERN', [pattern])\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: downloaded Fortnite\n",
      "Match found: downloading Minecraft\n",
      "Match found: download Winzip\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"i downloaded Fortnite on my laptop and can't open the game at all. Help? so when I was downloading Minecraft, I got the Windows version where it is the '.zip' folder and I used the default program to unpack it... do I also need to download Winzip?\")\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{'LEMMA': 'download'}, {'POS': 'PROPN'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('DOWNLOAD_THINGS_PATTERN', [pattern] )\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 5\n",
      "Match found: beautiful design\n",
      "Match found: smart search\n",
      "Match found: automatic labels\n",
      "Match found: optional voice\n",
      "Match found: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Features of the app include a beautiful design, smart search, automatic labels and optional voice responses.\")\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "pattern = [{'POS': 'ADJ'}, {'POS': 'NOUN'}, {'POS': 'NOUN', 'OP': '?'}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add('ADJ_NOUN_PATTERN', [pattern])\n",
    "matches = matcher(doc)\n",
    "print('Total matches found:', len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print('Match found:', doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Large-sclae Data Analysis with `spaCy`\n",
    "\n",
    "### Data Structures\n",
    "\n",
    "**`Vocab`** - stores data shared across multiple doccuments  \n",
    "encodes strings as **hash values**  \n",
    "strings are only stored once in the `StringStore` via `nlp.vocab.strings`  \n",
    "**`StringStore`** - is a bidirectional lookup table  \n",
    "\n",
    "    coffee_hash = nlp.vocab.strings['coffee']\n",
    "    coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "    \n",
    "However, hashes cannot be reversed (that's why we need to provide the shared vocab)  \n",
    "\n",
    "    # This will result in an error:\n",
    "    string = nlp.vocab.strings[3197928453018144401]\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value:  3197928453018144401\n",
      "string values:  coffee\n"
     ]
    }
   ],
   "source": [
    "doc = nlp( 'I love coffee' )\n",
    "print( 'hash value: ', nlp.vocab.strings['coffee'] )\n",
    "print( 'string values: ', nlp.vocab.strings[3197928453018144401] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value:  3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "print( 'hash value: ', doc.vocab.strings['coffee'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Lexemes** - entries in the vocabulary.  \n",
    "\n",
    "`Lexeme` objects are entries in the `Vocab` and contain context-independent information about a word  \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "lexeme = nlp.vocab['coffee']\n",
    "print( lexeme.text, lexeme.orth, lexeme.is_alpha )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Data Structures: Doc, Span and Token\n",
    "\n",
    "Best Practices:\n",
    "\n",
    "* `Doc` and `Span` are very powerful and hold references and relationships of words and sentences\n",
    "    * Convert strings as late as possible\n",
    "    * Use token attributes is available (e.g. `token.i` as index)\n",
    "* Don't forget to pass the shared `vocab`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello world!"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Doc Object\n",
    "\n",
    "# create an nlp object\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "#import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "words = ['Hello', 'world', '!']\n",
    "spaces = [True, False, False]\n",
    "#create a doc object manually\n",
    "doc = Doc( nlp.vocab, words=words, spaces=spaces )\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is cool!\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Desired text: \"spaCy is cool!\"\n",
    "words = ['spaCy', 'is', 'cool', '!']\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Create a Doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n",
      "Hello world\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Hello world!"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a span is a sclice of a doc object\n",
    "from spacy.tokens import Span\n",
    "# ccreate a span manually\n",
    "span = Span( doc, 0, 2 )\n",
    "print( span )\n",
    "#create a span with a label\n",
    "lspan = Span( doc, 0, 2, label = 'GREETING' )\n",
    "print( lspan )\n",
    "#add a span to a doc's entities\n",
    "doc.ents = [lspan]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# Create a doc from the words and spaces\n",
    "doc = Doc(nlp.vocab, words=['I', 'like', 'David', 'Bowie'], spaces=[True, True, True, False])\n",
    "\n",
    "# Create a span for \"David Bowie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label='PERSON')\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Word Vectors nd Semantic Similarity\n",
    "\n",
    "Comparing Semantic Similarity:  \n",
    "\n",
    "* `spaCy` can compare similarity between objects\n",
    "    - Doc.similarity()\n",
    "    - Span.similarity()\n",
    "    - Token.similarity()\n",
    "* returns a similarity score between 0 and 1\n",
    "* **Necessary** - a larger word model which has word vectors included:\n",
    "    - `en_core_web_md` medium english model\n",
    "    - `en_core_web_lg` large english model\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I needed to run this:  \n",
    "\n",
    "    pip3 install spacy\n",
    "    python3 -m spacy download en_core_web_sm\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8627204117787385\n",
      "0.644411562288608\n"
     ]
    }
   ],
   "source": [
    "# load one of the larger english models\n",
    "# comparing documents\n",
    "nlp = spacy.load( 'en_core_web_md' )\n",
    "doc1 = nlp( 'I like fast food' )\n",
    "doc2 = nlp( 'I like pizza' )\n",
    "doc3 = nlp( 'The dog is asleep on the couch' )\n",
    "print( doc1.similarity( doc2 ) )\n",
    "print( doc1.similarity( doc3 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73695457\n",
      "0.110321715\n"
     ]
    }
   ],
   "source": [
    "# comparing tokens\n",
    "doc = nlp( 'Vultures love pizza and pasta' )\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "token3 = doc[0]\n",
    "print( token1.similarity(token2) )\n",
    "print( token1.similarity(token3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31157307324660816\n"
     ]
    }
   ],
   "source": [
    "# comparing documents & tokens\n",
    "doc = nlp( 'I like science fiction' )\n",
    "token = nlp( 'soap' )[0]\n",
    "print( doc.similarity(token) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6199092090831612\n",
      "0.7486420021584955\n"
     ]
    }
   ],
   "source": [
    "# comparing docs & spans\n",
    "span = nlp( 'I like pizza and pasta' )[2:6]\n",
    "doc = nlp( 'McDonals sells burgers' )\n",
    "doc2 = nlp( 'Luigis sells pasta' )\n",
    "print( span.similarity( doc ) )\n",
    "print( span.similarity( doc2 ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "But how does `spaCy` predict similarity?\n",
    "\n",
    "* similarity is determined using **word vectors**\n",
    "* **word vectors** - multi-dimentional representations of word meaning\n",
    "* generated using algorithms like [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) and lots of text for modeling\n",
    "* new words can be added to `spaCy`'s statistical models\n",
    "* by default, similarity is given as cosine similarity, but there are other measures\n",
    "* for multi-token objects (`Doc`s and `Span`s) the vectors default to the average of the individual token vectors. As a result, short phrases are better than long documents which regress to a mean\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "[ 0.035414 -0.4573    0.42617   0.23448   0.18446   0.78676   0.15513\n",
      " -0.41701   0.36996  -0.25015 ]\n"
     ]
    }
   ],
   "source": [
    "# examine a word vector\n",
    "doc = nlp( 'I am learning to code python' )\n",
    "# access the vector via the token.vector attribute\n",
    "print( len( doc[ 5 ].vector ) )\n",
    "print( doc[ 5 ].vector[0:10] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Similarity Depends on Context.  \n",
    "Similarity measures can be very useful in some NLP tasks: recommendation systems to suggest related content, flagging duplicate posts on social media platforms  \n",
    "However, there is no objective definition of similarity, so one measure doesnt fit for every task.  \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Models & Rules  \n",
    "\n",
    "|                         |                   **Statistical Models**                   | **Rule-based Systems**                                 |\n",
    "|:-----------------------:|:----------------------------------------------------------:|--------------------------------------------------------|\n",
    "| **Use Cases**           | application needs to generalize based on examples          | dictionary with finite number of examples              |\n",
    "| **Real World Examples** | product names, person names, subject/object relationships  | countries of the world, cities, drug names, dog breeds |\n",
    "| **spaCy Features**      | entity reconizer, dependency parser, part-of-speech tagger | tokenizer, Matcher,  PhraseMatcher                     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recap: rule-based matching\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher( nlp.vocab )\n",
    "# patterns are lists of dictionaries describing the tokens\n",
    "patter = [{'LEMMA': 'love', 'POS':'VERB'}, {'LOWER':'cats'}]\n",
    "matcher.add( 'LOVE_CATS', [pattern] )\n",
    "# operators can spcify how often a token should be matched\n",
    "patter = [{'LOWER': 'very', 'OP':'+'}, {'LOWER':'happy'}]\n",
    "doc = nlp( \"I love cats and I'm very very happy\" )\n",
    "matches = matcher( doc )\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span:  Golden Retriever\n",
      "Root token:  Retriever\n",
      "Root head token:  have\n",
      "Previous token:  a DET\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher( nlp.vocab )\n",
    "matcher.add( 'DOG', [[{'LOWER':'golden'},{'LOWER':'retriever'}]])\n",
    "doc = nlp( 'I have a Golden Retriever' )\n",
    "for match_id, start, end in matcher( doc ):\n",
    "    span = doc[start:end]\n",
    "    print( 'Matched span: ', span.text )\n",
    "    print( 'Root token: ', span.root.text )\n",
    "    print( 'Root head token: ', span.root.head.text )\n",
    "    print( 'Previous token: ', doc[ start-1 ].text, doc[ start-1 ].pos_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN1 Amazon Prime\n",
      "PATTERN2 ad-free viewing\n",
      "PATTERN2 ad-free viewing\n"
     ]
    }
   ],
   "source": [
    "message = \"\"\"Twitch Prime, the perks program for Amazon Prime members offering free loot, games and other benefits, \n",
    "is ditching one of its best features: ad-free viewing. According to an email sent out to Amazon Prime members \n",
    "today, ad-free viewing will no longer be included as a part of Twitch Prime for new members, beginning on \n",
    "September 14. However, members with existing annual subscriptions will be able to continue to enjoy ad-free \n",
    "viewing until their subscription comes up for renewal. Those with monthly subscriptions will have access to \n",
    "ad-free viewing until October 15.\"\"\"\n",
    "\n",
    "doc = nlp( message )\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{'LOWER': 'amazon'}, {'IS_TITLE': True, 'POS': 'PROPN'}]\n",
    "pattern2 = [{\"TEXT\": \"ad\"},{\"TEXT\": \"-\"},{\"TEXT\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('PATTERN1', [pattern1])\n",
    "matcher.add('PATTERN2', [pattern2])\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Phrase Matching with `Phrasematcher`:  \n",
    "\n",
    "* similar to regex or keyword searches but for use with tokens\n",
    "* Use a `Doc` object as a pattern\n",
    "* good for matching against large word lists. is fast & more efficient than `Matcher`\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matcched span:  Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher( nlp.vocab )\n",
    "pattern = nlp( \"Golden Retriever\" )\n",
    "matcher.add( 'DOG', [pattern] )\n",
    "\n",
    "doc = nlp( 'I have a Golden Retriever' )\n",
    "\n",
    "#iterate over the matches\n",
    "\n",
    "for match_id, start, end in matcher( doc ):\n",
    "    span = doc[ start:end ]\n",
    "    print( 'Matched span: ', span.text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Czech Republic, Slovakia]\n"
     ]
    }
   ],
   "source": [
    "text = 'Czech Republic may help Slovakia protect its airspace'\n",
    "doc = nlp( text )\n",
    "\n",
    "COUNTRIES = ['Afghanistan','Åland Islands','Albania','Algeria','American Samoa','Andorra','Angola','Anguilla',\n",
    " 'Antarctica','Antigua and Barbuda','Argentina','Armenia','Aruba','Australia','Austria','Azerbaijan','Bahamas',\n",
    " 'Bahrain','Bangladesh','Barbados','Belarus','Belgium','Belize','Benin','Bermuda','Bhutan','Bolivia (Plurinational State of)',\n",
    " 'Bonaire, Sint Eustatius and Saba','Bosnia and Herzegovina','Botswana','Bouvet Island','Brazil','British Indian Ocean Territory',\n",
    " 'United States Minor Outlying Islands','Virgin Islands (British)','Virgin Islands (U.S.)','Brunei Darussalam',\n",
    " 'Bulgaria','Burkina Faso','Burundi','Cambodia','Cameroon','Canada','Cabo Verde','Cayman Islands','Central African Republic',\n",
    " 'Chad','Chile','China','Christmas Island','Cocos (Keeling) Islands','Colombia','Comoros','Congo','Congo (Democratic Republic of the)',\n",
    " 'Cook Islands','Costa Rica','Croatia','Cuba','Curaçao','Cyprus','Czech Republic','Denmark','Djibouti','Dominica',\n",
    " 'Dominican Republic','Ecuador','Egypt','El Salvador','Equatorial Guinea','Eritrea','Estonia','Ethiopia','Falkland Islands (Malvinas)',\n",
    " 'Faroe Islands','Fiji','Finland','France','French Guiana','French Polynesia','French Southern Territories','Gabon',\n",
    " 'Gambia','Georgia','Germany','Ghana','Gibraltar','Greece','Greenland','Grenada','Guadeloupe','Guam','Guatemala',\n",
    " 'Guernsey','Guinea','Guinea-Bissau','Guyana','Haiti','Heard Island and McDonald Islands','Holy See','Honduras',\n",
    " 'Hong Kong','Hungary','Iceland','India','Indonesia',\"Côte d'Ivoire\",'Iran (Islamic Republic of)','Iraq','Ireland',\n",
    " 'Isle of Man','Israel','Italy','Jamaica','Japan','Jersey','Jordan','Kazakhstan','Kenya','Kiribati','Kuwait','Kyrgyzstan',\n",
    " \"Lao People's Democratic Republic\",'Latvia','Lebanon','Lesotho','Liberia','Libya','Liechtenstein','Lithuania',\n",
    " 'Luxembourg','Macao','Macedonia (the former Yugoslav Republic of)','Madagascar','Malawi','Malaysia','Maldives',\n",
    " 'Mali','Malta','Marshall Islands','Martinique','Mauritania','Mauritius','Mayotte','Mexico','Micronesia (Federated States of)',\n",
    " 'Moldova (Republic of)','Monaco','Mongolia','Montenegro','Montserrat','Morocco','Mozambique','Myanmar','Namibia',\n",
    " 'Nauru','Nepal','Netherlands','New Caledonia','New Zealand','Nicaragua','Niger','Nigeria','Niue','Norfolk Island',\n",
    " \"Korea (Democratic People's Republic of)\",'Northern Mariana Islands','Norway','Oman','Pakistan','Palau','Palestine, State of',\n",
    " 'Panama','Papua New Guinea','Paraguay','Peru','Philippines','Pitcairn','Poland','Portugal','Puerto Rico','Qatar',\n",
    " 'Republic of Kosovo','Réunion','Romania','Russian Federation','Rwanda','Saint Barthélemy','Saint Helena, Ascension and Tristan da Cunha',\n",
    " 'Saint Kitts and Nevis','Saint Lucia','Saint Martin (French part)','Saint Pierre and Miquelon','Saint Vincent and the Grenadines',\n",
    " 'Samoa','San Marino','Sao Tome and Principe','Saudi Arabia','Senegal','Serbia','Seychelles','Sierra Leone','Singapore',\n",
    " 'Sint Maarten (Dutch part)','Slovakia','Slovenia','Solomon Islands','Somalia','South Africa','South Georgia and the South Sandwich Islands',\n",
    " 'Korea (Republic of)','South Sudan','Spain','Sri Lanka','Sudan','Suriname','Svalbard and Jan Mayen','Swaziland',\n",
    " 'Sweden','Switzerland','Syrian Arab Republic','Taiwan','Tajikistan','Tanzania, United Republic of','Thailand','Timor-Leste',\n",
    " 'Togo','Tokelau','Tonga','Trinidad and Tobago','Tunisia','Turkey','Turkmenistan','Turks and Caicos Islands','Tuvalu',\n",
    " 'Uganda','Ukraine','United Arab Emirates','United Kingdom of Great Britain and Northern Ireland',\n",
    " 'United States of America','Uruguay','Uzbekistan','Vanuatu','Venezuela (Bolivarian Republic of)','Viet Nam',\n",
    " 'Wallis and Futuna','Western Sahara','Yemen','Zambia','Zimbabwe']\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher( nlp.vocab )\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add('COUNTRY', None, *patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namibia\n",
      "South Africa\n",
      "Cambodia\n",
      "Kuwait\n",
      "Somalia\n",
      "Haiti\n",
      "Mozambique\n",
      "Somalia\n",
      "Rwanda\n",
      "Singapore\n",
      "Afghanistan\n",
      "Iraq\n",
      "Sudan\n",
      "Congo\n",
      "Haiti\n",
      "in --> Namibia\n",
      "in --> South Africa\n",
      "Africa --> Cambodia\n",
      "of --> Kuwait\n",
      "as --> Somalia\n",
      "Somalia --> Haiti\n",
      "Haiti --> Mozambique\n",
      "in --> Somalia\n",
      "for --> Rwanda\n",
      "Britain --> Singapore\n",
      "of --> Afghanistan\n",
      "invaded --> Iraq\n",
      "in --> Sudan\n",
      "of --> Congo\n",
      "earthquake --> Haiti\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"After the Cold War, the UN saw a radical expansion in its peacekeeping duties, taking on more missions in \n",
    "ten years than it had in the previous four decades.Between 1988 and 2000, the number of adopted Security Council \n",
    "resolutions more than doubled, and the peacekeeping budget increased more than tenfold. The UN negotiated an end \n",
    "to the Salvadoran Civil War, launched a successful peacekeeping mission in Namibia, and oversaw democratic \n",
    "elections in post-apartheid South Africa and post-Khmer Rouge Cambodia. In 1991, the UN authorized a US-led \n",
    "coalition that repulsed the Iraqi invasion of Kuwait. Brian Urquhart, Under-Secretary-General from 1971 to 1985, \n",
    "later described the hopes raised by these successes as a \"false renaissance\" for the organization, given the more \n",
    "troubled missions that followed. Though the UN Charter had been written primarily to prevent aggression by one \n",
    "nation against another, in the early 1990s the UN faced a number of simultaneous, serious crises within nations \n",
    "such as Somalia, Haiti, Mozambique, and the former Yugoslavia. The UN mission in Somalia was widely \n",
    "viewed as a failure after the US withdrawal following casualties in the Battle of Mogadishu, and the UN mission \n",
    "to Bosnia faced \"worldwide ridicule\" for its indecisive and confused mission in the face of ethnic cleansing. \n",
    "In 1994, the UN Assistance Mission for Rwanda failed to intervene in the Rwandan genocide amid indecision in \n",
    "the Security Council. Beginning in the last decades of the Cold War, American and European critics of the UN \n",
    "condemned the organization for perceived mismanagement and corruption. In 1984, the US President, Ronald Reagan, \n",
    "withdrew his nation\\'s funding from UNESCO (the United Nations Educational, Scientific and Cultural Organization, \n",
    "founded 1946) over allegations of mismanagement, followed by Britain and Singapore. Boutros Boutros-Ghali, \n",
    "Secretary-General from 1992 to 1996, initiated a reform of the Secretariat, reducing the size of the organization \n",
    "somewhat. His successor, Kofi Annan (1997–2006), initiated further management \n",
    "reforms in the face of threats from the United States to withhold its UN dues. In the late 1990s and 2000s, \n",
    "international interventions authorized by the UN took a wider variety of forms. The UN mission in the Sierra \n",
    "Leone Civil War of 1991–2002 was supplemented by British Royal Marines, and the invasion of Afghanistan in 2001 \n",
    "was overseen by NATO. In 2003, the United States invaded Iraq despite failing to pass a UN Security Council \n",
    "resolution for authorization, prompting a new round of questioning of the organization\\'s effectiveness. Under \n",
    "the eighth Secretary-General, Ban Ki-moon, the UN has intervened with peacekeepers in crises including the War in \n",
    "Darfur in Sudan and the Kivu conflict in the Democratic Republic of Congo and sent observers and chemical weapons\n",
    "inspectors to the Syrian Civil War. In 2013, an internal review of UN actions in the final battles of the Sri \n",
    "Lankan Civil War in 2009 concluded that the organization had suffered \"systemic failure\". One hundred and one \n",
    "UN personnel died in the 2010 Haiti earthquake, the worst loss of life in the organization\\'s history. The \n",
    "Millennium Summit was held in 2000 to discuss the UN\\'s role in the 21st century. The three day meeting was the \n",
    "largest gathering of world leaders in history, and culminated in the adoption by all member states of the \n",
    "Millennium Development Goals (MDGs), a commitment to achieve international development in areas such as poverty \n",
    "reduction, gender equality, and public health. Progress towards these goals, which were to be met by 2015, was \n",
    "ultimately uneven. The 2005 World Summit reaffirmed the UN\\'s focus on promoting development, peacekeeping, \n",
    "human rights, and global security. The Sustainable Development Goals were launched in 2015 to succeed the \n",
    "Millennium Development Goals. In addition to addressing global challenges, the UN has sought to improve its \n",
    "accountability and democratic legitimacy by engaging more with civil society and fostering a global constituency. \n",
    "In an effort to enhance transparency, in 2016 the organization held its first public \n",
    "debate between candidates for Secretary-General. On 1 January 2017, Portuguese diplomat António Guterres, who \n",
    "previously served as UN High Commissioner for Refugees, became the ninth Secretary-General. Guterres has \n",
    "highlighted several key goals for his administration, including an emphasis on diplomacy for preventing conflicts, \n",
    "more effective peacekeeping efforts, and streamlining the organization to be more responsive and versatile to \n",
    "global needs.\"\"\"\n",
    "\n",
    "# Create a doc and find matches in it\n",
    "doc = nlp( text )\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "    print( span.text )\n",
    "\n",
    "    # Overwrite the doc.ents and add the span\n",
    "    #doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "# Print the entities in the document\n",
    "#print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == 'GPE'])\n",
    "\n",
    "# Create a doc and find matches in it\n",
    "doc = nlp(text)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\" and overwrite the doc.ents\n",
    "    span = Span(doc, start, end, label='GPE')\n",
    "    #doc.ents = list(doc.ents) + [span]\n",
    "    \n",
    "    # Get the span's root head token\n",
    "    span_root_head = span.root.head\n",
    "    # Print the text of the span root's head token and the span text\n",
    "    print(span_root_head.text, '-->', span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## `spaCy`'s Processing Pipeline\n",
    "\n",
    "### Processing Pipelines\n",
    "\n",
    "**pipeline** - a series of functions applied to a `Doc` to add attributes  \n",
    "ex: calling `nlp()`. this pipeline takes text as input $\\rightarrow$ tokenizer $\\rightarrow$ tagger $\\rightarrow$ parser $\\rightarrow$ ner $\\rightarrow$ $\\rightarrow$ $\\rightarrow$ and returns a `Doc` object  \n",
    "The tokenizer turns a string into a `Doc` object. `spaCy` then applies every component in the pipeline on the `Doc`, in order.\n",
    "\n",
    "\n",
    "|   **Name**  |     **Description**     | **Creates**                                               |\n",
    "|:-----------:|:-----------------------:|-----------------------------------------------------------|\n",
    "| **tagger**  | Part-of-speach tagger   | `Token.tag`                                               |\n",
    "| **parser**  | Dependency parser       | `Token.dep`, `Token.head`, `Doc.sents`, `Doc.noun_chunks` |\n",
    "| **ner**     | Named entity recognizer | `Doc.ents`, `Token.ent_iob`, `Token.ent_type`             |\n",
    "| **textcat** | Text classifier         | `Doc.cats`                                                |\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x7f52a0b917d0>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x7f52a22b4170>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x7f529e3ff830>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x7f529fa26960>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer object at 0x7f529e596fa0>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x7f52a22a4e50>)]\n"
     ]
    }
   ],
   "source": [
    "# use the nlp.pipe_names for a list of pipeline component names\n",
    "print( nlp.pipe_names )\n",
    "\n",
    "# for a list of component names and component function tuples\n",
    "print( nlp.pipeline )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Custom Pipeline Components\n",
    "\n",
    "**custom pipeline components** - let you add your ownfunction to the `spaCy` pipeline that is executes when you call `nlp()` on some text  \n",
    "\n",
    "* components are functions that take a `doc`, modify it and return it\n",
    "* can be added using the `nlp.add_pipe` method  \n",
    "\n",
    "for exmple:\n",
    "\n",
    "    def custom_component( doc ):\n",
    "        # do something to the doc here\n",
    "        return doc\n",
    "    nlp.add_pipe( custom_component )\n",
    "    \n",
    "    \n",
    "| **Argument** |    **Description**   | **Example**                                |\n",
    "|:------------:|:--------------------:|--------------------------------------------|\n",
    "| **`last`**   | If `True`, add last  | `nlp.add_pipe( component, last=True )`     |\n",
    "| **`first`**  | If `True`, add first | `nlp.add_pipe( component, first=True )`    |\n",
    "| **`before`** | Add before component | `nlp.add_pipe( component, before='ner')`   |\n",
    "| **`after`**  | Add after component  | `nlp.add_pipe( component, after='tagger')` |\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline:  ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'print_info']\n"
     ]
    }
   ],
   "source": [
    "from spacy.language import Language\n",
    "# create an nlp object\n",
    "nlp = spacy.load( 'en_core_web_sm' )\n",
    "# define a custom component\n",
    "@Language.component(\"info_component\")\n",
    "def custom_component( doc ):\n",
    "    # print the docs length\n",
    "    print( 'Doc length: ', len( doc ) )\n",
    "    # return the doc object\n",
    "    return doc\n",
    "# add the component first in the pipeline\n",
    "nlp.add_pipe(\"info_component\", name=\"print_info\", last=True)\n",
    "# print the pipeline component names\n",
    "print( 'Pipeline: ', nlp.pipe_names )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc length:  3\n"
     ]
    }
   ],
   "source": [
    "# process some text\n",
    "doc = nlp( 'I like coffee' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline:  ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "[('a Golden Retriever', 'FAC')]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load( 'en_core_web_sm' )\n",
    "# Define the custom component\n",
    "@Language.component(\"info_component\")\n",
    "def animal_component(doc):\n",
    "    # Create a Span for each match and assign the label 'ANIMAL'\n",
    "    # and overwrite the doc.ents with the matched spans\n",
    "    doc.ents = [Span(doc, start, end, label='ANIMAL')\n",
    "                for match_id, start, end in matcher(doc)]\n",
    "    return doc\n",
    "    \n",
    "# Add the component to the pipeline after the 'ner' component \n",
    "#nlp.add_pipe(\"info_component\", name = \"animal_component\", after='ner')\n",
    "print( 'Pipeline: ', nlp.pipe_names )\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(a Golden Retriever,)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Setting Custon Attributes\n",
    "\n",
    "* add custom metadata to documents, tokens and spans\n",
    "* accessible via the `._` property\n",
    "* registered on the global `Doc`, `Token`, or `Span` using the `set_extension` method\n",
    "\n",
    "examples:  \n",
    "\n",
    "    doc._.title = 'My document'\n",
    "    token._.ir_color = True\n",
    "    span._.has_color = False\n",
    "\n",
    "Extension Attribute Types:  \n",
    "\n",
    "1. Attribute extensions - set a default value that can be overwritten\n",
    "2. Property extensions - define a getter and an optional setter function\n",
    "    - getter only called when you retreive the attribute value\n",
    "3. Method extensions - assign a function that becomes available as an object method. lets you pass arguments to the extension function\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Token, Span\n",
    "Doc.set_extension( 'title', default=None )\n",
    "Token.set_extension( 'is_color', default=False )\n",
    "Span.set_extension( 'has_color', default=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overwrite extension attribute value\n",
    "doc = nlp( 'The sky is blue' )\n",
    "doc[3]._.is_color = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E090] Extension 'is_color' already exists on Token. To overwrite the existing extension, set `force=True` on `Token.set_extension`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-1909bb1c6bf2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'The sky is blue'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mToken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_extension\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'is_color'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_is_color\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_color\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/spacy/tokens/token.pyx\u001b[0m in \u001b[0;36mspacy.tokens.token.Token.set_extension\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E090] Extension 'is_color' already exists on Token. To overwrite the existing extension, set `force=True` on `Token.set_extension`."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load( 'en_core_web_sm' )\n",
    "def get_is_color( token ):\n",
    "    colors = ['red','yellow','blue']\n",
    "    return token.text in colors\n",
    "doc = nlp( 'The sky is blue' )\n",
    "Token.set_extension( 'is_color', getter=get_is_color )\n",
    "print( doc[3]._.is_color, '-', doc[3].text )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None -blue\n"
     ]
    }
   ],
   "source": [
    "# define a method with arguments\n",
    "def has_token( doc, token_text ):\n",
    "    in_doc = token_text in [token_text for token in doc ]\n",
    "Doc.set_extension( 'has_token', method=has_token )\n",
    "doc = nlp( \"The sky is blue\" )\n",
    "print( doc._.has_token('blue'), '-blue' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Scaling and Performance\n",
    "\n",
    "Processing large volumes of text:  \n",
    "\n",
    "* use the `nlp.pipe` method\n",
    "* processes texts as a stream, yields `Doc` objects\n",
    "* much faster than calling `nlp()` on each text\n",
    "\n",
    "good/bad:\n",
    "\n",
    "    BAD:\n",
    "    docs = [nlp( text ) for text in LOTS_OF_TEXTS ]\n",
    "    \n",
    "    GOOD:\n",
    "    docs = list( nlp.pipe( LOTS_OF_TEXT ) )\n",
    "    \n",
    "    \n",
    "Passing in Context:  \n",
    "\n",
    "* setting `as_tuples=True` on `nlp.pipe` lets you pass in `(text,context)` tuples\n",
    "* yields `( doc,context )` tuples\n",
    "* useful for associating metadata with the `doc`\n",
    "    \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a text 15\n",
      "And another text 16\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    ( 'This is a text', {'id':1, 'page_number':15}),\n",
    "    ( 'And another text', {'id':2, 'page_number':16})\n",
    "]\n",
    "\n",
    "for doc, context in nlp.pipe( data, as_tuples = True ):\n",
    "    print( doc.text, context['page_number'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Training a Neural Network Model\n",
    "\n",
    "### Training and Updating Models\n",
    "\n",
    "Training and updating Neural Network models.  \n",
    "**Why update an already trained model?**  \n",
    "\n",
    "* better results when trained with data from your domain\n",
    "* learn classification schemes specific to your problem...the model will need to learn this\n",
    "* Essential for text classification and useful for NER\n",
    "* Not so applicable for part-of-speech tagging or dependency parsing\n",
    "\n",
    "**How training works:**  \n",
    "\n",
    "1. **initialize** - the model weights start randomly with `nlp.begin_training`\n",
    "2. **Predict** - a few exampled with the current weights by calling `nlp.update`\n",
    "3. **Compare** - prediction with ground truth (labels)\n",
    "4. **Calculate** - how to change the weights to improve prediction\n",
    "5. **Update** - weights\n",
    "6. **Iterate** - got back to step 2\n",
    "\n",
    "![](training.png)\n",
    "\n",
    "where the **gradient** is how we should change the weights\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Example: Training an Entity Recognizer\n",
    "an entity recognizer tags words and phrases in context. tokens can only carry one entity.  \n",
    "examples need to come with context:  \n",
    "\n",
    "    (\"iPhone X is coming\", {'entities':[(0, 8, 'GADGET')]})\n",
    "    (\"I need a new phone! Any tipes?\", {'entities':[]})\n",
    "    \n",
    "**Goal:** teach the model to generalize  \n",
    "For updating an existing model: a few hundred to a few thousand examples  \n",
    "To train a new category: a few thousand to a million examples: `spaCy`'s English models were trained on 2 million words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to preorder the iPhone X [(4, 6, 'GADGET'), (4, 5, 'GADGET')]\n",
      "iPhone X is coming [(0, 2, 'GADGET'), (0, 1, 'GADGET')]\n",
      "Should I pay $1,000 for the iPhone X? [(7, 9, 'GADGET'), (7, 8, 'GADGET')]\n",
      "The iPhone 8 reviews are here [(1, 2, 'GADGET'), (1, 3, 'GADGET')]\n",
      "Your iPhone goes up to 11 today [(1, 2, 'GADGET')]\n",
      "I need a new phone! Any tips? []\n",
      "\n",
      "('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET'), (20, 26, 'GADGET')]})\n",
      "('iPhone X is coming', {'entities': [(0, 8, 'GADGET'), (0, 6, 'GADGET')]})\n",
      "('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET'), (28, 34, 'GADGET')]})\n",
      "('The iPhone 8 reviews are here', {'entities': [(4, 10, 'GADGET'), (4, 12, 'GADGET')]})\n",
      "('Your iPhone goes up to 11 today', {'entities': [(5, 11, 'GADGET')]})\n",
      "('I need a new phone! Any tips?', {'entities': []})\n"
     ]
    }
   ],
   "source": [
    "TEXTS = ['How to preorder the iPhone X',\n",
    " 'iPhone X is coming',\n",
    " 'Should I pay $1,000 for the iPhone X?',\n",
    " 'The iPhone 8 reviews are here',\n",
    " 'Your iPhone goes up to 11 today',\n",
    " 'I need a new phone! Any tips?']\n",
    "\n",
    "# Two tokens whose lowercase forms match 'iphone' and 'x'\n",
    "pattern1 = [{'LOWER': 'iphone'}, {'LOWER': 'x'}]\n",
    "\n",
    "# Token whose lowercase form matches 'iphone' and an optional digit\n",
    "pattern2 = [{'LOWER': 'iphone'}, {'IS_DIGIT': True, 'OP': '?'}]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add patterns to the matcher\n",
    "matcher.add('GADGET', [pattern1, pattern2])\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "docs = nlp.pipe( TEXTS )\n",
    "for doc in docs:   \n",
    "    # Find the matches in the doc\n",
    "    matches = matcher( doc )\n",
    "    #print( matches )\n",
    "    # Get a list of (start, end, label) tuples of matches in the text\n",
    "    entities = [(start, end, 'GADGET') for match_id, start, end  in matches]\n",
    "    print(doc.text, entities) \n",
    "\n",
    "print('')\n",
    "TRAINING_DATA = []\n",
    "\n",
    "# Create a Doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # Match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    # Get (start character, end character, label) tuples of matches\n",
    "    entities = [(span.start_char, span.end_char, 'GADGET') for span in spans]\n",
    "    \n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {'entities': entities})\n",
    "    # Append the example to the training data\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    \n",
    "print(*TRAINING_DATA, sep='\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### The Training Loop\n",
    "\n",
    "**The steps of a training loop:**  \n",
    "\n",
    "1. **Loop** for a number of times\n",
    "2. **Shuffle** the training data\n",
    "3. **Divide** the data into batches\n",
    "4. **Update** the model for each batch\n",
    "\n",
    "An Example loop:  \n",
    "\n",
    "    # Loop for 10 iterations\n",
    "    for i in range( 10 ):\n",
    "        # Shuffle the training data\n",
    "        random.shuffle( TRAINING_DATA )\n",
    "        #Create Batches and iterate over them\n",
    "        for batch in space.util.minibatch( TRAINING_DATA ):\n",
    "            #split the batch into texts and annotations\n",
    "            texts = [text for text, annotation in batch ]\n",
    "            annotations = [annotation for text, annotation in batch ]\n",
    "            #Update the whole model\n",
    "            nlp.update( texts, annotations )\n",
    "     # Save the model\n",
    "     nlp.to_disk( path_to_model )\n",
    "     \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # start with a blank English model\n",
    "    nlp = spacy.blank( 'en' )\n",
    "\n",
    "    # create a blank entity recognizer and add it to the pipeline\n",
    "    ner = nlp.create_pipe( 'ner' )\n",
    "    nlp.add_pipe( ner )\n",
    "\n",
    "    # add a new label\n",
    "    ner.add_label( 'GADGET' )\n",
    "\n",
    "    # start the training\n",
    "    nlp.begin_training()\n",
    "\n",
    "    # Train for 10 iterations over the data\n",
    "    for itn in range( 10 ):\n",
    "        random.shuffle( examples )\n",
    "        #Create Batches and iterate over them\n",
    "        for batch in space.util.minibatch( examples, size = 2 ):\n",
    "            #split the batch into texts and annotations\n",
    "            texts = [text for text, annotation in batch ]\n",
    "            annotations = [annotation for text, annotation in batch ]\n",
    "            #Update the whole model\n",
    "            nlp.update( texts, annotations )\n",
    "            \n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SETTING UP THE PIPELINE\n",
    "# Create a blank 'en' model\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "# Create a new entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe('ner')\n",
    "nlp.add_pipe('ner')\n",
    "\n",
    "# Add the label 'GADGET' to the entity recognizer\n",
    "ner.add_label( 'GADGET' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = [('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET')]}),\n",
    " ('iPhone X is coming', {'entities': [(0, 8, 'GADGET')]}),\n",
    " ('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET')]}),\n",
    " ('The iPhone 8 reviews are here', {'entities': [(4, 12, 'GADGET')]}),\n",
    " ('Your iPhone goes up to 11 today', {'entities': [(5, 11, 'GADGET')]}),\n",
    " ('I need a new phone! Any tips?', {'entities': []})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 5.8333335518836975}\n",
      "{'ner': 9.089479327201843}\n",
      "{'ner': 13.766608476638794}\n",
      "{'ner': 18.248479187488556}\n",
      "{'ner': 25.34346306324005}\n",
      "{'ner': 31.3095862865448}\n",
      "{'ner': 6.028357297182083}\n",
      "{'ner': 8.689824372529984}\n",
      "{'ner': 13.222599774599075}\n",
      "{'ner': 16.056995928287506}\n",
      "{'ner': 18.62058037519455}\n",
      "{'ner': 21.735183596611023}\n",
      "{'ner': 2.659496918320656}\n",
      "{'ner': 5.56480174139142}\n",
      "{'ner': 7.74324381724}\n",
      "{'ner': 9.414004173129797}\n",
      "{'ner': 10.444192636758089}\n",
      "{'ner': 11.913115268573165}\n",
      "{'ner': 1.651505522429943}\n",
      "{'ner': 2.796864077914506}\n",
      "{'ner': 3.7544691434595734}\n",
      "{'ner': 4.693495137267746}\n",
      "{'ner': 5.975541429215809}\n",
      "{'ner': 6.169256124405365}\n",
      "{'ner': 0.7565131396988818}\n",
      "{'ner': 1.6984776622023219}\n",
      "{'ner': 2.4542741169036617}\n",
      "{'ner': 2.5021985150572164}\n",
      "{'ner': 8.372751412510524}\n",
      "{'ner': 10.521014072136722}\n",
      "{'ner': 0.7625262818328338}\n",
      "{'ner': 2.7938871729784296}\n",
      "{'ner': 5.117457886584816}\n",
      "{'ner': 5.183832102189626}\n",
      "{'ner': 6.920053617726808}\n",
      "{'ner': 8.298221526360976}\n",
      "{'ner': 1.1167632454275918}\n",
      "{'ner': 2.1506785166593545}\n",
      "{'ner': 2.1613183157428466}\n",
      "{'ner': 3.3231107994265585}\n",
      "{'ner': 4.441406826933871}\n",
      "{'ner': 5.655625448387582}\n",
      "{'ner': 1.2415613230497229}\n",
      "{'ner': 1.6184058984778176}\n",
      "{'ner': 1.803303256190361}\n",
      "{'ner': 2.2806442825932436}\n",
      "{'ner': 2.353965064015938}\n",
      "{'ner': 2.3543307755032856}\n",
      "{'ner': 0.00035937063273649983}\n",
      "{'ner': 1.1049012373957998}\n",
      "{'ner': 1.1266176694324814}\n",
      "{'ner': 1.1402413637062425}\n",
      "{'ner': 1.149275374555752}\n",
      "{'ner': 1.1645199927552978}\n",
      "{'ner': 0.00011324281831470717}\n",
      "{'ner': 0.0004228178429199758}\n",
      "{'ner': 1.8564026709814399}\n",
      "{'ner': 1.8564386697331303}\n",
      "{'ner': 1.856445443138098}\n",
      "{'ner': 1.856463823741033}\n"
     ]
    }
   ],
   "source": [
    "# BUILD A TRAINING LOOP\n",
    "import random\n",
    "from spacy.training.example import Example\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for itn in range(10):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "    \n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        for text, annotations in batch:\n",
    "            # create Example\n",
    "            doc = nlp.make_doc(text)\n",
    "            example = Example.from_dict(doc, annotations)\n",
    "            # Update the model\n",
    "            nlp.update([example], losses=losses, drop=0.3)\n",
    "            print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers printed to the IPython shell represent the loss on each iteration, the amount of work left for the optimizer. The lower the number, the better. In real life, you normally want to use a lot more data than this, ideally at least a few hundred or a few thousand examples.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA = ['Apple is slowing down the iPhone 8 and iPhone X - how to stop it',\n",
    " \"I finally understand what the iPhone X 'notch' is for\",\n",
    " 'Everything you need to know about the Samsung Galaxy S9',\n",
    " 'Looking to compare iPad models? Here’s how the 2018 lineup stacks up',\n",
    " 'The iPhone 8 and iPhone 8 Plus are smartphones designed, developed, and marketed by Apple',\n",
    " 'what is the cheapest ipad, especially ipad pro???',\n",
    " 'Samsung Galaxy is a series of mobile computing devices designed, manufactured and marketed by Samsung Electronics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is slowing down the iPhone 8 and iPhone X - how to stop it\n",
      "(iPhone 8, iPhone X) \n",
      "\n",
      "\n",
      "I finally understand what the iPhone X 'notch' is for\n",
      "(iPhone X,) \n",
      "\n",
      "\n",
      "Everything you need to know about the Samsung Galaxy S9\n",
      "() \n",
      "\n",
      "\n",
      "Looking to compare iPad models? Here’s how the 2018 lineup stacks up\n",
      "(2018 lineup,) \n",
      "\n",
      "\n",
      "The iPhone 8 and iPhone 8 Plus are smartphones designed, developed, and marketed by Apple\n",
      "(iPhone 8, iPhone 8) \n",
      "\n",
      "\n",
      "what is the cheapest ipad, especially ipad pro???\n",
      "() \n",
      "\n",
      "\n",
      "Samsung Galaxy is a series of mobile computing devices designed, manufactured and marketed by Samsung Electronics\n",
      "() \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = nlp.pipe( TEST_DATA )\n",
    "# Process each text in TEST_DATA\n",
    "for doc in docs:\n",
    "    # Print the document text and entitites\n",
    "    print( doc.text )\n",
    "    print( doc.ents, '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Best Practices for training spaCy models\n",
    "\n",
    "* **Problem 1: catastrophic forgetting** - when an existing model overfits new data to learn a new label, it might 'unlearn' how to predict labels it was previously accurate for.\n",
    "    * **Solution** - mix in previously correct predictions\n",
    "    * Run existing `spaCy` model over data and extract all other relevant entities\n",
    "* **Problem 2:** Model's can't learn everything. Models can struggle to learn if the decision is difficult to make based on context.\n",
    "    * **Solution** label schemes need to be consistent and not too specific.\n",
    "    * for example: `CLOTHING` might be more succesful than `ADULT_CLOTHING` & `CHILDRENS_CLOTHING`\n",
    "    * pick categories that are reflected in the local context\n",
    "    * more genaric is better than too specific   \n",
    "    \n",
    "Examples:  \n",
    "BAD:\n",
    "\n",
    "    TRAINING_DATA = [('Reddit is a website', {'entities': [(0, 6, 'WEBSITE')]})]\n",
    "    \n",
    "GOOD:  \n",
    "\n",
    "    TRAINING_DATA = [('Reddit is a website', {'entities': [(0, 6, 'WEBSITE')]}),\n",
    "                     ('Obama is a person', {'entities': [(0, 5, 'PERSON')]})\n",
    "                     \n",
    "BAD:\n",
    "\n",
    "    LABELS = ['ADULT_SHOES', 'CHILDRENS_SHOES', 'YA_SHOES', 'YA_CLOTHING', 'WOMENS_CLOTHING']\n",
    "    \n",
    "GOOD:\n",
    "\n",
    "    LABELS = ['SHOES', 'CLOTHING']\n",
    "    \n",
    "<BR>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
